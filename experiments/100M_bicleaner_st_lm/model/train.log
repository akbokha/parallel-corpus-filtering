[2019-07-24 10:08:25] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-24 10:08:25] [marian] Running on baldur as process 138128 with command line:
[2019-07-24 10:08:25] [marian] /fs/bil0/abdel/marian-dev/build/marian --sync-sgd --model ../experiments/100M_bicleaner_st_lm/model/model.npz -T . --devices 2 3 --train-sets ../experiments/100M_bicleaner_st_lm/data/train.bpe.de ../experiments/100M_bicleaner_st_lm/data/train.bpe.en --vocabs ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json --mini-batch-fit -w 5000 --dim-vocabs 50000 50000 --layer-normalization --dropout-rnn 0.2 --dropout-src 0.1 --dropout-trg 0.1 --learn-rate 0.0001 --after-epochs 0 --early-stopping 5 --valid-freq 20000 --save-freq 20000 --disp-freq 2000 --valid-mini-batch 8 --valid-sets ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en --valid-metrics cross-entropy perplexity translation --valid-translation-output ../experiments/100M_bicleaner_st_lm/model/dev.out --valid-script-path ../experiments/100M_bicleaner_st_lm/score-dev.sh --seed 1111 --exponential-smoothing --normalize=1 --beam-size=12 --quiet-translation --log ../experiments/100M_bicleaner_st_lm/model/train.log --valid-log ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-24 10:08:25] [config] after-batches: 0
[2019-07-24 10:08:25] [config] after-epochs: 0
[2019-07-24 10:08:25] [config] allow-unk: false
[2019-07-24 10:08:25] [config] beam-size: 12
[2019-07-24 10:08:25] [config] bert-class-symbol: "[CLS]"
[2019-07-24 10:08:25] [config] bert-mask-symbol: "[MASK]"
[2019-07-24 10:08:25] [config] bert-masking-fraction: 0.15
[2019-07-24 10:08:25] [config] bert-sep-symbol: "[SEP]"
[2019-07-24 10:08:25] [config] bert-train-type-embeddings: true
[2019-07-24 10:08:25] [config] bert-type-vocab-size: 2
[2019-07-24 10:08:25] [config] best-deep: false
[2019-07-24 10:08:25] [config] clip-gemm: 0
[2019-07-24 10:08:25] [config] clip-norm: 1
[2019-07-24 10:08:25] [config] cost-type: ce-mean
[2019-07-24 10:08:25] [config] cpu-threads: 0
[2019-07-24 10:08:25] [config] data-weighting: ""
[2019-07-24 10:08:25] [config] data-weighting-type: sentence
[2019-07-24 10:08:25] [config] dec-cell: gru
[2019-07-24 10:08:25] [config] dec-cell-base-depth: 2
[2019-07-24 10:08:25] [config] dec-cell-high-depth: 1
[2019-07-24 10:08:25] [config] dec-depth: 1
[2019-07-24 10:08:25] [config] devices:
[2019-07-24 10:08:25] [config]   - 2
[2019-07-24 10:08:25] [config]   - 3
[2019-07-24 10:08:25] [config] dim-emb: 512
[2019-07-24 10:08:25] [config] dim-rnn: 1024
[2019-07-24 10:08:25] [config] dim-vocabs:
[2019-07-24 10:08:25] [config]   - 50000
[2019-07-24 10:08:25] [config]   - 50000
[2019-07-24 10:08:25] [config] disp-first: 0
[2019-07-24 10:08:25] [config] disp-freq: 2000
[2019-07-24 10:08:25] [config] disp-label-counts: false
[2019-07-24 10:08:25] [config] dropout-rnn: 0.2
[2019-07-24 10:08:25] [config] dropout-src: 0.1
[2019-07-24 10:08:25] [config] dropout-trg: 0.1
[2019-07-24 10:08:25] [config] dump-config: ""
[2019-07-24 10:08:25] [config] early-stopping: 5
[2019-07-24 10:08:25] [config] embedding-fix-src: false
[2019-07-24 10:08:25] [config] embedding-fix-trg: false
[2019-07-24 10:08:25] [config] embedding-normalization: false
[2019-07-24 10:08:25] [config] embedding-vectors:
[2019-07-24 10:08:25] [config]   []
[2019-07-24 10:08:25] [config] enc-cell: gru
[2019-07-24 10:08:25] [config] enc-cell-depth: 1
[2019-07-24 10:08:25] [config] enc-depth: 1
[2019-07-24 10:08:25] [config] enc-type: bidirectional
[2019-07-24 10:08:25] [config] exponential-smoothing: 0.0001
[2019-07-24 10:08:25] [config] grad-dropping-momentum: 0
[2019-07-24 10:08:25] [config] grad-dropping-rate: 0
[2019-07-24 10:08:25] [config] grad-dropping-warmup: 100
[2019-07-24 10:08:25] [config] guided-alignment: none
[2019-07-24 10:08:25] [config] guided-alignment-cost: mse
[2019-07-24 10:08:25] [config] guided-alignment-weight: 0.1
[2019-07-24 10:08:25] [config] ignore-model-config: false
[2019-07-24 10:08:25] [config] input-types:
[2019-07-24 10:08:25] [config]   []
[2019-07-24 10:08:25] [config] interpolate-env-vars: false
[2019-07-24 10:08:25] [config] keep-best: false
[2019-07-24 10:08:25] [config] label-smoothing: 0
[2019-07-24 10:08:25] [config] layer-normalization: true
[2019-07-24 10:08:25] [config] learn-rate: 0.0001
[2019-07-24 10:08:25] [config] log: ../experiments/100M_bicleaner_st_lm/model/train.log
[2019-07-24 10:08:25] [config] log-level: info
[2019-07-24 10:08:25] [config] log-time-zone: ""
[2019-07-24 10:08:25] [config] lr-decay: 0
[2019-07-24 10:08:25] [config] lr-decay-freq: 50000
[2019-07-24 10:08:25] [config] lr-decay-inv-sqrt:
[2019-07-24 10:08:25] [config]   - 0
[2019-07-24 10:08:25] [config] lr-decay-repeat-warmup: false
[2019-07-24 10:08:25] [config] lr-decay-reset-optimizer: false
[2019-07-24 10:08:25] [config] lr-decay-start:
[2019-07-24 10:08:25] [config]   - 10
[2019-07-24 10:08:25] [config]   - 1
[2019-07-24 10:08:25] [config] lr-decay-strategy: epoch+stalled
[2019-07-24 10:08:25] [config] lr-report: false
[2019-07-24 10:08:25] [config] lr-warmup: 0
[2019-07-24 10:08:25] [config] lr-warmup-at-reload: false
[2019-07-24 10:08:25] [config] lr-warmup-cycle: false
[2019-07-24 10:08:25] [config] lr-warmup-start-rate: 0
[2019-07-24 10:08:25] [config] max-length: 50
[2019-07-24 10:08:25] [config] max-length-crop: false
[2019-07-24 10:08:25] [config] max-length-factor: 3
[2019-07-24 10:08:25] [config] maxi-batch: 100
[2019-07-24 10:08:25] [config] maxi-batch-sort: trg
[2019-07-24 10:08:25] [config] mini-batch: 64
[2019-07-24 10:08:25] [config] mini-batch-fit: true
[2019-07-24 10:08:25] [config] mini-batch-fit-step: 10
[2019-07-24 10:08:25] [config] mini-batch-overstuff: 1
[2019-07-24 10:08:25] [config] mini-batch-track-lr: false
[2019-07-24 10:08:25] [config] mini-batch-understuff: 1
[2019-07-24 10:08:25] [config] mini-batch-warmup: 0
[2019-07-24 10:08:25] [config] mini-batch-words: 0
[2019-07-24 10:08:25] [config] mini-batch-words-ref: 0
[2019-07-24 10:08:25] [config] model: ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-24 10:08:25] [config] multi-loss-type: sum
[2019-07-24 10:08:25] [config] multi-node: false
[2019-07-24 10:08:25] [config] multi-node-overlap: true
[2019-07-24 10:08:25] [config] n-best: false
[2019-07-24 10:08:25] [config] no-nccl: false
[2019-07-24 10:08:25] [config] no-reload: false
[2019-07-24 10:08:25] [config] no-restore-corpus: false
[2019-07-24 10:08:25] [config] no-shuffle: false
[2019-07-24 10:08:25] [config] normalize: 1
[2019-07-24 10:08:25] [config] num-devices: 0
[2019-07-24 10:08:25] [config] optimizer: adam
[2019-07-24 10:08:25] [config] optimizer-delay: 1
[2019-07-24 10:08:25] [config] optimizer-params:
[2019-07-24 10:08:25] [config]   []
[2019-07-24 10:08:25] [config] overwrite: false
[2019-07-24 10:08:25] [config] pretrained-model: ""
[2019-07-24 10:08:25] [config] quiet: false
[2019-07-24 10:08:25] [config] quiet-translation: true
[2019-07-24 10:08:25] [config] relative-paths: false
[2019-07-24 10:08:25] [config] right-left: false
[2019-07-24 10:08:25] [config] save-freq: 20000
[2019-07-24 10:08:25] [config] seed: 1111
[2019-07-24 10:08:25] [config] shuffle-in-ram: false
[2019-07-24 10:08:25] [config] skip: false
[2019-07-24 10:08:25] [config] sqlite: ""
[2019-07-24 10:08:25] [config] sqlite-drop: false
[2019-07-24 10:08:25] [config] sync-sgd: true
[2019-07-24 10:08:25] [config] tempdir: .
[2019-07-24 10:08:25] [config] tied-embeddings: false
[2019-07-24 10:08:25] [config] tied-embeddings-all: false
[2019-07-24 10:08:25] [config] tied-embeddings-src: false
[2019-07-24 10:08:25] [config] train-sets:
[2019-07-24 10:08:25] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de
[2019-07-24 10:08:25] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en
[2019-07-24 10:08:25] [config] transformer-aan-activation: swish
[2019-07-24 10:08:25] [config] transformer-aan-depth: 2
[2019-07-24 10:08:25] [config] transformer-aan-nogate: false
[2019-07-24 10:08:25] [config] transformer-decoder-autoreg: self-attention
[2019-07-24 10:08:25] [config] transformer-dim-aan: 2048
[2019-07-24 10:08:25] [config] transformer-dim-ffn: 2048
[2019-07-24 10:08:25] [config] transformer-dropout: 0
[2019-07-24 10:08:25] [config] transformer-dropout-attention: 0
[2019-07-24 10:08:25] [config] transformer-dropout-ffn: 0
[2019-07-24 10:08:25] [config] transformer-ffn-activation: swish
[2019-07-24 10:08:25] [config] transformer-ffn-depth: 2
[2019-07-24 10:08:25] [config] transformer-guided-alignment-layer: last
[2019-07-24 10:08:25] [config] transformer-heads: 8
[2019-07-24 10:08:25] [config] transformer-no-projection: false
[2019-07-24 10:08:25] [config] transformer-postprocess: dan
[2019-07-24 10:08:25] [config] transformer-postprocess-emb: d
[2019-07-24 10:08:25] [config] transformer-preprocess: ""
[2019-07-24 10:08:25] [config] transformer-tied-layers:
[2019-07-24 10:08:25] [config]   []
[2019-07-24 10:08:25] [config] transformer-train-position-embeddings: false
[2019-07-24 10:08:25] [config] type: amun
[2019-07-24 10:08:25] [config] ulr: false
[2019-07-24 10:08:25] [config] ulr-dim-emb: 0
[2019-07-24 10:08:25] [config] ulr-dropout: 0
[2019-07-24 10:08:25] [config] ulr-keys-vectors: ""
[2019-07-24 10:08:25] [config] ulr-query-vectors: ""
[2019-07-24 10:08:25] [config] ulr-softmax-temperature: 1
[2019-07-24 10:08:25] [config] ulr-trainable-transformation: false
[2019-07-24 10:08:25] [config] valid-freq: 20000
[2019-07-24 10:08:25] [config] valid-log: ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-24 10:08:25] [config] valid-max-length: 1000
[2019-07-24 10:08:25] [config] valid-metrics:
[2019-07-24 10:08:25] [config]   - cross-entropy
[2019-07-24 10:08:25] [config]   - perplexity
[2019-07-24 10:08:25] [config]   - translation
[2019-07-24 10:08:25] [config] valid-mini-batch: 8
[2019-07-24 10:08:25] [config] valid-script-path: ../experiments/100M_bicleaner_st_lm/score-dev.sh
[2019-07-24 10:08:25] [config] valid-sets:
[2019-07-24 10:08:25] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de
[2019-07-24 10:08:25] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en
[2019-07-24 10:08:25] [config] valid-translation-output: ../experiments/100M_bicleaner_st_lm/model/dev.out
[2019-07-24 10:08:25] [config] vocabs:
[2019-07-24 10:08:25] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-24 10:08:25] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-24 10:08:25] [config] word-penalty: 0
[2019-07-24 10:08:25] [config] workspace: 5000
[2019-07-24 10:08:25] [config] Model is being created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-24 10:08:25] Using synchronous training
[2019-07-24 10:08:25] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-24 10:08:25] [data] Using unused word id eos for 0
[2019-07-24 10:08:25] [data] Using unused word id UNK for 1
[2019-07-24 10:08:25] [data] Setting vocabulary size for input 0 to 50000
[2019-07-24 10:08:25] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-24 10:08:26] [data] Using unused word id eos for 0
[2019-07-24 10:08:26] [data] Using unused word id UNK for 1
[2019-07-24 10:08:26] [data] Setting vocabulary size for input 1 to 50000
[2019-07-24 10:08:26] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-24 10:08:26] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-24 10:08:26] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-24 10:08:27] [memory] Extending reserved space to 5120 MB (device gpu3)
[2019-07-24 10:08:27] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-24 10:08:27] [comm] NCCLCommunicator constructed successfully.
[2019-07-24 10:08:27] [training] Using 2 GPUs
[2019-07-24 10:08:27] [memory] Reserving 422 MB, device gpu2
[2019-07-24 10:08:27] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-24 10:08:27] [memory] Reserving 422 MB, device gpu2
[2019-07-24 10:08:41] [batching] Done. Typical MB size is 13760 target words
[2019-07-24 10:08:42] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-24 10:08:42] [memory] Extending reserved space to 5120 MB (device gpu3)
[2019-07-24 10:08:42] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-24 10:08:42] [comm] NCCLCommunicator constructed successfully.
[2019-07-24 10:08:42] [training] Using 2 GPUs
[2019-07-24 10:08:42] Training started
[2019-07-24 10:08:42] [data] Shuffling data
[2019-07-24 10:08:44] [data] Done reading 4864128 sentences
[2019-07-24 10:09:07] [data] Done shuffling 4864128 sentences to temp files
[2019-07-24 10:09:12] [training] Batches are processed as 1 process(es) x 2 devices/process
[2019-07-24 10:09:12] [memory] Reserving 422 MB, device gpu2
[2019-07-24 10:09:12] [memory] Reserving 422 MB, device gpu3
[2019-07-24 10:09:13] [memory] Reserving 422 MB, device gpu2
[2019-07-24 10:09:13] [memory] Reserving 422 MB, device gpu3
[2019-07-24 10:09:13] [memory] Reserving 211 MB, device gpu2
[2019-07-24 10:09:13] [memory] Reserving 211 MB, device gpu3
[2019-07-24 10:09:13] [memory] Reserving 422 MB, device gpu2
[2019-07-24 10:09:13] [memory] Reserving 422 MB, device gpu3
[2019-07-24 10:34:42] Ep. 1 : Up. 2000 : Sen. 666,588 : Cost 130.88385010 : Time 1576.08s : 9509.56 words/s
[2019-07-24 11:00:17] Ep. 1 : Up. 4000 : Sen. 1,332,688 : Cost 97.52057648 : Time 1535.73s : 9745.69 words/s
[2019-07-24 11:25:47] Ep. 1 : Up. 6000 : Sen. 1,995,598 : Cost 82.31631470 : Time 1529.99s : 9736.56 words/s
[2019-07-24 11:51:19] Ep. 1 : Up. 8000 : Sen. 2,661,066 : Cost 73.68851471 : Time 1531.94s : 9748.51 words/s
[2019-07-24 12:16:56] Ep. 1 : Up. 10000 : Sen. 3,326,748 : Cost 68.57045746 : Time 1536.34s : 9725.36 words/s
[2019-07-24 12:42:29] Ep. 1 : Up. 12000 : Sen. 3,992,544 : Cost 65.19651031 : Time 1533.80s : 9761.09 words/s
[2019-07-24 12:51:42] Seen 4231167 samples
[2019-07-24 12:51:42] Starting epoch 2
[2019-07-24 12:51:42] [data] Shuffling data
[2019-07-24 12:51:45] [data] Done reading 4864128 sentences
[2019-07-24 12:52:02] [data] Done shuffling 4864128 sentences to temp files
[2019-07-24 13:08:30] Ep. 2 : Up. 14000 : Sen. 427,200 : Cost 62.06927490 : Time 1560.84s : 9587.47 words/s
[2019-07-24 13:34:04] Ep. 2 : Up. 16000 : Sen. 1,091,184 : Cost 60.16693115 : Time 1534.14s : 9737.73 words/s
[2019-07-24 13:59:37] Ep. 2 : Up. 18000 : Sen. 1,756,394 : Cost 58.26856613 : Time 1532.35s : 9737.31 words/s
[2019-07-24 14:25:17] Ep. 2 : Up. 20000 : Sen. 2,424,902 : Cost 57.37661362 : Time 1540.20s : 9755.81 words/s
[2019-07-24 14:25:17] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-24 14:25:26] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter20000.npz
[2019-07-24 14:25:33] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-24 14:25:42] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-24 14:26:05] [valid] Ep. 2 : Up. 20000 : cross-entropy : 57.0394 : new best
[2019-07-24 14:26:11] [valid] Ep. 2 : Up. 20000 : perplexity : 9.45503 : new best
[2019-07-24 14:27:05] [valid] Ep. 2 : Up. 20000 : translation : 25.43 : new best
[2019-07-24 14:52:43] Ep. 2 : Up. 22000 : Sen. 3,092,536 : Cost 56.25093842 : Time 1646.39s : 9109.12 words/s
[2019-07-24 15:18:19] Ep. 2 : Up. 24000 : Sen. 3,757,592 : Cost 55.29177856 : Time 1535.63s : 9745.35 words/s
[2019-07-24 15:36:30] Seen 4231167 samples
[2019-07-24 15:36:30] Starting epoch 3
[2019-07-24 15:36:30] [data] Shuffling data
[2019-07-24 15:36:33] [data] Done reading 4864128 sentences
[2019-07-24 15:36:52] [data] Done shuffling 4864128 sentences to temp files
[2019-07-24 15:44:16] Ep. 3 : Up. 26000 : Sen. 192,264 : Cost 53.95672989 : Time 1557.43s : 9603.15 words/s
[2019-07-24 16:09:52] Ep. 3 : Up. 28000 : Sen. 857,864 : Cost 52.83171463 : Time 1535.64s : 9744.06 words/s
[2019-07-24 16:35:25] Ep. 3 : Up. 30000 : Sen. 1,522,476 : Cost 52.16708755 : Time 1533.20s : 9741.42 words/s
[2019-07-24 17:00:57] Ep. 3 : Up. 32000 : Sen. 2,185,488 : Cost 51.60712051 : Time 1531.54s : 9710.78 words/s
[2019-07-24 17:26:32] Ep. 3 : Up. 34000 : Sen. 2,850,962 : Cost 51.26274490 : Time 1535.61s : 9740.80 words/s
[2019-07-24 17:52:10] Ep. 3 : Up. 36000 : Sen. 3,516,884 : Cost 50.87535095 : Time 1537.38s : 9746.87 words/s
[2019-07-24 18:17:46] Ep. 3 : Up. 38000 : Sen. 4,184,808 : Cost 50.27993774 : Time 1536.15s : 9755.28 words/s
[2019-07-24 18:19:34] Seen 4231167 samples
[2019-07-24 18:19:34] Starting epoch 4
[2019-07-24 18:19:34] [data] Shuffling data
[2019-07-24 18:19:36] [data] Done reading 4864128 sentences
[2019-07-24 18:19:55] [data] Done shuffling 4864128 sentences to temp files
[2019-07-24 18:43:36] Ep. 4 : Up. 40000 : Sen. 615,192 : Cost 49.25005722 : Time 1550.13s : 9592.36 words/s
[2019-07-24 18:43:36] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-24 18:43:45] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter40000.npz
[2019-07-24 18:43:52] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-24 18:44:02] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-24 18:44:25] [valid] Ep. 4 : Up. 40000 : cross-entropy : 48.3154 : new best
[2019-07-24 18:44:31] [valid] Ep. 4 : Up. 40000 : perplexity : 6.70563 : new best
[2019-07-24 18:45:25] [valid] Ep. 4 : Up. 40000 : translation : 28.27 : new best
[2019-07-24 19:11:00] Ep. 4 : Up. 42000 : Sen. 1,280,000 : Cost 48.85905838 : Time 1644.40s : 9078.95 words/s
[2019-07-24 19:36:34] Ep. 4 : Up. 44000 : Sen. 1,944,238 : Cost 48.60482407 : Time 1533.14s : 9732.34 words/s
[2019-07-24 20:02:08] Ep. 4 : Up. 46000 : Sen. 2,609,976 : Cost 48.47484589 : Time 1534.08s : 9754.40 words/s
[2019-07-24 20:27:46] Ep. 4 : Up. 48000 : Sen. 3,276,800 : Cost 48.21767044 : Time 1538.52s : 9739.13 words/s
[2019-07-24 20:53:21] Ep. 4 : Up. 50000 : Sen. 3,941,592 : Cost 47.84053421 : Time 1534.53s : 9737.33 words/s
[2019-07-24 21:04:31] Seen 4231167 samples
[2019-07-24 21:04:31] Starting epoch 5
[2019-07-24 21:04:31] [data] Shuffling data
[2019-07-24 21:04:34] [data] Done reading 4864128 sentences
[2019-07-24 21:04:52] [data] Done shuffling 4864128 sentences to temp files
[2019-07-24 21:19:22] Ep. 5 : Up. 52000 : Sen. 376,876 : Cost 47.13520432 : Time 1560.90s : 9596.26 words/s
[2019-07-24 21:44:57] Ep. 5 : Up. 54000 : Sen. 1,041,166 : Cost 46.85512161 : Time 1535.82s : 9725.53 words/s
[2019-07-24 22:10:37] Ep. 5 : Up. 56000 : Sen. 1,708,340 : Cost 46.61496353 : Time 1539.51s : 9735.84 words/s
[2019-07-24 22:36:14] Ep. 5 : Up. 58000 : Sen. 2,375,362 : Cost 46.41595459 : Time 1536.70s : 9742.94 words/s
[2019-07-24 23:01:51] Ep. 5 : Up. 60000 : Sen. 3,041,540 : Cost 46.33766937 : Time 1537.06s : 9743.95 words/s
[2019-07-24 23:01:51] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-24 23:02:01] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter60000.npz
[2019-07-24 23:02:08] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-24 23:02:18] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-24 23:02:43] [valid] Ep. 5 : Up. 60000 : cross-entropy : 44.9274 : new best
[2019-07-24 23:02:48] [valid] Ep. 5 : Up. 60000 : perplexity : 5.86794 : new best
[2019-07-24 23:03:42] [valid] Ep. 5 : Up. 60000 : translation : 29.3 : new best
[2019-07-24 23:16:10] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-24 23:16:10] [marian] Running on hodor as process 136202 with command line:
[2019-07-24 23:16:10] [marian] /fs/bil0/abdel/marian-dev/build/marian --sync-sgd --model ../experiments/100M_bicleaner_st_lm/model/model.npz -T . --devices 2 3 --train-sets ../experiments/100M_bicleaner_st_lm/data/train.bpe.de ../experiments/100M_bicleaner_st_lm/data/train.bpe.en --vocabs ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json --mini-batch-fit -w 5000 --dim-vocabs 50000 50000 --layer-normalization --dropout-rnn 0.2 --dropout-src 0.1 --dropout-trg 0.1 --learn-rate 0.0001 --after-epochs 0 --early-stopping 5 --valid-freq 20000 --save-freq 20000 --disp-freq 2000 --valid-mini-batch 8 --valid-sets ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en --valid-metrics cross-entropy perplexity translation --valid-translation-output ../experiments/100M_bicleaner_st_lm/model/dev.out --valid-script-path ../experiments/100M_bicleaner_st_lm/score-dev.sh --seed 1111 --exponential-smoothing --normalize=1 --beam-size=12 --quiet-translation --log ../experiments/100M_bicleaner_st_lm/model/train.log --valid-log ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-24 23:16:10] [config] after-batches: 0
[2019-07-24 23:16:10] [config] after-epochs: 0
[2019-07-24 23:16:10] [config] allow-unk: false
[2019-07-24 23:16:10] [config] beam-size: 12
[2019-07-24 23:16:10] [config] bert-class-symbol: "[CLS]"
[2019-07-24 23:16:10] [config] bert-mask-symbol: "[MASK]"
[2019-07-24 23:16:10] [config] bert-masking-fraction: 0.15
[2019-07-24 23:16:10] [config] bert-sep-symbol: "[SEP]"
[2019-07-24 23:16:10] [config] bert-train-type-embeddings: true
[2019-07-24 23:16:10] [config] bert-type-vocab-size: 2
[2019-07-24 23:16:10] [config] best-deep: false
[2019-07-24 23:16:10] [config] clip-gemm: 0
[2019-07-24 23:16:10] [config] clip-norm: 1
[2019-07-24 23:16:10] [config] cost-type: ce-mean
[2019-07-24 23:16:10] [config] cpu-threads: 0
[2019-07-24 23:16:10] [config] data-weighting: ""
[2019-07-24 23:16:10] [config] data-weighting-type: sentence
[2019-07-24 23:16:10] [config] dec-cell: gru
[2019-07-24 23:16:10] [config] dec-cell-base-depth: 2
[2019-07-24 23:16:10] [config] dec-cell-high-depth: 1
[2019-07-24 23:16:10] [config] dec-depth: 1
[2019-07-24 23:16:10] [config] devices:
[2019-07-24 23:16:10] [config]   - 2
[2019-07-24 23:16:10] [config]   - 3
[2019-07-24 23:16:10] [config] dim-emb: 512
[2019-07-24 23:16:10] [config] dim-rnn: 1024
[2019-07-24 23:16:10] [config] dim-vocabs:
[2019-07-24 23:16:10] [config]   - 50000
[2019-07-24 23:16:10] [config]   - 50000
[2019-07-24 23:16:10] [config] disp-first: 0
[2019-07-24 23:16:10] [config] disp-freq: 2000
[2019-07-24 23:16:10] [config] disp-label-counts: false
[2019-07-24 23:16:10] [config] dropout-rnn: 0.2
[2019-07-24 23:16:10] [config] dropout-src: 0.1
[2019-07-24 23:16:10] [config] dropout-trg: 0.1
[2019-07-24 23:16:10] [config] dump-config: ""
[2019-07-24 23:16:10] [config] early-stopping: 5
[2019-07-24 23:16:10] [config] embedding-fix-src: false
[2019-07-24 23:16:10] [config] embedding-fix-trg: false
[2019-07-24 23:16:10] [config] embedding-normalization: false
[2019-07-24 23:16:10] [config] embedding-vectors:
[2019-07-24 23:16:10] [config]   []
[2019-07-24 23:16:10] [config] enc-cell: gru
[2019-07-24 23:16:10] [config] enc-cell-depth: 1
[2019-07-24 23:16:10] [config] enc-depth: 1
[2019-07-24 23:16:10] [config] enc-type: bidirectional
[2019-07-24 23:16:10] [config] exponential-smoothing: 0.0001
[2019-07-24 23:16:10] [config] grad-dropping-momentum: 0
[2019-07-24 23:16:10] [config] grad-dropping-rate: 0
[2019-07-24 23:16:10] [config] grad-dropping-warmup: 100
[2019-07-24 23:16:10] [config] guided-alignment: none
[2019-07-24 23:16:10] [config] guided-alignment-cost: mse
[2019-07-24 23:16:10] [config] guided-alignment-weight: 0.1
[2019-07-24 23:16:10] [config] ignore-model-config: false
[2019-07-24 23:16:10] [config] input-types:
[2019-07-24 23:16:10] [config]   []
[2019-07-24 23:16:10] [config] interpolate-env-vars: false
[2019-07-24 23:16:10] [config] keep-best: false
[2019-07-24 23:16:10] [config] label-smoothing: 0
[2019-07-24 23:16:10] [config] layer-normalization: true
[2019-07-24 23:16:10] [config] learn-rate: 0.0001
[2019-07-24 23:16:10] [config] log: ../experiments/100M_bicleaner_st_lm/model/train.log
[2019-07-24 23:16:10] [config] log-level: info
[2019-07-24 23:16:10] [config] log-time-zone: ""
[2019-07-24 23:16:10] [config] lr-decay: 0
[2019-07-24 23:16:10] [config] lr-decay-freq: 50000
[2019-07-24 23:16:10] [config] lr-decay-inv-sqrt:
[2019-07-24 23:16:10] [config]   - 0
[2019-07-24 23:16:10] [config] lr-decay-repeat-warmup: false
[2019-07-24 23:16:10] [config] lr-decay-reset-optimizer: false
[2019-07-24 23:16:10] [config] lr-decay-start:
[2019-07-24 23:16:10] [config]   - 10
[2019-07-24 23:16:10] [config]   - 1
[2019-07-24 23:16:10] [config] lr-decay-strategy: epoch+stalled
[2019-07-24 23:16:10] [config] lr-report: false
[2019-07-24 23:16:10] [config] lr-warmup: 0
[2019-07-24 23:16:10] [config] lr-warmup-at-reload: false
[2019-07-24 23:16:10] [config] lr-warmup-cycle: false
[2019-07-24 23:16:10] [config] lr-warmup-start-rate: 0
[2019-07-24 23:16:10] [config] max-length: 50
[2019-07-24 23:16:10] [config] max-length-crop: false
[2019-07-24 23:16:10] [config] max-length-factor: 3
[2019-07-24 23:16:10] [config] maxi-batch: 100
[2019-07-24 23:16:10] [config] maxi-batch-sort: trg
[2019-07-24 23:16:10] [config] mini-batch: 64
[2019-07-24 23:16:10] [config] mini-batch-fit: true
[2019-07-24 23:16:10] [config] mini-batch-fit-step: 10
[2019-07-24 23:16:10] [config] mini-batch-overstuff: 1
[2019-07-24 23:16:10] [config] mini-batch-track-lr: false
[2019-07-24 23:16:10] [config] mini-batch-understuff: 1
[2019-07-24 23:16:10] [config] mini-batch-warmup: 0
[2019-07-24 23:16:10] [config] mini-batch-words: 0
[2019-07-24 23:16:10] [config] mini-batch-words-ref: 0
[2019-07-24 23:16:10] [config] model: ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-24 23:16:10] [config] multi-loss-type: sum
[2019-07-24 23:16:10] [config] multi-node: false
[2019-07-24 23:16:10] [config] multi-node-overlap: true
[2019-07-24 23:16:10] [config] n-best: false
[2019-07-24 23:16:10] [config] no-nccl: false
[2019-07-24 23:16:10] [config] no-reload: false
[2019-07-24 23:16:10] [config] no-restore-corpus: false
[2019-07-24 23:16:10] [config] no-shuffle: false
[2019-07-24 23:16:10] [config] normalize: 1
[2019-07-24 23:16:10] [config] num-devices: 0
[2019-07-24 23:16:10] [config] optimizer: adam
[2019-07-24 23:16:10] [config] optimizer-delay: 1
[2019-07-24 23:16:10] [config] optimizer-params:
[2019-07-24 23:16:10] [config]   []
[2019-07-24 23:16:10] [config] overwrite: false
[2019-07-24 23:16:10] [config] pretrained-model: ""
[2019-07-24 23:16:10] [config] quiet: false
[2019-07-24 23:16:10] [config] quiet-translation: true
[2019-07-24 23:16:10] [config] relative-paths: false
[2019-07-24 23:16:10] [config] right-left: false
[2019-07-24 23:16:10] [config] save-freq: 20000
[2019-07-24 23:16:10] [config] seed: 1111
[2019-07-24 23:16:10] [config] shuffle-in-ram: false
[2019-07-24 23:16:10] [config] skip: false
[2019-07-24 23:16:10] [config] sqlite: ""
[2019-07-24 23:16:10] [config] sqlite-drop: false
[2019-07-24 23:16:10] [config] sync-sgd: true
[2019-07-24 23:16:10] [config] tempdir: .
[2019-07-24 23:16:10] [config] tied-embeddings: false
[2019-07-24 23:16:10] [config] tied-embeddings-all: false
[2019-07-24 23:16:10] [config] tied-embeddings-src: false
[2019-07-24 23:16:10] [config] train-sets:
[2019-07-24 23:16:10] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de
[2019-07-24 23:16:10] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en
[2019-07-24 23:16:10] [config] transformer-aan-activation: swish
[2019-07-24 23:16:10] [config] transformer-aan-depth: 2
[2019-07-24 23:16:10] [config] transformer-aan-nogate: false
[2019-07-24 23:16:10] [config] transformer-decoder-autoreg: self-attention
[2019-07-24 23:16:10] [config] transformer-dim-aan: 2048
[2019-07-24 23:16:10] [config] transformer-dim-ffn: 2048
[2019-07-24 23:16:10] [config] transformer-dropout: 0
[2019-07-24 23:16:10] [config] transformer-dropout-attention: 0
[2019-07-24 23:16:10] [config] transformer-dropout-ffn: 0
[2019-07-24 23:16:10] [config] transformer-ffn-activation: swish
[2019-07-24 23:16:10] [config] transformer-ffn-depth: 2
[2019-07-24 23:16:10] [config] transformer-guided-alignment-layer: last
[2019-07-24 23:16:10] [config] transformer-heads: 8
[2019-07-24 23:16:10] [config] transformer-no-projection: false
[2019-07-24 23:16:10] [config] transformer-postprocess: dan
[2019-07-24 23:16:10] [config] transformer-postprocess-emb: d
[2019-07-24 23:16:10] [config] transformer-preprocess: ""
[2019-07-24 23:16:10] [config] transformer-tied-layers:
[2019-07-24 23:16:10] [config]   []
[2019-07-24 23:16:10] [config] transformer-train-position-embeddings: false
[2019-07-24 23:16:10] [config] type: amun
[2019-07-24 23:16:10] [config] ulr: false
[2019-07-24 23:16:10] [config] ulr-dim-emb: 0
[2019-07-24 23:16:10] [config] ulr-dropout: 0
[2019-07-24 23:16:10] [config] ulr-keys-vectors: ""
[2019-07-24 23:16:10] [config] ulr-query-vectors: ""
[2019-07-24 23:16:10] [config] ulr-softmax-temperature: 1
[2019-07-24 23:16:10] [config] ulr-trainable-transformation: false
[2019-07-24 23:16:10] [config] valid-freq: 20000
[2019-07-24 23:16:10] [config] valid-log: ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-24 23:16:10] [config] valid-max-length: 1000
[2019-07-24 23:16:10] [config] valid-metrics:
[2019-07-24 23:16:10] [config]   - cross-entropy
[2019-07-24 23:16:10] [config]   - perplexity
[2019-07-24 23:16:10] [config]   - translation
[2019-07-24 23:16:10] [config] valid-mini-batch: 8
[2019-07-24 23:16:10] [config] valid-script-path: ../experiments/100M_bicleaner_st_lm/score-dev.sh
[2019-07-24 23:16:10] [config] valid-sets:
[2019-07-24 23:16:10] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de
[2019-07-24 23:16:10] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en
[2019-07-24 23:16:10] [config] valid-translation-output: ../experiments/100M_bicleaner_st_lm/model/dev.out
[2019-07-24 23:16:10] [config] version: v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-24 23:16:10] [config] vocabs:
[2019-07-24 23:16:10] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-24 23:16:10] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-24 23:16:10] [config] word-penalty: 0
[2019-07-24 23:16:10] [config] workspace: 5000
[2019-07-24 23:16:10] [config] Loaded model has been created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-24 23:16:10] Using synchronous training
[2019-07-24 23:16:10] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-24 23:16:10] [data] Using unused word id eos for 0
[2019-07-24 23:16:10] [data] Using unused word id UNK for 1
[2019-07-24 23:16:10] [data] Setting vocabulary size for input 0 to 50000
[2019-07-24 23:16:10] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-24 23:16:11] [data] Using unused word id eos for 0
[2019-07-24 23:16:11] [data] Using unused word id UNK for 1
[2019-07-24 23:16:11] [data] Setting vocabulary size for input 1 to 50000
[2019-07-24 23:16:11] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-24 23:16:11] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-24 23:16:12] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-24 23:16:13] [memory] Extending reserved space to 5120 MB (device gpu3)
[2019-07-24 23:16:13] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-24 23:16:13] [comm] NCCLCommunicator constructed successfully.
[2019-07-24 23:16:13] [training] Using 2 GPUs
[2019-07-24 23:16:13] [memory] Reserving 422 MB, device gpu2
[2019-07-24 23:16:13] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-24 23:16:14] [memory] Reserving 422 MB, device gpu2
[2019-07-24 23:16:23] [batching] Done. Typical MB size is 13760 target words
[2019-07-24 23:16:23] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-24 23:16:23] [memory] Extending reserved space to 5120 MB (device gpu3)
[2019-07-24 23:16:23] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-24 23:16:23] [comm] NCCLCommunicator constructed successfully.
[2019-07-24 23:16:23] [training] Using 2 GPUs
[2019-07-24 23:16:23] Loading model from ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-24 23:16:29] Loading model from ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-24 23:16:31] Loading Adam parameters from ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-24 23:16:41] [memory] Reserving 422 MB, device gpu2
[2019-07-24 23:16:41] [memory] Reserving 422 MB, device gpu3
[2019-07-24 23:16:42] [training] Model reloaded from ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-24 23:16:42] [data] Restoring the corpus state to epoch 5, batch 60000
[2019-07-24 23:16:42] [data] Shuffling data
[2019-07-24 23:16:55] [data] Done reading 4864128 sentences
[2019-07-24 23:17:14] [data] Done shuffling 4864128 sentences to temp files
[2019-07-24 23:18:45] Training started
[2019-07-24 23:18:45] [training] Batches are processed as 1 process(es) x 2 devices/process
[2019-07-24 23:18:45] [memory] Reserving 422 MB, device gpu2
[2019-07-24 23:18:45] [memory] Reserving 422 MB, device gpu3
[2019-07-24 23:18:45] [memory] Reserving 422 MB, device gpu3
[2019-07-24 23:18:45] [memory] Reserving 422 MB, device gpu2
[2019-07-24 23:18:45] Loading model from ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-24 23:18:51] [memory] Reserving 422 MB, device cpu0
[2019-07-24 23:18:51] [memory] Reserving 211 MB, device gpu2
[2019-07-24 23:18:51] [memory] Reserving 211 MB, device gpu3
[2019-07-24 23:35:45] Ep. 5 : Up. 62000 : Sen. 3,709,872 : Cost 46.19030380 : Time 1174.36s : 12779.59 words/s
[2019-07-24 23:48:57] Seen 4231167 samples
[2019-07-24 23:48:57] Starting epoch 6
[2019-07-24 23:48:57] [data] Shuffling data
[2019-07-24 23:49:00] [data] Done reading 4864128 sentences
[2019-07-24 23:49:18] [data] Done shuffling 4864128 sentences to temp files
[2019-07-24 23:53:03] Ep. 6 : Up. 64000 : Sen. 143,906 : Cost 45.93682480 : Time 1038.07s : 14403.69 words/s
[2019-07-25 00:09:56] Ep. 6 : Up. 66000 : Sen. 809,948 : Cost 45.05677414 : Time 1013.39s : 14784.55 words/s
[2019-07-25 00:26:54] Ep. 6 : Up. 68000 : Sen. 1,478,136 : Cost 45.00164795 : Time 1017.30s : 14752.21 words/s
[2019-07-25 00:43:46] Ep. 6 : Up. 70000 : Sen. 2,142,078 : Cost 45.07719421 : Time 1012.01s : 14740.71 words/s
[2019-07-25 01:00:41] Ep. 6 : Up. 72000 : Sen. 2,809,336 : Cost 45.05327225 : Time 1015.60s : 14780.07 words/s
[2019-07-25 01:17:36] Ep. 6 : Up. 74000 : Sen. 3,475,200 : Cost 44.84162140 : Time 1015.00s : 14725.68 words/s
[2019-07-25 01:34:24] Ep. 6 : Up. 76000 : Sen. 4,138,342 : Cost 44.66870880 : Time 1007.93s : 14759.94 words/s
[2019-07-25 01:36:47] Seen 4231167 samples
[2019-07-25 01:36:47] Starting epoch 7
[2019-07-25 01:36:47] [data] Shuffling data
[2019-07-25 01:36:50] [data] Done reading 4864128 sentences
[2019-07-25 01:37:09] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 01:51:42] Ep. 7 : Up. 78000 : Sen. 571,616 : Cost 44.12965012 : Time 1038.20s : 14402.13 words/s
[2019-07-25 02:08:35] Ep. 7 : Up. 80000 : Sen. 1,235,200 : Cost 43.93529129 : Time 1012.22s : 14722.09 words/s
[2019-07-25 02:08:35] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-25 02:08:44] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter80000.npz
[2019-07-25 02:08:50] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 02:09:00] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-25 02:09:21] [valid] Ep. 7 : Up. 80000 : cross-entropy : 42.9711 : new best
[2019-07-25 02:09:25] [valid] Ep. 7 : Up. 80000 : perplexity : 5.43281 : new best
[2019-07-25 02:10:09] [valid] Ep. 7 : Up. 80000 : translation : 29.9 : new best
[2019-07-25 02:27:07] Ep. 7 : Up. 82000 : Sen. 1,900,800 : Cost 43.80895615 : Time 1112.28s : 13435.83 words/s
[2019-07-25 02:44:01] Ep. 7 : Up. 84000 : Sen. 2,565,872 : Cost 44.05004120 : Time 1014.42s : 14736.86 words/s
[2019-07-25 03:00:54] Ep. 7 : Up. 86000 : Sen. 3,231,124 : Cost 44.04573822 : Time 1013.06s : 14750.62 words/s
[2019-07-25 03:17:51] Ep. 7 : Up. 88000 : Sen. 3,897,864 : Cost 43.91369247 : Time 1016.61s : 14745.80 words/s
[2019-07-25 03:26:19] Seen 4231167 samples
[2019-07-25 03:26:19] Starting epoch 8
[2019-07-25 03:26:19] [data] Shuffling data
[2019-07-25 03:26:22] [data] Done reading 4864128 sentences
[2019-07-25 03:26:42] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 03:35:16] Ep. 8 : Up. 90000 : Sen. 332,800 : Cost 43.45766830 : Time 1044.79s : 14337.48 words/s
[2019-07-25 03:52:13] Ep. 8 : Up. 92000 : Sen. 999,958 : Cost 42.96876907 : Time 1017.58s : 14724.63 words/s
[2019-07-25 04:09:11] Ep. 8 : Up. 94000 : Sen. 1,665,574 : Cost 43.07009888 : Time 1017.26s : 14701.66 words/s
[2019-07-25 04:26:09] Ep. 8 : Up. 96000 : Sen. 2,331,966 : Cost 43.08716202 : Time 1018.57s : 14694.47 words/s
[2019-07-25 04:43:07] Ep. 8 : Up. 98000 : Sen. 2,997,524 : Cost 43.20823669 : Time 1017.48s : 14708.32 words/s
[2019-07-25 05:00:00] Ep. 8 : Up. 100000 : Sen. 3,661,064 : Cost 43.24500275 : Time 1013.51s : 14703.47 words/s
[2019-07-25 05:00:00] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-25 05:00:09] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter100000.npz
[2019-07-25 05:00:16] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 05:00:26] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-25 05:00:47] [valid] Ep. 8 : Up. 100000 : cross-entropy : 41.9128 : new best
[2019-07-25 05:00:51] [valid] Ep. 8 : Up. 100000 : perplexity : 5.21101 : new best
[2019-07-25 05:01:34] [valid] Ep. 8 : Up. 100000 : translation : 30.3 : new best
[2019-07-25 05:16:12] Seen 4231167 samples
[2019-07-25 05:16:12] Starting epoch 9
[2019-07-25 05:16:12] [data] Shuffling data
[2019-07-25 05:16:14] [data] Done reading 4864128 sentences
[2019-07-25 05:16:34] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 05:18:56] Ep. 9 : Up. 102000 : Sen. 87,830 : Cost 42.95087814 : Time 1135.96s : 13029.01 words/s
[2019-07-25 05:35:54] Ep. 9 : Up. 104000 : Sen. 754,852 : Cost 42.20893097 : Time 1017.63s : 14719.76 words/s
[2019-07-25 05:52:50] Ep. 9 : Up. 106000 : Sen. 1,419,268 : Cost 42.31576538 : Time 1015.98s : 14700.85 words/s
[2019-07-25 06:09:49] Ep. 9 : Up. 108000 : Sen. 2,084,488 : Cost 42.24754715 : Time 1018.98s : 14666.41 words/s
[2019-07-25 06:26:46] Ep. 9 : Up. 110000 : Sen. 2,751,388 : Cost 42.32112885 : Time 1016.92s : 14719.14 words/s
[2019-07-25 06:43:45] Ep. 9 : Up. 112000 : Sen. 3,418,476 : Cost 42.39097977 : Time 1019.11s : 14712.69 words/s
[2019-07-25 07:00:42] Ep. 9 : Up. 114000 : Sen. 4,083,200 : Cost 42.51019669 : Time 1016.64s : 14695.09 words/s
[2019-07-25 07:04:27] Seen 4231167 samples
[2019-07-25 07:04:27] Starting epoch 10
[2019-07-25 07:04:27] [data] Shuffling data
[2019-07-25 07:04:30] [data] Done reading 4864128 sentences
[2019-07-25 07:04:51] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 07:18:05] Ep. 10 : Up. 116000 : Sen. 516,912 : Cost 41.73200989 : Time 1043.23s : 14308.76 words/s
[2019-07-25 07:34:59] Ep. 10 : Up. 118000 : Sen. 1,181,205 : Cost 41.68493271 : Time 1014.15s : 14712.14 words/s
[2019-07-25 07:51:57] Ep. 10 : Up. 120000 : Sen. 1,847,830 : Cost 41.81047440 : Time 1018.13s : 14728.32 words/s
[2019-07-25 07:51:57] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-25 07:52:07] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter120000.npz
[2019-07-25 07:52:13] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 07:52:23] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-25 07:52:45] [valid] Ep. 10 : Up. 120000 : cross-entropy : 41.1128 : new best
[2019-07-25 07:52:48] [valid] Ep. 10 : Up. 120000 : perplexity : 5.04937 : new best
[2019-07-25 07:53:32] [valid] Ep. 10 : Up. 120000 : translation : 30.65 : new best
[2019-07-25 08:10:32] Ep. 10 : Up. 122000 : Sen. 2,515,728 : Cost 41.67569733 : Time 1115.05s : 13452.83 words/s
[2019-07-25 08:27:26] Ep. 10 : Up. 124000 : Sen. 3,179,345 : Cost 41.70742798 : Time 1013.93s : 14702.99 words/s
[2019-07-25 08:44:20] Ep. 10 : Up. 126000 : Sen. 3,843,590 : Cost 41.66932297 : Time 1013.70s : 14721.67 words/s
[2019-07-25 08:54:13] Seen 4231167 samples
[2019-07-25 08:54:13] Starting epoch 11
[2019-07-25 08:54:13] [data] Shuffling data
[2019-07-25 08:54:15] [data] Done reading 4864128 sentences
[2019-07-25 08:54:36] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 09:01:50] Ep. 11 : Up. 128000 : Sen. 278,374 : Cost 41.50988770 : Time 1050.58s : 14255.95 words/s
[2019-07-25 09:18:44] Ep. 11 : Up. 130000 : Sen. 941,148 : Cost 41.03034973 : Time 1013.43s : 14702.40 words/s
[2019-07-25 09:35:40] Ep. 11 : Up. 132000 : Sen. 1,606,136 : Cost 41.26104736 : Time 1016.07s : 14712.35 words/s
[2019-07-25 09:52:36] Ep. 11 : Up. 134000 : Sen. 2,272,876 : Cost 41.31508255 : Time 1016.21s : 14748.16 words/s
[2019-07-25 10:09:33] Ep. 11 : Up. 136000 : Sen. 2,939,464 : Cost 41.18839645 : Time 1017.33s : 14719.50 words/s
[2019-07-25 10:26:32] Ep. 11 : Up. 138000 : Sen. 3,606,272 : Cost 41.25178528 : Time 1018.19s : 14734.34 words/s
[2019-07-25 10:42:25] Seen 4231167 samples
[2019-07-25 10:42:25] Starting epoch 12
[2019-07-25 10:42:25] [data] Shuffling data
[2019-07-25 10:42:28] [data] Done reading 4864128 sentences
[2019-07-25 10:42:49] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 10:43:57] Ep. 12 : Up. 140000 : Sen. 40,959 : Cost 41.24794006 : Time 1045.03s : 14280.27 words/s
[2019-07-25 10:43:57] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-25 10:44:06] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter140000.npz
[2019-07-25 10:44:13] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 10:44:23] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-25 10:44:44] [valid] Ep. 12 : Up. 140000 : cross-entropy : 40.502 : new best
[2019-07-25 10:44:48] [valid] Ep. 12 : Up. 140000 : perplexity : 4.92934 : new best
[2019-07-25 10:45:32] [valid] Ep. 12 : Up. 140000 : translation : 30.86 : new best
[2019-07-25 11:02:29] Ep. 12 : Up. 142000 : Sen. 705,448 : Cost 40.53347778 : Time 1112.33s : 13421.59 words/s
[2019-07-25 11:19:26] Ep. 12 : Up. 144000 : Sen. 1,370,060 : Cost 40.67005157 : Time 1016.86s : 14668.51 words/s
[2019-07-25 11:36:25] Ep. 12 : Up. 146000 : Sen. 2,035,200 : Cost 40.67688370 : Time 1019.26s : 14664.60 words/s
[2019-07-25 11:53:21] Ep. 12 : Up. 148000 : Sen. 2,698,850 : Cost 40.82013321 : Time 1016.26s : 14681.06 words/s
[2019-07-25 12:10:21] Ep. 12 : Up. 150000 : Sen. 3,366,136 : Cost 40.74104691 : Time 1020.14s : 14698.88 words/s
[2019-07-25 12:27:25] Ep. 12 : Up. 152000 : Sen. 4,035,438 : Cost 40.94536972 : Time 1023.39s : 14711.87 words/s
[2019-07-25 12:32:24] Seen 4231167 samples
[2019-07-25 12:32:24] Starting epoch 13
[2019-07-25 12:32:24] [data] Shuffling data
[2019-07-25 12:32:26] [data] Done reading 4864128 sentences
[2019-07-25 12:32:47] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 12:44:53] Ep. 13 : Up. 154000 : Sen. 472,376 : Cost 40.25251007 : Time 1048.02s : 14307.29 words/s
[2019-07-25 13:01:54] Ep. 13 : Up. 156000 : Sen. 1,136,463 : Cost 40.22795105 : Time 1020.71s : 14640.72 words/s
[2019-07-25 13:18:52] Ep. 13 : Up. 158000 : Sen. 1,801,558 : Cost 40.12947464 : Time 1018.31s : 14656.65 words/s
[2019-07-25 13:35:52] Ep. 13 : Up. 160000 : Sen. 2,466,408 : Cost 40.37662506 : Time 1019.87s : 14664.52 words/s
[2019-07-25 13:35:52] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-25 13:36:01] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter160000.npz
[2019-07-25 13:36:07] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 13:36:17] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-25 13:36:38] [valid] Ep. 13 : Up. 160000 : cross-entropy : 40.101 : new best
[2019-07-25 13:36:42] [valid] Ep. 13 : Up. 160000 : perplexity : 4.8521 : new best
[2019-07-25 13:37:27] [valid] Ep. 13 : Up. 160000 : translation : 30.95 : new best
[2019-07-25 13:54:30] Ep. 13 : Up. 162000 : Sen. 3,131,618 : Cost 40.39156342 : Time 1118.57s : 13361.96 words/s

[CALL STACK]
[0x727f12]                                                            
[0x728985]          marian::data::Corpus::  next  ()                   + 0x6f5
[0x716e4f]          marian::data::CorpusIterator::  increment  ()      + 0x2f
[0x681a7d]          marian::data::BatchGenerator<marian::data::CorpusBase>::  fetchBatches  () + 0x10dd
[0x682adb]          std::future<std::result_of<marian::data::BatchGenerator<marian::data::CorpusBase>::fetchBatchesAsync()::{lambda()#1} ()>::type> marian::ThreadPool::enqueue<marian::data::BatchGenerator<marian::data::CorpusBase>::fetchBatchesAsync()::{lambda()#1}>(std::result_of&&,(marian::data::BatchGenerator<marian::data::CorpusBase>::fetchBatchesAsync()::{lambda()#1}&&)...)::{lambda()#1}::  operator()  () const + 0x2b
[0x6834be]          std::_Function_handler<std::unique_ptr<std::__future_base::_Result_base,std::__future_base::_Result_base::_Deleter> (),std::__future_base::_Task_setter<std::unique_ptr<std::__future_base::_Result<std::deque<std::shared_ptr<marian::data::CorpusBatch>,std::allocator<std::shared_ptr<marian::data::CorpusBatch>>>>,std::__future_base::_Result_base::_Deleter>,std::_Bind_simple<std::reference_wrapper<std::future<std::result_of<marian::data::BatchGenerator<marian::data::CorpusBase>::fetchBatchesAsync()::{lambda()#1} ()>::type> marian::ThreadPool::enqueue<marian::data::BatchGenerator<marian::data::CorpusBase>::fetchBatchesAsync()::{lambda()#1}>(std::result_of&&,(marian::data::BatchGenerator<marian::data::CorpusBase>::fetchBatchesAsync()::{lambda()#1}&&)...)::{lambda()#1}> ()>,std::deque<std::shared_ptr<marian::data::CorpusBatch>,std::allocator<std::shared_ptr<marian::data::CorpusBatch>>>>>::  _M_invoke  (std::_Any_data const&) + 0x3e
[0x5a5f19]          std::__future_base::_State_baseV2::  _M_do_set  (std::function<std::unique_ptr<std::__future_base::_Result_base,std::__future_base::_Result_base::_Deleter> ()>*,  bool*) + 0x29
[0x7f9ad937ca99]                                                       + 0xea99
[0x59fac2]                                                            
[0x5a7341]          std::__future_base::_Task_state<std::future<std::result_of<marian::data::BatchGenerator<marian::data::CorpusBase>::fetchBatchesAsync()::{lambda()#1} ()>::type> marian::ThreadPool::enqueue<marian::data::BatchGenerator<marian::data::CorpusBase>::fetchBatchesAsync()::{lambda()#1}>(std::result_of&&,(marian::data::BatchGenerator<marian::data::CorpusBase>::fetchBatchesAsync()::{lambda()#1}&&)...)::{lambda()#1},std::allocator<int>,std::deque<std::shared_ptr<marian::data::CorpusBatch>,std::allocator<std::shared_ptr>> ()>::  _M_run  () + 0x51
[0x5d00b4]          std::thread::_Impl<std::_Bind_simple<marian::ThreadPool::reserve(unsigned long)::{lambda()#1} ()>>::  _M_run  () + 0x174
[0x7f9ad8e9cc80]                                                       + 0xb8c80
[0x7f9ad93756ba]                                                       + 0x76ba
[0x7f9ad860241d]    clone                                              + 0x6d

[2019-07-25 17:07:52] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-25 17:07:52] [marian] Running on hodor as process 8583 with command line:
[2019-07-25 17:07:52] [marian] /fs/bil0/abdel/marian-dev/build/marian --sync-sgd --model ../experiments/100M_bicleaner_st_lm/model/model.npz -T . --devices 3 --train-sets ../experiments/100M_bicleaner_st_lm/data/train.bpe.de ../experiments/100M_bicleaner_st_lm/data/train.bpe.en --vocabs ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json --mini-batch-fit -w 5000 --dim-vocabs 50000 50000 --layer-normalization --dropout-rnn 0.2 --dropout-src 0.1 --dropout-trg 0.1 --learn-rate 0.0001 --after-epochs 0 --early-stopping 5 --valid-freq 20000 --save-freq 20000 --disp-freq 2000 --valid-mini-batch 8 --valid-sets ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en --valid-metrics cross-entropy perplexity translation --valid-translation-output ../experiments/100M_bicleaner_st_lm/model/dev.out --valid-script-path ../experiments/100M_bicleaner_st_lm/score-dev.sh --seed 1111 --exponential-smoothing --normalize=1 --beam-size=12 --quiet-translation --log ../experiments/100M_bicleaner_st_lm/model/train.log --valid-log ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-25 17:07:52] [config] after-batches: 0
[2019-07-25 17:07:52] [config] after-epochs: 0
[2019-07-25 17:07:52] [config] allow-unk: false
[2019-07-25 17:07:52] [config] beam-size: 12
[2019-07-25 17:07:52] [config] bert-class-symbol: "[CLS]"
[2019-07-25 17:07:52] [config] bert-mask-symbol: "[MASK]"
[2019-07-25 17:07:52] [config] bert-masking-fraction: 0.15
[2019-07-25 17:07:52] [config] bert-sep-symbol: "[SEP]"
[2019-07-25 17:07:52] [config] bert-train-type-embeddings: true
[2019-07-25 17:07:52] [config] bert-type-vocab-size: 2
[2019-07-25 17:07:52] [config] best-deep: false
[2019-07-25 17:07:52] [config] clip-gemm: 0
[2019-07-25 17:07:52] [config] clip-norm: 1
[2019-07-25 17:07:52] [config] cost-type: ce-mean
[2019-07-25 17:07:52] [config] cpu-threads: 0
[2019-07-25 17:07:52] [config] data-weighting: ""
[2019-07-25 17:07:52] [config] data-weighting-type: sentence
[2019-07-25 17:07:52] [config] dec-cell: gru
[2019-07-25 17:07:52] [config] dec-cell-base-depth: 2
[2019-07-25 17:07:52] [config] dec-cell-high-depth: 1
[2019-07-25 17:07:52] [config] dec-depth: 1
[2019-07-25 17:07:52] [config] devices:
[2019-07-25 17:07:52] [config]   - 3
[2019-07-25 17:07:52] [config] dim-emb: 512
[2019-07-25 17:07:52] [config] dim-rnn: 1024
[2019-07-25 17:07:52] [config] dim-vocabs:
[2019-07-25 17:07:52] [config]   - 50000
[2019-07-25 17:07:52] [config]   - 50000
[2019-07-25 17:07:52] [config] disp-first: 0
[2019-07-25 17:07:52] [config] disp-freq: 2000
[2019-07-25 17:07:52] [config] disp-label-counts: false
[2019-07-25 17:07:52] [config] dropout-rnn: 0.2
[2019-07-25 17:07:52] [config] dropout-src: 0.1
[2019-07-25 17:07:52] [config] dropout-trg: 0.1
[2019-07-25 17:07:52] [config] dump-config: ""
[2019-07-25 17:07:52] [config] early-stopping: 5
[2019-07-25 17:07:52] [config] embedding-fix-src: false
[2019-07-25 17:07:52] [config] embedding-fix-trg: false
[2019-07-25 17:07:52] [config] embedding-normalization: false
[2019-07-25 17:07:52] [config] embedding-vectors:
[2019-07-25 17:07:52] [config]   []
[2019-07-25 17:07:52] [config] enc-cell: gru
[2019-07-25 17:07:52] [config] enc-cell-depth: 1
[2019-07-25 17:07:52] [config] enc-depth: 1
[2019-07-25 17:07:52] [config] enc-type: bidirectional
[2019-07-25 17:07:52] [config] exponential-smoothing: 0.0001
[2019-07-25 17:07:52] [config] grad-dropping-momentum: 0
[2019-07-25 17:07:52] [config] grad-dropping-rate: 0
[2019-07-25 17:07:52] [config] grad-dropping-warmup: 100
[2019-07-25 17:07:52] [config] guided-alignment: none
[2019-07-25 17:07:52] [config] guided-alignment-cost: mse
[2019-07-25 17:07:52] [config] guided-alignment-weight: 0.1
[2019-07-25 17:07:52] [config] ignore-model-config: false
[2019-07-25 17:07:52] [config] input-types:
[2019-07-25 17:07:52] [config]   []
[2019-07-25 17:07:52] [config] interpolate-env-vars: false
[2019-07-25 17:07:52] [config] keep-best: false
[2019-07-25 17:07:52] [config] label-smoothing: 0
[2019-07-25 17:07:52] [config] layer-normalization: true
[2019-07-25 17:07:52] [config] learn-rate: 0.0001
[2019-07-25 17:07:52] [config] log: ../experiments/100M_bicleaner_st_lm/model/train.log
[2019-07-25 17:07:52] [config] log-level: info
[2019-07-25 17:07:52] [config] log-time-zone: ""
[2019-07-25 17:07:52] [config] lr-decay: 0
[2019-07-25 17:07:52] [config] lr-decay-freq: 50000
[2019-07-25 17:07:52] [config] lr-decay-inv-sqrt:
[2019-07-25 17:07:52] [config]   - 0
[2019-07-25 17:07:52] [config] lr-decay-repeat-warmup: false
[2019-07-25 17:07:52] [config] lr-decay-reset-optimizer: false
[2019-07-25 17:07:52] [config] lr-decay-start:
[2019-07-25 17:07:52] [config]   - 10
[2019-07-25 17:07:52] [config]   - 1
[2019-07-25 17:07:52] [config] lr-decay-strategy: epoch+stalled
[2019-07-25 17:07:52] [config] lr-report: false
[2019-07-25 17:07:52] [config] lr-warmup: 0
[2019-07-25 17:07:52] [config] lr-warmup-at-reload: false
[2019-07-25 17:07:52] [config] lr-warmup-cycle: false
[2019-07-25 17:07:52] [config] lr-warmup-start-rate: 0
[2019-07-25 17:07:52] [config] max-length: 50
[2019-07-25 17:07:52] [config] max-length-crop: false
[2019-07-25 17:07:52] [config] max-length-factor: 3
[2019-07-25 17:07:52] [config] maxi-batch: 100
[2019-07-25 17:07:52] [config] maxi-batch-sort: trg
[2019-07-25 17:07:52] [config] mini-batch: 64
[2019-07-25 17:07:52] [config] mini-batch-fit: true
[2019-07-25 17:07:52] [config] mini-batch-fit-step: 10
[2019-07-25 17:07:52] [config] mini-batch-overstuff: 1
[2019-07-25 17:07:52] [config] mini-batch-track-lr: false
[2019-07-25 17:07:52] [config] mini-batch-understuff: 1
[2019-07-25 17:07:52] [config] mini-batch-warmup: 0
[2019-07-25 17:07:52] [config] mini-batch-words: 0
[2019-07-25 17:07:52] [config] mini-batch-words-ref: 0
[2019-07-25 17:07:52] [config] model: ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 17:07:52] [config] multi-loss-type: sum
[2019-07-25 17:07:52] [config] multi-node: false
[2019-07-25 17:07:52] [config] multi-node-overlap: true
[2019-07-25 17:07:52] [config] n-best: false
[2019-07-25 17:07:52] [config] no-nccl: false
[2019-07-25 17:07:52] [config] no-reload: false
[2019-07-25 17:07:52] [config] no-restore-corpus: false
[2019-07-25 17:07:52] [config] no-shuffle: false
[2019-07-25 17:07:52] [config] normalize: 1
[2019-07-25 17:07:52] [config] num-devices: 0
[2019-07-25 17:07:52] [config] optimizer: adam
[2019-07-25 17:07:52] [config] optimizer-delay: 1
[2019-07-25 17:07:52] [config] optimizer-params:
[2019-07-25 17:07:52] [config]   []
[2019-07-25 17:07:52] [config] overwrite: false
[2019-07-25 17:07:52] [config] pretrained-model: ""
[2019-07-25 17:07:52] [config] quiet: false
[2019-07-25 17:07:52] [config] quiet-translation: true
[2019-07-25 17:07:52] [config] relative-paths: false
[2019-07-25 17:07:52] [config] right-left: false
[2019-07-25 17:07:52] [config] save-freq: 20000
[2019-07-25 17:07:52] [config] seed: 1111
[2019-07-25 17:07:52] [config] shuffle-in-ram: false
[2019-07-25 17:07:52] [config] skip: false
[2019-07-25 17:07:52] [config] sqlite: ""
[2019-07-25 17:07:52] [config] sqlite-drop: false
[2019-07-25 17:07:52] [config] sync-sgd: true
[2019-07-25 17:07:52] [config] tempdir: .
[2019-07-25 17:07:52] [config] tied-embeddings: false
[2019-07-25 17:07:52] [config] tied-embeddings-all: false
[2019-07-25 17:07:52] [config] tied-embeddings-src: false
[2019-07-25 17:07:52] [config] train-sets:
[2019-07-25 17:07:52] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de
[2019-07-25 17:07:52] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en
[2019-07-25 17:07:52] [config] transformer-aan-activation: swish
[2019-07-25 17:07:52] [config] transformer-aan-depth: 2
[2019-07-25 17:07:52] [config] transformer-aan-nogate: false
[2019-07-25 17:07:52] [config] transformer-decoder-autoreg: self-attention
[2019-07-25 17:07:52] [config] transformer-dim-aan: 2048
[2019-07-25 17:07:52] [config] transformer-dim-ffn: 2048
[2019-07-25 17:07:52] [config] transformer-dropout: 0
[2019-07-25 17:07:52] [config] transformer-dropout-attention: 0
[2019-07-25 17:07:52] [config] transformer-dropout-ffn: 0
[2019-07-25 17:07:52] [config] transformer-ffn-activation: swish
[2019-07-25 17:07:52] [config] transformer-ffn-depth: 2
[2019-07-25 17:07:52] [config] transformer-guided-alignment-layer: last
[2019-07-25 17:07:52] [config] transformer-heads: 8
[2019-07-25 17:07:52] [config] transformer-no-projection: false
[2019-07-25 17:07:52] [config] transformer-postprocess: dan
[2019-07-25 17:07:52] [config] transformer-postprocess-emb: d
[2019-07-25 17:07:52] [config] transformer-preprocess: ""
[2019-07-25 17:07:52] [config] transformer-tied-layers:
[2019-07-25 17:07:52] [config]   []
[2019-07-25 17:07:52] [config] transformer-train-position-embeddings: false
[2019-07-25 17:07:52] [config] type: amun
[2019-07-25 17:07:52] [config] ulr: false
[2019-07-25 17:07:52] [config] ulr-dim-emb: 0
[2019-07-25 17:07:52] [config] ulr-dropout: 0
[2019-07-25 17:07:52] [config] ulr-keys-vectors: ""
[2019-07-25 17:07:52] [config] ulr-query-vectors: ""
[2019-07-25 17:07:52] [config] ulr-softmax-temperature: 1
[2019-07-25 17:07:52] [config] ulr-trainable-transformation: false
[2019-07-25 17:07:52] [config] valid-freq: 20000
[2019-07-25 17:07:52] [config] valid-log: ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-25 17:07:52] [config] valid-max-length: 1000
[2019-07-25 17:07:52] [config] valid-metrics:
[2019-07-25 17:07:52] [config]   - cross-entropy
[2019-07-25 17:07:52] [config]   - perplexity
[2019-07-25 17:07:52] [config]   - translation
[2019-07-25 17:07:52] [config] valid-mini-batch: 8
[2019-07-25 17:07:52] [config] valid-script-path: ../experiments/100M_bicleaner_st_lm/score-dev.sh
[2019-07-25 17:07:52] [config] valid-sets:
[2019-07-25 17:07:52] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de
[2019-07-25 17:07:52] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en
[2019-07-25 17:07:52] [config] valid-translation-output: ../experiments/100M_bicleaner_st_lm/model/dev.out
[2019-07-25 17:07:52] [config] version: v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-25 17:07:52] [config] vocabs:
[2019-07-25 17:07:52] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-25 17:07:52] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-25 17:07:52] [config] word-penalty: 0
[2019-07-25 17:07:52] [config] workspace: 5000
[2019-07-25 17:07:52] [config] Loaded model has been created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-25 17:07:52] Using synchronous training
[2019-07-25 17:07:52] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-25 17:07:52] [data] Using unused word id eos for 0
[2019-07-25 17:07:52] [data] Using unused word id UNK for 1
[2019-07-25 17:07:52] [data] Setting vocabulary size for input 0 to 50000
[2019-07-25 17:07:52] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-25 17:07:53] [data] Using unused word id eos for 0
[2019-07-25 17:07:53] [data] Using unused word id UNK for 1
[2019-07-25 17:07:53] [data] Setting vocabulary size for input 1 to 50000
[2019-07-25 17:07:53] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-25 17:07:53] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-25 17:07:54] [memory] Extending reserved space to 5120 MB (device gpu3)
[2019-07-25 17:07:54] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-25 17:07:54] [comm] NCCLCommunicator constructed successfully.
[2019-07-25 17:07:54] [training] Using 1 GPUs
[2019-07-25 17:07:54] [memory] Reserving 422 MB, device gpu3
[2019-07-25 17:07:54] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-25 17:07:54] [memory] Reserving 422 MB, device gpu3
[2019-07-25 17:08:03] [batching] Done. Typical MB size is 6880 target words
[2019-07-25 17:08:03] [memory] Extending reserved space to 5120 MB (device gpu3)
[2019-07-25 17:08:03] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-25 17:08:03] [comm] NCCLCommunicator constructed successfully.
[2019-07-25 17:08:03] [training] Using 1 GPUs
[2019-07-25 17:08:03] Loading model from ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-25 17:08:10] Loading Adam parameters from ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-25 17:08:21] [memory] Reserving 844 MB, device gpu3
[2019-07-25 17:08:22] [training] Model reloaded from ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 17:08:22] [data] Restoring the corpus state to epoch 13, batch 160000
[2019-07-25 17:08:22] [data] Shuffling data
[2019-07-25 17:08:35] [data] Done reading 4864128 sentences
[2019-07-25 17:09:02] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 17:09:49] Training started
[2019-07-25 17:09:49] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-07-25 17:09:49] [memory] Reserving 422 MB, device gpu3
[2019-07-25 17:09:50] [memory] Reserving 422 MB, device gpu3
[2019-07-25 17:09:50] Loading model from ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 17:09:56] [memory] Reserving 422 MB, device cpu0
[2019-07-25 17:09:56] [memory] Reserving 422 MB, device gpu3
[2019-07-25 17:24:52] Ep. 13 : Up. 162000 : Sen. 2,813,955 : Cost 39.49576950 : Time 1018.89s : 7683.33 words/s
[2019-07-25 20:29:59] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-25 20:29:59] [marian] Running on bil as process 2561 with command line:
[2019-07-25 20:29:59] [marian] /fs/bil0/abdel/marian-dev/build/marian --sync-sgd --model ../experiments/100M_bicleaner_st_lm/model/model.npz -T . --devices 2 --train-sets ../experiments/100M_bicleaner_st_lm/data/train.bpe.de ../experiments/100M_bicleaner_st_lm/data/train.bpe.en --vocabs ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json --mini-batch-fit -w 5000 --dim-vocabs 50000 50000 --layer-normalization --dropout-rnn 0.2 --dropout-src 0.1 --dropout-trg 0.1 --learn-rate 0.0001 --after-epochs 0 --early-stopping 5 --valid-freq 20000 --save-freq 20000 --disp-freq 2000 --valid-mini-batch 8 --valid-sets ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en --valid-metrics cross-entropy perplexity translation --valid-translation-output ../experiments/100M_bicleaner_st_lm/model/dev.out --valid-script-path ../experiments/100M_bicleaner_st_lm/score-dev.sh --seed 1111 --exponential-smoothing --normalize=1 --beam-size=12 --quiet-translation --log ../experiments/100M_bicleaner_st_lm/model/train.log --valid-log ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-25 20:29:59] [config] after-batches: 0
[2019-07-25 20:29:59] [config] after-epochs: 0
[2019-07-25 20:29:59] [config] allow-unk: false
[2019-07-25 20:29:59] [config] beam-size: 12
[2019-07-25 20:29:59] [config] bert-class-symbol: "[CLS]"
[2019-07-25 20:29:59] [config] bert-mask-symbol: "[MASK]"
[2019-07-25 20:29:59] [config] bert-masking-fraction: 0.15
[2019-07-25 20:29:59] [config] bert-sep-symbol: "[SEP]"
[2019-07-25 20:29:59] [config] bert-train-type-embeddings: true
[2019-07-25 20:29:59] [config] bert-type-vocab-size: 2
[2019-07-25 20:29:59] [config] best-deep: false
[2019-07-25 20:29:59] [config] clip-gemm: 0
[2019-07-25 20:29:59] [config] clip-norm: 1
[2019-07-25 20:29:59] [config] cost-type: ce-mean
[2019-07-25 20:29:59] [config] cpu-threads: 0
[2019-07-25 20:29:59] [config] data-weighting: ""
[2019-07-25 20:29:59] [config] data-weighting-type: sentence
[2019-07-25 20:29:59] [config] dec-cell: gru
[2019-07-25 20:29:59] [config] dec-cell-base-depth: 2
[2019-07-25 20:29:59] [config] dec-cell-high-depth: 1
[2019-07-25 20:29:59] [config] dec-depth: 1
[2019-07-25 20:29:59] [config] devices:
[2019-07-25 20:29:59] [config]   - 2
[2019-07-25 20:29:59] [config] dim-emb: 512
[2019-07-25 20:29:59] [config] dim-rnn: 1024
[2019-07-25 20:29:59] [config] dim-vocabs:
[2019-07-25 20:29:59] [config]   - 50000
[2019-07-25 20:29:59] [config]   - 50000
[2019-07-25 20:29:59] [config] disp-first: 0
[2019-07-25 20:29:59] [config] disp-freq: 2000
[2019-07-25 20:29:59] [config] disp-label-counts: false
[2019-07-25 20:29:59] [config] dropout-rnn: 0.2
[2019-07-25 20:29:59] [config] dropout-src: 0.1
[2019-07-25 20:29:59] [config] dropout-trg: 0.1
[2019-07-25 20:29:59] [config] dump-config: ""
[2019-07-25 20:29:59] [config] early-stopping: 5
[2019-07-25 20:29:59] [config] embedding-fix-src: false
[2019-07-25 20:29:59] [config] embedding-fix-trg: false
[2019-07-25 20:29:59] [config] embedding-normalization: false
[2019-07-25 20:29:59] [config] embedding-vectors:
[2019-07-25 20:29:59] [config]   []
[2019-07-25 20:29:59] [config] enc-cell: gru
[2019-07-25 20:29:59] [config] enc-cell-depth: 1
[2019-07-25 20:29:59] [config] enc-depth: 1
[2019-07-25 20:29:59] [config] enc-type: bidirectional
[2019-07-25 20:29:59] [config] exponential-smoothing: 0.0001
[2019-07-25 20:29:59] [config] grad-dropping-momentum: 0
[2019-07-25 20:29:59] [config] grad-dropping-rate: 0
[2019-07-25 20:29:59] [config] grad-dropping-warmup: 100
[2019-07-25 20:29:59] [config] guided-alignment: none
[2019-07-25 20:29:59] [config] guided-alignment-cost: mse
[2019-07-25 20:29:59] [config] guided-alignment-weight: 0.1
[2019-07-25 20:29:59] [config] ignore-model-config: false
[2019-07-25 20:29:59] [config] input-types:
[2019-07-25 20:29:59] [config]   []
[2019-07-25 20:29:59] [config] interpolate-env-vars: false
[2019-07-25 20:29:59] [config] keep-best: false
[2019-07-25 20:29:59] [config] label-smoothing: 0
[2019-07-25 20:29:59] [config] layer-normalization: true
[2019-07-25 20:29:59] [config] learn-rate: 0.0001
[2019-07-25 20:29:59] [config] log: ../experiments/100M_bicleaner_st_lm/model/train.log
[2019-07-25 20:29:59] [config] log-level: info
[2019-07-25 20:29:59] [config] log-time-zone: ""
[2019-07-25 20:29:59] [config] lr-decay: 0
[2019-07-25 20:29:59] [config] lr-decay-freq: 50000
[2019-07-25 20:29:59] [config] lr-decay-inv-sqrt:
[2019-07-25 20:29:59] [config]   - 0
[2019-07-25 20:29:59] [config] lr-decay-repeat-warmup: false
[2019-07-25 20:29:59] [config] lr-decay-reset-optimizer: false
[2019-07-25 20:29:59] [config] lr-decay-start:
[2019-07-25 20:29:59] [config]   - 10
[2019-07-25 20:29:59] [config]   - 1
[2019-07-25 20:29:59] [config] lr-decay-strategy: epoch+stalled
[2019-07-25 20:29:59] [config] lr-report: false
[2019-07-25 20:29:59] [config] lr-warmup: 0
[2019-07-25 20:29:59] [config] lr-warmup-at-reload: false
[2019-07-25 20:29:59] [config] lr-warmup-cycle: false
[2019-07-25 20:29:59] [config] lr-warmup-start-rate: 0
[2019-07-25 20:29:59] [config] max-length: 50
[2019-07-25 20:29:59] [config] max-length-crop: false
[2019-07-25 20:29:59] [config] max-length-factor: 3
[2019-07-25 20:29:59] [config] maxi-batch: 100
[2019-07-25 20:29:59] [config] maxi-batch-sort: trg
[2019-07-25 20:29:59] [config] mini-batch: 64
[2019-07-25 20:29:59] [config] mini-batch-fit: true
[2019-07-25 20:29:59] [config] mini-batch-fit-step: 10
[2019-07-25 20:29:59] [config] mini-batch-overstuff: 1
[2019-07-25 20:29:59] [config] mini-batch-track-lr: false
[2019-07-25 20:29:59] [config] mini-batch-understuff: 1
[2019-07-25 20:29:59] [config] mini-batch-warmup: 0
[2019-07-25 20:29:59] [config] mini-batch-words: 0
[2019-07-25 20:29:59] [config] mini-batch-words-ref: 0
[2019-07-25 20:29:59] [config] model: ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 20:29:59] [config] multi-loss-type: sum
[2019-07-25 20:29:59] [config] multi-node: false
[2019-07-25 20:29:59] [config] multi-node-overlap: true
[2019-07-25 20:29:59] [config] n-best: false
[2019-07-25 20:29:59] [config] no-nccl: false
[2019-07-25 20:29:59] [config] no-reload: false
[2019-07-25 20:29:59] [config] no-restore-corpus: false
[2019-07-25 20:29:59] [config] no-shuffle: false
[2019-07-25 20:29:59] [config] normalize: 1
[2019-07-25 20:29:59] [config] num-devices: 0
[2019-07-25 20:29:59] [config] optimizer: adam
[2019-07-25 20:29:59] [config] optimizer-delay: 1
[2019-07-25 20:29:59] [config] optimizer-params:
[2019-07-25 20:29:59] [config]   []
[2019-07-25 20:29:59] [config] overwrite: false
[2019-07-25 20:29:59] [config] pretrained-model: ""
[2019-07-25 20:29:59] [config] quiet: false
[2019-07-25 20:29:59] [config] quiet-translation: true
[2019-07-25 20:29:59] [config] relative-paths: false
[2019-07-25 20:29:59] [config] right-left: false
[2019-07-25 20:29:59] [config] save-freq: 20000
[2019-07-25 20:29:59] [config] seed: 1111
[2019-07-25 20:29:59] [config] shuffle-in-ram: false
[2019-07-25 20:29:59] [config] skip: false
[2019-07-25 20:29:59] [config] sqlite: ""
[2019-07-25 20:29:59] [config] sqlite-drop: false
[2019-07-25 20:29:59] [config] sync-sgd: true
[2019-07-25 20:29:59] [config] tempdir: .
[2019-07-25 20:29:59] [config] tied-embeddings: false
[2019-07-25 20:29:59] [config] tied-embeddings-all: false
[2019-07-25 20:29:59] [config] tied-embeddings-src: false
[2019-07-25 20:29:59] [config] train-sets:
[2019-07-25 20:29:59] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de
[2019-07-25 20:29:59] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en
[2019-07-25 20:29:59] [config] transformer-aan-activation: swish
[2019-07-25 20:29:59] [config] transformer-aan-depth: 2
[2019-07-25 20:29:59] [config] transformer-aan-nogate: false
[2019-07-25 20:29:59] [config] transformer-decoder-autoreg: self-attention
[2019-07-25 20:29:59] [config] transformer-dim-aan: 2048
[2019-07-25 20:29:59] [config] transformer-dim-ffn: 2048
[2019-07-25 20:29:59] [config] transformer-dropout: 0
[2019-07-25 20:29:59] [config] transformer-dropout-attention: 0
[2019-07-25 20:29:59] [config] transformer-dropout-ffn: 0
[2019-07-25 20:29:59] [config] transformer-ffn-activation: swish
[2019-07-25 20:29:59] [config] transformer-ffn-depth: 2
[2019-07-25 20:29:59] [config] transformer-guided-alignment-layer: last
[2019-07-25 20:29:59] [config] transformer-heads: 8
[2019-07-25 20:29:59] [config] transformer-no-projection: false
[2019-07-25 20:29:59] [config] transformer-postprocess: dan
[2019-07-25 20:29:59] [config] transformer-postprocess-emb: d
[2019-07-25 20:29:59] [config] transformer-preprocess: ""
[2019-07-25 20:29:59] [config] transformer-tied-layers:
[2019-07-25 20:29:59] [config]   []
[2019-07-25 20:29:59] [config] transformer-train-position-embeddings: false
[2019-07-25 20:29:59] [config] type: amun
[2019-07-25 20:29:59] [config] ulr: false
[2019-07-25 20:29:59] [config] ulr-dim-emb: 0
[2019-07-25 20:29:59] [config] ulr-dropout: 0
[2019-07-25 20:29:59] [config] ulr-keys-vectors: ""
[2019-07-25 20:29:59] [config] ulr-query-vectors: ""
[2019-07-25 20:29:59] [config] ulr-softmax-temperature: 1
[2019-07-25 20:29:59] [config] ulr-trainable-transformation: false
[2019-07-25 20:29:59] [config] valid-freq: 20000
[2019-07-25 20:29:59] [config] valid-log: ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-25 20:29:59] [config] valid-max-length: 1000
[2019-07-25 20:29:59] [config] valid-metrics:
[2019-07-25 20:29:59] [config]   - cross-entropy
[2019-07-25 20:29:59] [config]   - perplexity
[2019-07-25 20:29:59] [config]   - translation
[2019-07-25 20:29:59] [config] valid-mini-batch: 8
[2019-07-25 20:29:59] [config] valid-script-path: ../experiments/100M_bicleaner_st_lm/score-dev.sh
[2019-07-25 20:29:59] [config] valid-sets:
[2019-07-25 20:29:59] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de
[2019-07-25 20:29:59] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en
[2019-07-25 20:29:59] [config] valid-translation-output: ../experiments/100M_bicleaner_st_lm/model/dev.out
[2019-07-25 20:29:59] [config] version: v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-25 20:29:59] [config] vocabs:
[2019-07-25 20:29:59] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-25 20:29:59] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-25 20:29:59] [config] word-penalty: 0
[2019-07-25 20:29:59] [config] workspace: 5000
[2019-07-25 20:29:59] [config] Loaded model has been created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-25 20:29:59] Using synchronous training
[2019-07-25 20:29:59] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-25 20:30:00] [data] Using unused word id eos for 0
[2019-07-25 20:30:00] [data] Using unused word id UNK for 1
[2019-07-25 20:30:00] [data] Setting vocabulary size for input 0 to 50000
[2019-07-25 20:30:00] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-25 20:30:00] [data] Using unused word id eos for 0
[2019-07-25 20:30:00] [data] Using unused word id UNK for 1
[2019-07-25 20:30:00] [data] Setting vocabulary size for input 1 to 50000
[2019-07-25 20:30:00] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-25 20:30:00] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-25 20:30:01] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-25 20:30:01] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-25 20:30:01] [comm] NCCLCommunicator constructed successfully.
[2019-07-25 20:30:01] [training] Using 1 GPUs
[2019-07-25 20:30:01] [memory] Reserving 422 MB, device gpu2
[2019-07-25 20:30:01] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-25 20:30:01] [memory] Reserving 422 MB, device gpu2
[2019-07-25 20:30:06] [batching] Done. Typical MB size is 6880 target words
[2019-07-25 20:30:06] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-25 20:30:06] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-25 20:30:06] [comm] NCCLCommunicator constructed successfully.
[2019-07-25 20:30:06] [training] Using 1 GPUs
[2019-07-25 20:30:06] Loading model from ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-25 20:30:09] Loading Adam parameters from ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-25 20:30:12] [memory] Reserving 844 MB, device gpu2
[2019-07-25 20:30:13] [training] Model reloaded from ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 20:30:13] [data] Restoring the corpus state to epoch 13, batch 160000
[2019-07-25 20:30:13] [data] Shuffling data
[2019-07-25 20:30:16] [data] Done reading 4864128 sentences
[2019-07-25 20:30:34] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 20:31:11] Training started
[2019-07-25 20:31:11] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-07-25 20:31:11] [memory] Reserving 422 MB, device gpu2
[2019-07-25 20:31:11] [memory] Reserving 422 MB, device gpu2
[2019-07-25 20:31:11] Loading model from ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 20:31:15] [memory] Reserving 422 MB, device cpu0
[2019-07-25 20:31:15] [memory] Reserving 422 MB, device gpu2
[2019-07-25 20:38:30] Ep. 13 : Up. 162000 : Sen. 2,813,955 : Cost 39.49621201 : Time 510.29s : 15341.07 words/s
[2019-07-25 20:45:44] Ep. 13 : Up. 164000 : Sen. 3,161,351 : Cost 39.74415970 : Time 433.83s : 17958.84 words/s
[2019-07-25 20:52:59] Ep. 13 : Up. 166000 : Sen. 3,508,391 : Cost 39.88103867 : Time 434.64s : 17929.92 words/s
[2019-07-25 21:00:15] Ep. 13 : Up. 168000 : Sen. 3,855,668 : Cost 40.65966415 : Time 436.10s : 17907.50 words/s
[2019-07-25 21:07:30] Ep. 13 : Up. 170000 : Sen. 4,203,613 : Cost 40.77803802 : Time 435.35s : 17938.21 words/s
[2019-07-25 21:14:45] Ep. 13 : Up. 172000 : Sen. 4,550,374 : Cost 41.03726959 : Time 434.84s : 17937.88 words/s
[2019-07-25 21:22:05] Ep. 13 : Up. 174000 : Sen. 4,897,455 : Cost 40.85453415 : Time 439.89s : 17695.96 words/s
[2019-07-25 21:29:26] Ep. 13 : Up. 176000 : Sen. 5,245,160 : Cost 41.03135300 : Time 440.80s : 17703.14 words/s
[2019-07-25 21:32:56] Seen 5410090 samples
[2019-07-25 21:32:56] Starting epoch 14
[2019-07-25 21:32:56] [data] Shuffling data
[2019-07-25 21:32:59] [data] Done reading 4864128 sentences
[2019-07-25 21:33:24] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 21:37:19] Ep. 14 : Up. 178000 : Sen. 183,685 : Cost 40.43215561 : Time 473.18s : 16574.54 words/s
[2019-07-25 21:44:42] Ep. 14 : Up. 180000 : Sen. 531,464 : Cost 39.88461685 : Time 443.29s : 17639.41 words/s
[2019-07-25 21:44:42] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-25 21:44:48] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter180000.npz
[2019-07-25 21:44:50] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 21:44:55] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-25 21:45:14] [valid] Ep. 14 : Up. 180000 : cross-entropy : 39.8892 : new best
[2019-07-25 21:45:21] [valid] Ep. 14 : Up. 180000 : perplexity : 4.81181 : new best
[2019-07-25 21:46:15] [valid] Ep. 14 : Up. 180000 : translation : 31.15 : new best
[2019-07-25 21:53:41] Ep. 14 : Up. 182000 : Sen. 879,625 : Cost 39.96612549 : Time 538.39s : 14527.38 words/s
[2019-07-25 22:01:02] Ep. 14 : Up. 184000 : Sen. 1,227,009 : Cost 40.02488327 : Time 441.77s : 17638.19 words/s
[2019-07-25 22:08:28] Ep. 14 : Up. 186000 : Sen. 1,575,290 : Cost 40.13964462 : Time 445.42s : 17614.35 words/s
[2019-07-25 22:15:51] Ep. 14 : Up. 188000 : Sen. 1,922,697 : Cost 40.16794586 : Time 443.51s : 17598.93 words/s
[2019-07-25 22:23:15] Ep. 14 : Up. 190000 : Sen. 2,268,934 : Cost 40.42300034 : Time 443.77s : 17553.84 words/s
[2019-07-25 22:30:36] Ep. 14 : Up. 192000 : Sen. 2,615,733 : Cost 40.00435257 : Time 441.25s : 17629.83 words/s
[2019-07-25 22:38:00] Ep. 14 : Up. 194000 : Sen. 2,962,936 : Cost 40.35456467 : Time 443.38s : 17611.08 words/s
[2019-07-25 22:45:23] Ep. 14 : Up. 196000 : Sen. 3,310,400 : Cost 40.29917908 : Time 443.74s : 17600.10 words/s
[2019-07-25 22:52:45] Ep. 14 : Up. 198000 : Sen. 3,657,102 : Cost 40.28011703 : Time 441.61s : 17607.08 words/s
[2019-07-25 23:00:07] Ep. 14 : Up. 200000 : Sen. 4,004,172 : Cost 40.23628235 : Time 442.13s : 17611.36 words/s
[2019-07-25 23:00:07] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-25 23:00:13] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter200000.npz
[2019-07-25 23:00:15] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-25 23:00:21] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-25 23:00:41] [valid] Ep. 14 : Up. 200000 : cross-entropy : 39.6604 : new best
[2019-07-25 23:00:48] [valid] Ep. 14 : Up. 200000 : perplexity : 4.76864 : new best
[2019-07-25 23:01:41] [valid] Ep. 14 : Up. 200000 : translation : 31.19 : new best
[2019-07-25 23:06:34] Seen 4231167 samples
[2019-07-25 23:06:34] Starting epoch 15
[2019-07-25 23:06:34] [data] Shuffling data
[2019-07-25 23:06:37] [data] Done reading 4864128 sentences
[2019-07-25 23:07:01] [data] Done shuffling 4864128 sentences to temp files
[2019-07-25 23:09:36] Ep. 15 : Up. 202000 : Sen. 120,656 : Cost 40.08168411 : Time 569.38s : 13732.19 words/s
[2019-07-25 23:17:01] Ep. 15 : Up. 204000 : Sen. 468,036 : Cost 39.62947083 : Time 444.86s : 17555.19 words/s
[2019-07-25 23:24:25] Ep. 15 : Up. 206000 : Sen. 815,564 : Cost 39.37227631 : Time 443.93s : 17579.68 words/s
[2019-07-25 23:31:48] Ep. 15 : Up. 208000 : Sen. 1,161,774 : Cost 39.75201797 : Time 443.14s : 17586.06 words/s
[2019-07-25 23:39:12] Ep. 15 : Up. 210000 : Sen. 1,509,314 : Cost 39.66057587 : Time 443.88s : 17575.33 words/s
[2019-07-25 23:46:36] Ep. 15 : Up. 212000 : Sen. 1,856,000 : Cost 39.85461044 : Time 444.04s : 17581.12 words/s
[2019-07-25 23:54:00] Ep. 15 : Up. 214000 : Sen. 2,203,418 : Cost 39.73016739 : Time 443.57s : 17591.31 words/s
[2019-07-26 00:01:25] Ep. 15 : Up. 216000 : Sen. 2,551,166 : Cost 40.04919434 : Time 445.36s : 17562.59 words/s
[2019-07-26 00:08:47] Ep. 15 : Up. 218000 : Sen. 2,897,864 : Cost 39.66133881 : Time 442.09s : 17565.40 words/s
[2019-07-26 00:16:12] Ep. 15 : Up. 220000 : Sen. 3,243,915 : Cost 40.05772781 : Time 444.99s : 17507.99 words/s
[2019-07-26 00:16:12] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 00:16:18] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter220000.npz
[2019-07-26 00:16:20] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 00:16:26] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 00:16:45] [valid] Ep. 15 : Up. 220000 : cross-entropy : 39.4841 : new best
[2019-07-26 00:16:52] [valid] Ep. 15 : Up. 220000 : perplexity : 4.73563 : new best
[2019-07-26 00:17:45] [valid] Ep. 15 : Up. 220000 : translation : 31.1 : stalled 1 times (last best: 31.19)
[2019-07-26 00:25:12] Ep. 15 : Up. 222000 : Sen. 3,591,336 : Cost 39.95175171 : Time 539.26s : 14456.04 words/s
[2019-07-26 00:32:38] Ep. 15 : Up. 224000 : Sen. 3,940,204 : Cost 40.01744843 : Time 446.40s : 17562.53 words/s
[2019-07-26 00:38:50] Seen 4231167 samples
[2019-07-26 00:38:50] Starting epoch 16
[2019-07-26 00:38:50] [data] Shuffling data
[2019-07-26 00:38:53] [data] Done reading 4864128 sentences
[2019-07-26 00:39:17] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 00:40:31] Ep. 16 : Up. 226000 : Sen. 56,716 : Cost 39.84867859 : Time 472.97s : 16509.23 words/s
[2019-07-26 00:47:56] Ep. 16 : Up. 228000 : Sen. 403,708 : Cost 39.10982132 : Time 445.20s : 17528.65 words/s
[2019-07-26 00:55:19] Ep. 16 : Up. 230000 : Sen. 749,030 : Cost 39.39123535 : Time 442.60s : 17539.47 words/s
[2019-07-26 01:02:43] Ep. 16 : Up. 232000 : Sen. 1,096,723 : Cost 39.13419724 : Time 444.13s : 17566.76 words/s
[2019-07-26 01:10:08] Ep. 16 : Up. 234000 : Sen. 1,443,710 : Cost 39.50194168 : Time 445.01s : 17582.92 words/s
[2019-07-26 01:17:31] Ep. 16 : Up. 236000 : Sen. 1,790,801 : Cost 39.36535263 : Time 443.42s : 17567.86 words/s
[2019-07-26 01:24:56] Ep. 16 : Up. 238000 : Sen. 2,137,864 : Cost 39.47259903 : Time 445.11s : 17529.35 words/s
[2019-07-26 01:32:21] Ep. 16 : Up. 240000 : Sen. 2,485,179 : Cost 39.43471909 : Time 444.78s : 17549.85 words/s
[2019-07-26 01:32:21] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 01:32:27] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter240000.npz
[2019-07-26 01:32:30] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 01:32:35] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 01:32:55] [valid] Ep. 16 : Up. 240000 : cross-entropy : 39.3357 : new best
[2019-07-26 01:33:01] [valid] Ep. 16 : Up. 240000 : perplexity : 4.70805 : new best
[2019-07-26 01:33:55] [valid] Ep. 16 : Up. 240000 : translation : 31.12 : stalled 2 times (last best: 31.19)
[2019-07-26 01:41:22] Ep. 16 : Up. 242000 : Sen. 2,833,036 : Cost 39.22570419 : Time 540.76s : 14424.12 words/s
[2019-07-26 01:48:45] Ep. 16 : Up. 244000 : Sen. 3,178,471 : Cost 39.61564255 : Time 442.73s : 17572.69 words/s
[2019-07-26 01:56:08] Ep. 16 : Up. 246000 : Sen. 3,526,004 : Cost 39.69245148 : Time 443.06s : 17588.79 words/s
[2019-07-26 02:03:33] Ep. 16 : Up. 248000 : Sen. 3,874,354 : Cost 39.50675964 : Time 445.37s : 17571.95 words/s
[2019-07-26 02:10:58] Ep. 16 : Up. 250000 : Sen. 4,222,268 : Cost 39.44440460 : Time 444.71s : 17549.02 words/s
[2019-07-26 02:11:10] Seen 4231167 samples
[2019-07-26 02:11:10] Starting epoch 17
[2019-07-26 02:11:10] [data] Shuffling data
[2019-07-26 02:11:13] [data] Done reading 4864128 sentences
[2019-07-26 02:11:36] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 02:18:50] Ep. 17 : Up. 252000 : Sen. 337,732 : Cost 38.67066574 : Time 472.47s : 16477.63 words/s
[2019-07-26 02:26:14] Ep. 17 : Up. 254000 : Sen. 684,013 : Cost 38.67285538 : Time 443.91s : 17522.80 words/s
[2019-07-26 02:33:40] Ep. 17 : Up. 256000 : Sen. 1,032,571 : Cost 38.78144073 : Time 445.37s : 17579.93 words/s
[2019-07-26 02:41:07] Ep. 17 : Up. 258000 : Sen. 1,380,814 : Cost 38.95354843 : Time 446.96s : 17537.01 words/s
[2019-07-26 02:48:33] Ep. 17 : Up. 260000 : Sen. 1,729,173 : Cost 39.01509857 : Time 446.84s : 17534.39 words/s
[2019-07-26 02:48:33] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 02:48:39] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter260000.npz
[2019-07-26 02:48:41] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 02:48:47] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 02:49:07] [valid] Ep. 17 : Up. 260000 : cross-entropy : 39.2157 : new best
[2019-07-26 02:49:13] [valid] Ep. 17 : Up. 260000 : perplexity : 4.68584 : new best
[2019-07-26 02:50:07] [valid] Ep. 17 : Up. 260000 : translation : 31.32 : new best
[2019-07-26 02:57:34] Ep. 17 : Up. 262000 : Sen. 2,075,948 : Cost 39.12816620 : Time 540.57s : 14423.63 words/s
[2019-07-26 03:04:59] Ep. 17 : Up. 264000 : Sen. 2,422,936 : Cost 39.02463913 : Time 444.73s : 17522.09 words/s
[2019-07-26 03:12:24] Ep. 17 : Up. 266000 : Sen. 2,770,015 : Cost 39.07404327 : Time 445.65s : 17527.16 words/s
[2019-07-26 03:19:50] Ep. 17 : Up. 268000 : Sen. 3,117,791 : Cost 39.26729202 : Time 445.13s : 17531.83 words/s
[2019-07-26 03:27:15] Ep. 17 : Up. 270000 : Sen. 3,465,972 : Cost 39.24351120 : Time 445.68s : 17550.85 words/s
[2019-07-26 03:34:39] Ep. 17 : Up. 272000 : Sen. 3,812,441 : Cost 39.20708084 : Time 443.52s : 17527.68 words/s
[2019-07-26 03:42:05] Ep. 17 : Up. 274000 : Sen. 4,159,178 : Cost 39.39246368 : Time 445.84s : 17499.71 words/s
[2019-07-26 03:43:36] Seen 4231167 samples
[2019-07-26 03:43:36] Starting epoch 18
[2019-07-26 03:43:36] [data] Shuffling data
[2019-07-26 03:43:39] [data] Done reading 4864128 sentences
[2019-07-26 03:44:01] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 03:49:56] Ep. 18 : Up. 276000 : Sen. 274,745 : Cost 38.57251358 : Time 470.96s : 16532.31 words/s
[2019-07-26 03:57:19] Ep. 18 : Up. 278000 : Sen. 622,592 : Cost 38.45288467 : Time 443.91s : 17600.66 words/s
[2019-07-26 04:04:42] Ep. 18 : Up. 280000 : Sen. 969,500 : Cost 38.58483124 : Time 442.25s : 17594.51 words/s
[2019-07-26 04:04:42] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 04:04:48] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter280000.npz
[2019-07-26 04:04:50] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 04:04:55] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 04:05:15] [valid] Ep. 18 : Up. 280000 : cross-entropy : 39.0789 : new best
[2019-07-26 04:05:22] [valid] Ep. 18 : Up. 280000 : perplexity : 4.66067 : new best
[2019-07-26 04:06:15] [valid] Ep. 18 : Up. 280000 : translation : 31.2 : stalled 1 times (last best: 31.32)
[2019-07-26 04:13:39] Ep. 18 : Up. 282000 : Sen. 1,315,480 : Cost 38.64397812 : Time 537.10s : 14498.97 words/s
[2019-07-26 04:21:00] Ep. 18 : Up. 284000 : Sen. 1,661,002 : Cost 38.75291824 : Time 441.16s : 17589.94 words/s
[2019-07-26 04:28:24] Ep. 18 : Up. 286000 : Sen. 2,009,426 : Cost 38.83179855 : Time 444.12s : 17637.09 words/s
[2019-07-26 04:35:45] Ep. 18 : Up. 288000 : Sen. 2,355,026 : Cost 38.80831909 : Time 441.08s : 17617.38 words/s
[2019-07-26 04:43:08] Ep. 18 : Up. 290000 : Sen. 2,703,038 : Cost 38.98181534 : Time 442.70s : 17646.38 words/s
[2019-07-26 04:50:32] Ep. 18 : Up. 292000 : Sen. 3,050,772 : Cost 38.97077179 : Time 444.02s : 17604.38 words/s
[2019-07-26 04:57:54] Ep. 18 : Up. 294000 : Sen. 3,397,872 : Cost 38.92035294 : Time 442.06s : 17620.75 words/s
[2019-07-26 05:05:18] Ep. 18 : Up. 296000 : Sen. 3,745,009 : Cost 38.90851212 : Time 443.84s : 17605.41 words/s
[2019-07-26 05:12:41] Ep. 18 : Up. 298000 : Sen. 4,094,108 : Cost 38.70666885 : Time 443.68s : 17650.85 words/s
[2019-07-26 05:15:37] Seen 4231167 samples
[2019-07-26 05:15:37] Starting epoch 19
[2019-07-26 05:15:37] [data] Shuffling data
[2019-07-26 05:15:39] [data] Done reading 4864128 sentences
[2019-07-26 05:16:02] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 05:20:31] Ep. 19 : Up. 300000 : Sen. 210,476 : Cost 38.37522125 : Time 469.50s : 16656.92 words/s
[2019-07-26 05:20:31] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 05:20:36] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter300000.npz
[2019-07-26 05:20:38] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 05:20:44] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 05:21:03] [valid] Ep. 19 : Up. 300000 : cross-entropy : 38.9512 : new best
[2019-07-26 05:21:10] [valid] Ep. 19 : Up. 300000 : perplexity : 4.63727 : new best
[2019-07-26 05:22:03] [valid] Ep. 19 : Up. 300000 : translation : 31.22 : stalled 2 times (last best: 31.32)
[2019-07-26 05:29:30] Ep. 19 : Up. 302000 : Sen. 557,774 : Cost 37.98898315 : Time 538.73s : 14482.99 words/s
[2019-07-26 05:36:51] Ep. 19 : Up. 304000 : Sen. 904,906 : Cost 38.07007217 : Time 441.30s : 17640.73 words/s
[2019-07-26 05:44:17] Ep. 19 : Up. 306000 : Sen. 1,252,262 : Cost 38.29589081 : Time 445.62s : 17493.00 words/s
[2019-07-26 05:51:40] Ep. 19 : Up. 308000 : Sen. 1,599,868 : Cost 38.47695541 : Time 443.45s : 17640.25 words/s
[2019-07-26 05:59:03] Ep. 19 : Up. 310000 : Sen. 1,946,680 : Cost 38.45466614 : Time 443.01s : 17609.38 words/s
[2019-07-26 06:06:27] Ep. 19 : Up. 312000 : Sen. 2,294,622 : Cost 38.63849640 : Time 444.12s : 17606.36 words/s
[2019-07-26 06:13:50] Ep. 19 : Up. 314000 : Sen. 2,640,147 : Cost 38.71176529 : Time 442.85s : 17554.50 words/s
[2019-07-26 06:21:14] Ep. 19 : Up. 316000 : Sen. 2,987,442 : Cost 38.67022705 : Time 444.11s : 17559.28 words/s
[2019-07-26 06:28:40] Ep. 19 : Up. 318000 : Sen. 3,334,170 : Cost 38.64199829 : Time 445.72s : 17483.80 words/s
[2019-07-26 06:36:06] Ep. 19 : Up. 320000 : Sen. 3,681,774 : Cost 38.73406982 : Time 446.08s : 17517.28 words/s
[2019-07-26 06:36:06] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 06:36:12] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter320000.npz
[2019-07-26 06:36:15] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 06:36:21] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 06:36:43] [valid] Ep. 19 : Up. 320000 : cross-entropy : 38.8644 : new best
[2019-07-26 06:36:50] [valid] Ep. 19 : Up. 320000 : perplexity : 4.62145 : new best
[2019-07-26 06:37:43] [valid] Ep. 19 : Up. 320000 : translation : 31.34 : new best
[2019-07-26 06:45:09] Ep. 19 : Up. 322000 : Sen. 4,029,723 : Cost 38.54272079 : Time 542.96s : 14357.10 words/s
[2019-07-26 06:49:27] Seen 4231167 samples
[2019-07-26 06:49:27] Starting epoch 20
[2019-07-26 06:49:27] [data] Shuffling data
[2019-07-26 06:49:30] [data] Done reading 4864128 sentences
[2019-07-26 06:49:53] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 06:53:04] Ep. 20 : Up. 324000 : Sen. 146,970 : Cost 38.40374374 : Time 474.70s : 16524.88 words/s
[2019-07-26 07:00:29] Ep. 20 : Up. 326000 : Sen. 494,654 : Cost 37.79024506 : Time 445.64s : 17524.66 words/s
[2019-07-26 07:07:55] Ep. 20 : Up. 328000 : Sen. 842,571 : Cost 37.99965668 : Time 445.38s : 17539.66 words/s
[2019-07-26 07:15:19] Ep. 20 : Up. 330000 : Sen. 1,189,807 : Cost 38.20242310 : Time 444.71s : 17565.90 words/s
[2019-07-26 07:22:43] Ep. 20 : Up. 332000 : Sen. 1,538,400 : Cost 37.88958740 : Time 443.60s : 17629.32 words/s
[2019-07-26 07:30:06] Ep. 20 : Up. 334000 : Sen. 1,884,073 : Cost 38.44715500 : Time 442.73s : 17587.72 words/s
[2019-07-26 07:37:27] Ep. 20 : Up. 336000 : Sen. 2,231,088 : Cost 38.12672806 : Time 441.14s : 17650.64 words/s
[2019-07-26 07:44:50] Ep. 20 : Up. 338000 : Sen. 2,578,622 : Cost 38.29491425 : Time 443.29s : 17613.67 words/s
[2019-07-26 07:52:13] Ep. 20 : Up. 340000 : Sen. 2,926,047 : Cost 38.15256882 : Time 442.87s : 17593.39 words/s
[2019-07-26 07:52:13] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 07:52:19] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter340000.npz
[2019-07-26 07:52:21] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 07:52:26] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 07:52:47] [valid] Ep. 20 : Up. 340000 : cross-entropy : 38.8366 : new best
[2019-07-26 07:52:53] [valid] Ep. 20 : Up. 340000 : perplexity : 4.6164 : new best
[2019-07-26 07:53:46] [valid] Ep. 20 : Up. 340000 : translation : 31.2 : stalled 1 times (last best: 31.34)
[2019-07-26 08:01:13] Ep. 20 : Up. 342000 : Sen. 3,273,302 : Cost 38.53069687 : Time 540.21s : 14473.57 words/s
[2019-07-26 08:08:37] Ep. 20 : Up. 344000 : Sen. 3,622,226 : Cost 38.45536041 : Time 444.26s : 17623.08 words/s
[2019-07-26 08:16:02] Ep. 20 : Up. 346000 : Sen. 3,969,745 : Cost 38.58810043 : Time 444.10s : 17610.11 words/s
[2019-07-26 08:21:36] Seen 4231167 samples
[2019-07-26 08:21:36] Starting epoch 21
[2019-07-26 08:21:36] [data] Shuffling data
[2019-07-26 08:21:39] [data] Done reading 4864128 sentences
[2019-07-26 08:22:01] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 08:23:51] Ep. 21 : Up. 348000 : Sen. 85,329 : Cost 38.27420807 : Time 469.57s : 16593.27 words/s
[2019-07-26 08:31:13] Ep. 21 : Up. 350000 : Sen. 433,392 : Cost 37.27407074 : Time 441.84s : 17660.61 words/s
[2019-07-26 08:38:34] Ep. 21 : Up. 352000 : Sen. 780,196 : Cost 37.62857819 : Time 441.44s : 17662.14 words/s
[2019-07-26 08:45:56] Ep. 21 : Up. 354000 : Sen. 1,127,374 : Cost 37.84811401 : Time 441.68s : 17663.94 words/s
[2019-07-26 08:53:17] Ep. 21 : Up. 356000 : Sen. 1,475,004 : Cost 37.82675934 : Time 441.27s : 17662.69 words/s
[2019-07-26 09:00:40] Ep. 21 : Up. 358000 : Sen. 1,822,936 : Cost 37.91456223 : Time 442.38s : 17668.65 words/s
[2019-07-26 09:08:01] Ep. 21 : Up. 360000 : Sen. 2,169,774 : Cost 37.98069000 : Time 441.41s : 17669.46 words/s
[2019-07-26 09:08:01] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 09:08:07] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter360000.npz
[2019-07-26 09:08:10] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 09:08:16] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 09:08:36] [valid] Ep. 21 : Up. 360000 : cross-entropy : 38.8005 : new best
[2019-07-26 09:08:42] [valid] Ep. 21 : Up. 360000 : perplexity : 4.60983 : new best
[2019-07-26 09:09:35] [valid] Ep. 21 : Up. 360000 : translation : 31.19 : stalled 2 times (last best: 31.34)
[2019-07-26 09:16:59] Ep. 21 : Up. 362000 : Sen. 2,517,481 : Cost 38.19165039 : Time 538.33s : 14527.13 words/s
[2019-07-26 09:24:21] Ep. 21 : Up. 364000 : Sen. 2,865,244 : Cost 37.93906403 : Time 441.42s : 17682.63 words/s
[2019-07-26 09:31:43] Ep. 21 : Up. 366000 : Sen. 3,212,536 : Cost 38.19055557 : Time 441.96s : 17651.36 words/s
[2019-07-26 09:39:04] Ep. 21 : Up. 368000 : Sen. 3,558,979 : Cost 38.21704865 : Time 441.55s : 17627.23 words/s
[2019-07-26 09:46:29] Ep. 21 : Up. 370000 : Sen. 3,906,565 : Cost 38.31745529 : Time 444.58s : 17596.38 words/s
[2019-07-26 09:53:24] Seen 4231167 samples
[2019-07-26 09:53:24] Starting epoch 22
[2019-07-26 09:53:24] [data] Shuffling data
[2019-07-26 09:53:27] [data] Done reading 4864128 sentences
[2019-07-26 09:53:53] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 09:54:22] Ep. 22 : Up. 372000 : Sen. 21,469 : Cost 38.29485703 : Time 472.77s : 16481.69 words/s
[2019-07-26 10:01:47] Ep. 22 : Up. 374000 : Sen. 369,251 : Cost 37.31324005 : Time 445.65s : 17551.29 words/s
[2019-07-26 10:09:12] Ep. 22 : Up. 376000 : Sen. 717,515 : Cost 37.25922012 : Time 444.14s : 17592.50 words/s
[2019-07-26 10:16:34] Ep. 22 : Up. 378000 : Sen. 1,064,295 : Cost 37.54484177 : Time 442.26s : 17629.29 words/s
[2019-07-26 10:23:56] Ep. 22 : Up. 380000 : Sen. 1,411,397 : Cost 37.39553070 : Time 442.24s : 17650.60 words/s
[2019-07-26 10:23:56] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 10:24:03] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter380000.npz
[2019-07-26 10:24:05] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 10:24:11] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 10:24:32] [valid] Ep. 22 : Up. 380000 : cross-entropy : 38.7335 : new best
[2019-07-26 10:24:39] [valid] Ep. 22 : Up. 380000 : perplexity : 4.59768 : new best
[2019-07-26 10:25:32] [valid] Ep. 22 : Up. 380000 : translation : 31.37 : new best
[2019-07-26 10:33:01] Ep. 22 : Up. 382000 : Sen. 1,759,046 : Cost 37.92972565 : Time 545.07s : 14325.39 words/s
[2019-07-26 10:40:35] Ep. 22 : Up. 384000 : Sen. 2,107,870 : Cost 37.65499496 : Time 453.73s : 17269.37 words/s
[2019-07-26 10:48:07] Ep. 22 : Up. 386000 : Sen. 2,454,423 : Cost 37.99706650 : Time 452.34s : 17234.80 words/s
[2019-07-26 10:55:37] Ep. 22 : Up. 388000 : Sen. 2,801,307 : Cost 38.09348679 : Time 450.04s : 17334.39 words/s
[2019-07-26 11:03:33] Ep. 22 : Up. 390000 : Sen. 3,148,626 : Cost 37.98006058 : Time 475.74s : 16373.03 words/s
[2019-07-26 11:11:47] Ep. 22 : Up. 392000 : Sen. 3,494,970 : Cost 38.10444260 : Time 494.35s : 15749.48 words/s
[2019-07-26 11:20:04] Ep. 22 : Up. 394000 : Sen. 3,842,907 : Cost 38.03475189 : Time 496.86s : 15756.36 words/s
[2019-07-26 11:28:20] Ep. 22 : Up. 396000 : Sen. 4,192,306 : Cost 37.94520950 : Time 496.08s : 15785.44 words/s
[2019-07-26 11:29:16] Seen 4231167 samples
[2019-07-26 11:29:16] Starting epoch 23
[2019-07-26 11:29:16] [data] Shuffling data
[2019-07-26 11:29:21] [data] Done reading 4864128 sentences
[2019-07-26 11:30:02] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 11:37:24] Ep. 23 : Up. 398000 : Sen. 307,332 : Cost 37.10940552 : Time 543.88s : 14315.21 words/s
[2019-07-26 11:45:40] Ep. 23 : Up. 400000 : Sen. 653,280 : Cost 37.28081512 : Time 496.00s : 15668.86 words/s
[2019-07-26 11:45:40] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 11:45:50] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter400000.npz
[2019-07-26 11:45:54] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 11:46:04] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 11:46:35] [valid] Ep. 23 : Up. 400000 : cross-entropy : 38.66 : new best
[2019-07-26 11:46:44] [valid] Ep. 23 : Up. 400000 : perplexity : 4.5844 : new best
[2019-07-26 11:47:59] [valid] Ep. 23 : Up. 400000 : translation : 31.32 : stalled 1 times (last best: 31.37)
[2019-07-26 11:56:23] Ep. 23 : Up. 402000 : Sen. 1,001,685 : Cost 37.34313202 : Time 642.64s : 12179.44 words/s
[2019-07-26 12:04:38] Ep. 23 : Up. 404000 : Sen. 1,349,558 : Cost 37.37841415 : Time 494.78s : 15786.25 words/s
[2019-07-26 12:12:57] Ep. 23 : Up. 406000 : Sen. 1,698,788 : Cost 37.38131714 : Time 499.34s : 15704.82 words/s
[2019-07-26 12:21:07] Ep. 23 : Up. 408000 : Sen. 2,046,214 : Cost 37.39356613 : Time 490.20s : 15910.93 words/s
[2019-07-26 12:29:21] Ep. 23 : Up. 410000 : Sen. 2,392,758 : Cost 37.73802567 : Time 493.96s : 15785.75 words/s
[2019-07-26 12:37:37] Ep. 23 : Up. 412000 : Sen. 2,740,113 : Cost 37.78699493 : Time 495.85s : 15749.67 words/s
[2019-07-26 12:45:51] Ep. 23 : Up. 414000 : Sen. 3,087,850 : Cost 37.71193314 : Time 494.42s : 15764.65 words/s
[2019-07-26 12:54:11] Ep. 23 : Up. 416000 : Sen. 3,436,000 : Cost 37.83319855 : Time 499.39s : 15687.16 words/s
[2019-07-26 13:02:31] Ep. 23 : Up. 418000 : Sen. 3,783,068 : Cost 37.72493362 : Time 500.49s : 15591.58 words/s
[2019-07-26 13:10:49] Ep. 23 : Up. 420000 : Sen. 4,130,840 : Cost 37.76887894 : Time 498.14s : 15687.86 words/s
[2019-07-26 13:10:49] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 13:10:59] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter420000.npz
[2019-07-26 13:11:02] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 13:11:12] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 13:11:43] [valid] Ep. 23 : Up. 420000 : cross-entropy : 38.6164 : new best
[2019-07-26 13:11:52] [valid] Ep. 23 : Up. 420000 : perplexity : 4.57654 : new best
[2019-07-26 13:13:08] [valid] Ep. 23 : Up. 420000 : translation : 31.32 : stalled 2 times (last best: 31.37)
[2019-07-26 13:15:34] Seen 4231167 samples
[2019-07-26 13:15:34] Starting epoch 24
[2019-07-26 13:15:34] [data] Shuffling data
[2019-07-26 13:15:39] [data] Done reading 4864128 sentences
[2019-07-26 13:16:18] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 13:22:17] Ep. 24 : Up. 422000 : Sen. 248,842 : Cost 37.20077133 : Time 687.50s : 11399.59 words/s
[2019-07-26 13:30:32] Ep. 24 : Up. 424000 : Sen. 596,613 : Cost 37.14402008 : Time 494.84s : 15795.74 words/s
[2019-07-26 13:38:46] Ep. 24 : Up. 426000 : Sen. 943,963 : Cost 37.09894562 : Time 494.45s : 15766.13 words/s
[2019-07-26 13:47:00] Ep. 24 : Up. 428000 : Sen. 1,290,483 : Cost 37.41883850 : Time 494.06s : 15786.74 words/s
[2019-07-26 13:55:18] Ep. 24 : Up. 430000 : Sen. 1,637,287 : Cost 37.26284409 : Time 497.38s : 15645.24 words/s
[2019-07-26 14:32:54] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-26 14:32:54] [marian] Running on bil as process 2123 with command line:
[2019-07-26 14:32:54] [marian] /fs/bil0/abdel/marian-dev/build/marian --sync-sgd --model ../experiments/100M_bicleaner_st_lm/model/model.npz -T . --devices 2 --train-sets ../experiments/100M_bicleaner_st_lm/data/train.bpe.de ../experiments/100M_bicleaner_st_lm/data/train.bpe.en --vocabs ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json --mini-batch-fit -w 5000 --dim-vocabs 50000 50000 --layer-normalization --dropout-rnn 0.2 --dropout-src 0.1 --dropout-trg 0.1 --learn-rate 0.0001 --after-epochs 0 --early-stopping 5 --valid-freq 20000 --save-freq 20000 --disp-freq 2000 --valid-mini-batch 8 --valid-sets ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en --valid-metrics cross-entropy perplexity translation --valid-translation-output ../experiments/100M_bicleaner_st_lm/model/dev.out --valid-script-path ../experiments/100M_bicleaner_st_lm/score-dev.sh --seed 1111 --exponential-smoothing --normalize=1 --beam-size=12 --quiet-translation --log ../experiments/100M_bicleaner_st_lm/model/train.log --valid-log ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-26 14:32:54] [config] after-batches: 0
[2019-07-26 14:32:54] [config] after-epochs: 0
[2019-07-26 14:32:54] [config] allow-unk: false
[2019-07-26 14:32:54] [config] beam-size: 12
[2019-07-26 14:32:54] [config] bert-class-symbol: "[CLS]"
[2019-07-26 14:32:54] [config] bert-mask-symbol: "[MASK]"
[2019-07-26 14:32:54] [config] bert-masking-fraction: 0.15
[2019-07-26 14:32:54] [config] bert-sep-symbol: "[SEP]"
[2019-07-26 14:32:54] [config] bert-train-type-embeddings: true
[2019-07-26 14:32:54] [config] bert-type-vocab-size: 2
[2019-07-26 14:32:54] [config] best-deep: false
[2019-07-26 14:32:54] [config] clip-gemm: 0
[2019-07-26 14:32:54] [config] clip-norm: 1
[2019-07-26 14:32:54] [config] cost-type: ce-mean
[2019-07-26 14:32:54] [config] cpu-threads: 0
[2019-07-26 14:32:54] [config] data-weighting: ""
[2019-07-26 14:32:54] [config] data-weighting-type: sentence
[2019-07-26 14:32:54] [config] dec-cell: gru
[2019-07-26 14:32:54] [config] dec-cell-base-depth: 2
[2019-07-26 14:32:54] [config] dec-cell-high-depth: 1
[2019-07-26 14:32:54] [config] dec-depth: 1
[2019-07-26 14:32:54] [config] devices:
[2019-07-26 14:32:54] [config]   - 2
[2019-07-26 14:32:54] [config] dim-emb: 512
[2019-07-26 14:32:54] [config] dim-rnn: 1024
[2019-07-26 14:32:54] [config] dim-vocabs:
[2019-07-26 14:32:54] [config]   - 50000
[2019-07-26 14:32:54] [config]   - 50000
[2019-07-26 14:32:54] [config] disp-first: 0
[2019-07-26 14:32:54] [config] disp-freq: 2000
[2019-07-26 14:32:54] [config] disp-label-counts: false
[2019-07-26 14:32:54] [config] dropout-rnn: 0.2
[2019-07-26 14:32:54] [config] dropout-src: 0.1
[2019-07-26 14:32:54] [config] dropout-trg: 0.1
[2019-07-26 14:32:54] [config] dump-config: ""
[2019-07-26 14:32:54] [config] early-stopping: 5
[2019-07-26 14:32:54] [config] embedding-fix-src: false
[2019-07-26 14:32:54] [config] embedding-fix-trg: false
[2019-07-26 14:32:54] [config] embedding-normalization: false
[2019-07-26 14:32:54] [config] embedding-vectors:
[2019-07-26 14:32:54] [config]   []
[2019-07-26 14:32:54] [config] enc-cell: gru
[2019-07-26 14:32:54] [config] enc-cell-depth: 1
[2019-07-26 14:32:54] [config] enc-depth: 1
[2019-07-26 14:32:54] [config] enc-type: bidirectional
[2019-07-26 14:32:54] [config] exponential-smoothing: 0.0001
[2019-07-26 14:32:54] [config] grad-dropping-momentum: 0
[2019-07-26 14:32:54] [config] grad-dropping-rate: 0
[2019-07-26 14:32:54] [config] grad-dropping-warmup: 100
[2019-07-26 14:32:54] [config] guided-alignment: none
[2019-07-26 14:32:54] [config] guided-alignment-cost: mse
[2019-07-26 14:32:54] [config] guided-alignment-weight: 0.1
[2019-07-26 14:32:54] [config] ignore-model-config: false
[2019-07-26 14:32:54] [config] input-types:
[2019-07-26 14:32:54] [config]   []
[2019-07-26 14:32:54] [config] interpolate-env-vars: false
[2019-07-26 14:32:54] [config] keep-best: false
[2019-07-26 14:32:54] [config] label-smoothing: 0
[2019-07-26 14:32:54] [config] layer-normalization: true
[2019-07-26 14:32:54] [config] learn-rate: 0.0001
[2019-07-26 14:32:54] [config] log: ../experiments/100M_bicleaner_st_lm/model/train.log
[2019-07-26 14:32:54] [config] log-level: info
[2019-07-26 14:32:54] [config] log-time-zone: ""
[2019-07-26 14:32:54] [config] lr-decay: 0
[2019-07-26 14:32:54] [config] lr-decay-freq: 50000
[2019-07-26 14:32:54] [config] lr-decay-inv-sqrt:
[2019-07-26 14:32:54] [config]   - 0
[2019-07-26 14:32:54] [config] lr-decay-repeat-warmup: false
[2019-07-26 14:32:54] [config] lr-decay-reset-optimizer: false
[2019-07-26 14:32:54] [config] lr-decay-start:
[2019-07-26 14:32:54] [config]   - 10
[2019-07-26 14:32:54] [config]   - 1
[2019-07-26 14:32:54] [config] lr-decay-strategy: epoch+stalled
[2019-07-26 14:32:54] [config] lr-report: false
[2019-07-26 14:32:54] [config] lr-warmup: 0
[2019-07-26 14:32:54] [config] lr-warmup-at-reload: false
[2019-07-26 14:32:54] [config] lr-warmup-cycle: false
[2019-07-26 14:32:54] [config] lr-warmup-start-rate: 0
[2019-07-26 14:32:54] [config] max-length: 50
[2019-07-26 14:32:54] [config] max-length-crop: false
[2019-07-26 14:32:54] [config] max-length-factor: 3
[2019-07-26 14:32:54] [config] maxi-batch: 100
[2019-07-26 14:32:54] [config] maxi-batch-sort: trg
[2019-07-26 14:32:54] [config] mini-batch: 64
[2019-07-26 14:32:54] [config] mini-batch-fit: true
[2019-07-26 14:32:54] [config] mini-batch-fit-step: 10
[2019-07-26 14:32:54] [config] mini-batch-overstuff: 1
[2019-07-26 14:32:54] [config] mini-batch-track-lr: false
[2019-07-26 14:32:54] [config] mini-batch-understuff: 1
[2019-07-26 14:32:54] [config] mini-batch-warmup: 0
[2019-07-26 14:32:54] [config] mini-batch-words: 0
[2019-07-26 14:32:54] [config] mini-batch-words-ref: 0
[2019-07-26 14:32:54] [config] model: ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 14:32:54] [config] multi-loss-type: sum
[2019-07-26 14:32:54] [config] multi-node: false
[2019-07-26 14:32:54] [config] multi-node-overlap: true
[2019-07-26 14:32:54] [config] n-best: false
[2019-07-26 14:32:54] [config] no-nccl: false
[2019-07-26 14:32:54] [config] no-reload: false
[2019-07-26 14:32:54] [config] no-restore-corpus: false
[2019-07-26 14:32:54] [config] no-shuffle: false
[2019-07-26 14:32:54] [config] normalize: 1
[2019-07-26 14:32:54] [config] num-devices: 0
[2019-07-26 14:32:54] [config] optimizer: adam
[2019-07-26 14:32:54] [config] optimizer-delay: 1
[2019-07-26 14:32:54] [config] optimizer-params:
[2019-07-26 14:32:54] [config]   []
[2019-07-26 14:32:54] [config] overwrite: false
[2019-07-26 14:32:54] [config] pretrained-model: ""
[2019-07-26 14:32:54] [config] quiet: false
[2019-07-26 14:32:54] [config] quiet-translation: true
[2019-07-26 14:32:54] [config] relative-paths: false
[2019-07-26 14:32:54] [config] right-left: false
[2019-07-26 14:32:54] [config] save-freq: 20000
[2019-07-26 14:32:54] [config] seed: 1111
[2019-07-26 14:32:54] [config] shuffle-in-ram: false
[2019-07-26 14:32:54] [config] skip: false
[2019-07-26 14:32:54] [config] sqlite: ""
[2019-07-26 14:32:54] [config] sqlite-drop: false
[2019-07-26 14:32:54] [config] sync-sgd: true
[2019-07-26 14:32:54] [config] tempdir: .
[2019-07-26 14:32:54] [config] tied-embeddings: false
[2019-07-26 14:32:54] [config] tied-embeddings-all: false
[2019-07-26 14:32:54] [config] tied-embeddings-src: false
[2019-07-26 14:32:54] [config] train-sets:
[2019-07-26 14:32:54] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de
[2019-07-26 14:32:54] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en
[2019-07-26 14:32:54] [config] transformer-aan-activation: swish
[2019-07-26 14:32:54] [config] transformer-aan-depth: 2
[2019-07-26 14:32:54] [config] transformer-aan-nogate: false
[2019-07-26 14:32:54] [config] transformer-decoder-autoreg: self-attention
[2019-07-26 14:32:54] [config] transformer-dim-aan: 2048
[2019-07-26 14:32:54] [config] transformer-dim-ffn: 2048
[2019-07-26 14:32:54] [config] transformer-dropout: 0
[2019-07-26 14:32:54] [config] transformer-dropout-attention: 0
[2019-07-26 14:32:54] [config] transformer-dropout-ffn: 0
[2019-07-26 14:32:54] [config] transformer-ffn-activation: swish
[2019-07-26 14:32:54] [config] transformer-ffn-depth: 2
[2019-07-26 14:32:54] [config] transformer-guided-alignment-layer: last
[2019-07-26 14:32:54] [config] transformer-heads: 8
[2019-07-26 14:32:54] [config] transformer-no-projection: false
[2019-07-26 14:32:54] [config] transformer-postprocess: dan
[2019-07-26 14:32:54] [config] transformer-postprocess-emb: d
[2019-07-26 14:32:54] [config] transformer-preprocess: ""
[2019-07-26 14:32:54] [config] transformer-tied-layers:
[2019-07-26 14:32:54] [config]   []
[2019-07-26 14:32:54] [config] transformer-train-position-embeddings: false
[2019-07-26 14:32:54] [config] type: amun
[2019-07-26 14:32:54] [config] ulr: false
[2019-07-26 14:32:54] [config] ulr-dim-emb: 0
[2019-07-26 14:32:54] [config] ulr-dropout: 0
[2019-07-26 14:32:54] [config] ulr-keys-vectors: ""
[2019-07-26 14:32:54] [config] ulr-query-vectors: ""
[2019-07-26 14:32:54] [config] ulr-softmax-temperature: 1
[2019-07-26 14:32:54] [config] ulr-trainable-transformation: false
[2019-07-26 14:32:54] [config] valid-freq: 20000
[2019-07-26 14:32:54] [config] valid-log: ../experiments/100M_bicleaner_st_lm/model/valid.log
[2019-07-26 14:32:54] [config] valid-max-length: 1000
[2019-07-26 14:32:54] [config] valid-metrics:
[2019-07-26 14:32:54] [config]   - cross-entropy
[2019-07-26 14:32:54] [config]   - perplexity
[2019-07-26 14:32:54] [config]   - translation
[2019-07-26 14:32:54] [config] valid-mini-batch: 8
[2019-07-26 14:32:54] [config] valid-script-path: ../experiments/100M_bicleaner_st_lm/score-dev.sh
[2019-07-26 14:32:54] [config] valid-sets:
[2019-07-26 14:32:54] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.de
[2019-07-26 14:32:54] [config]   - ../experiments/100M_bicleaner_st_lm/data/dev.bpe.en
[2019-07-26 14:32:54] [config] valid-translation-output: ../experiments/100M_bicleaner_st_lm/model/dev.out
[2019-07-26 14:32:54] [config] version: v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-26 14:32:54] [config] vocabs:
[2019-07-26 14:32:54] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-26 14:32:54] [config]   - ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-26 14:32:54] [config] word-penalty: 0
[2019-07-26 14:32:54] [config] workspace: 5000
[2019-07-26 14:32:54] [config] Loaded model has been created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-26 14:32:54] Using synchronous training
[2019-07-26 14:32:54] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.de.json
[2019-07-26 14:32:54] [data] Using unused word id eos for 0
[2019-07-26 14:32:54] [data] Using unused word id UNK for 1
[2019-07-26 14:32:54] [data] Setting vocabulary size for input 0 to 50000
[2019-07-26 14:32:54] [data] Loading vocabulary from JSON/Yaml file ../experiments/100M_bicleaner_st_lm/data/train.bpe.en.json
[2019-07-26 14:32:55] [data] Using unused word id eos for 0
[2019-07-26 14:32:55] [data] Using unused word id UNK for 1
[2019-07-26 14:32:55] [data] Setting vocabulary size for input 1 to 50000
[2019-07-26 14:32:55] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-26 14:32:55] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-26 14:32:57] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-26 14:32:57] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-26 14:32:57] [comm] NCCLCommunicator constructed successfully.
[2019-07-26 14:32:57] [training] Using 1 GPUs
[2019-07-26 14:32:57] [memory] Reserving 422 MB, device gpu2
[2019-07-26 14:32:57] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-26 14:32:57] [memory] Reserving 422 MB, device gpu2
[2019-07-26 14:33:02] [batching] Done. Typical MB size is 6880 target words
[2019-07-26 14:33:02] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-26 14:33:02] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-26 14:33:02] [comm] NCCLCommunicator constructed successfully.
[2019-07-26 14:33:02] [training] Using 1 GPUs
[2019-07-26 14:33:02] Loading model from ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 14:33:05] Loading Adam parameters from ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 14:33:08] [memory] Reserving 844 MB, device gpu2
[2019-07-26 14:33:09] [training] Model reloaded from ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 14:33:09] [data] Restoring the corpus state to epoch 23, batch 420000
[2019-07-26 14:33:09] [data] Shuffling data
[2019-07-26 14:33:13] [data] Done reading 4864128 sentences
[2019-07-26 14:33:31] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 14:35:33] Training started
[2019-07-26 14:35:33] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-07-26 14:35:33] [memory] Reserving 422 MB, device gpu2
[2019-07-26 14:35:33] [memory] Reserving 422 MB, device gpu2
[2019-07-26 14:35:33] Loading model from ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 14:35:36] [memory] Reserving 422 MB, device cpu0
[2019-07-26 14:35:37] [memory] Reserving 422 MB, device gpu2
[2019-07-26 14:37:42] Seen 4231167 samples
[2019-07-26 14:37:42] Starting epoch 24
[2019-07-26 14:37:42] [data] Shuffling data
[2019-07-26 14:37:45] [data] Done reading 4864128 sentences
[2019-07-26 14:38:04] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 14:43:18] Ep. 24 : Up. 422000 : Sen. 248,842 : Cost 37.07050323 : Time 623.29s : 12573.92 words/s
[2019-07-26 14:50:36] Ep. 24 : Up. 424000 : Sen. 596,613 : Cost 36.99652100 : Time 438.41s : 17828.77 words/s
[2019-07-26 14:57:54] Ep. 24 : Up. 426000 : Sen. 943,963 : Cost 37.09705353 : Time 437.77s : 17807.34 words/s
[2019-07-26 15:05:12] Ep. 24 : Up. 428000 : Sen. 1,290,483 : Cost 37.27218246 : Time 437.60s : 17823.60 words/s
[2019-07-26 15:12:28] Ep. 24 : Up. 430000 : Sen. 1,637,287 : Cost 37.25174332 : Time 436.62s : 17822.31 words/s
[2019-07-26 15:19:47] Ep. 24 : Up. 432000 : Sen. 1,984,842 : Cost 37.47095871 : Time 438.64s : 17811.98 words/s
[2019-07-26 15:27:07] Ep. 24 : Up. 434000 : Sen. 2,332,502 : Cost 37.44146729 : Time 439.44s : 17781.93 words/s
[2019-07-26 15:35:24] Ep. 24 : Up. 436000 : Sen. 2,679,521 : Cost 37.70933151 : Time 497.10s : 15708.07 words/s
[2019-07-26 15:43:36] Ep. 24 : Up. 438000 : Sen. 3,027,332 : Cost 37.41821671 : Time 491.99s : 15878.47 words/s
[2019-07-26 15:51:34] Ep. 24 : Up. 440000 : Sen. 3,373,396 : Cost 37.66237259 : Time 478.80s : 16252.80 words/s
[2019-07-26 15:51:34] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 15:51:42] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter440000.npz
[2019-07-26 15:51:45] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 15:51:52] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 15:52:16] [valid] Ep. 24 : Up. 440000 : cross-entropy : 38.6182 : new best
[2019-07-26 15:52:24] [valid] Ep. 24 : Up. 440000 : perplexity : 4.57686 : new best
[2019-07-26 15:53:33] [valid] Ep. 24 : Up. 440000 : translation : 31.17 : stalled 2 times (last best: 31.37)
[2019-07-26 16:01:35] Ep. 24 : Up. 442000 : Sen. 3,720,846 : Cost 37.67271423 : Time 600.55s : 13013.89 words/s
[2019-07-26 16:09:35] Ep. 24 : Up. 444000 : Sen. 4,069,103 : Cost 37.71815491 : Time 479.68s : 16274.95 words/s
[2019-07-26 16:13:18] Seen 4231167 samples
[2019-07-26 16:13:18] Starting epoch 25
[2019-07-26 16:13:18] [data] Shuffling data
[2019-07-26 16:13:23] [data] Done reading 4864128 sentences
[2019-07-26 16:13:56] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 16:18:09] Ep. 25 : Up. 446000 : Sen. 184,774 : Cost 37.11528397 : Time 514.27s : 15150.61 words/s
[2019-07-26 16:26:09] Ep. 25 : Up. 448000 : Sen. 532,584 : Cost 37.06583786 : Time 480.46s : 16290.28 words/s
[2019-07-26 16:34:07] Ep. 25 : Up. 450000 : Sen. 879,327 : Cost 36.93681335 : Time 477.16s : 16321.32 words/s
[2019-07-26 16:42:07] Ep. 25 : Up. 452000 : Sen. 1,226,284 : Cost 37.02138138 : Time 480.01s : 16232.57 words/s
[2019-07-26 16:50:03] Ep. 25 : Up. 454000 : Sen. 1,573,327 : Cost 37.06316376 : Time 476.59s : 16348.32 words/s
[2019-07-26 16:58:02] Ep. 25 : Up. 456000 : Sen. 1,920,940 : Cost 37.35150528 : Time 479.32s : 16312.39 words/s
[2019-07-26 17:06:12] Ep. 25 : Up. 458000 : Sen. 2,266,940 : Cost 37.17914963 : Time 489.53s : 15905.30 words/s
[2019-07-26 17:15:04] Ep. 25 : Up. 460000 : Sen. 2,613,900 : Cost 37.56022263 : Time 532.42s : 14647.39 words/s
[2019-07-26 17:15:04] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 17:15:13] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter460000.npz
[2019-07-26 17:15:16] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 17:15:26] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 17:15:54] [valid] Ep. 25 : Up. 460000 : cross-entropy : 38.6058 : new best
[2019-07-26 17:16:03] [valid] Ep. 25 : Up. 460000 : perplexity : 4.57462 : new best
[2019-07-26 17:17:23] [valid] Ep. 25 : Up. 460000 : translation : 31.27 : stalled 3 times (last best: 31.37)
[2019-07-26 17:26:21] Ep. 25 : Up. 462000 : Sen. 2,961,308 : Cost 37.44951630 : Time 677.01s : 11538.46 words/s
[2019-07-26 17:35:13] Ep. 25 : Up. 464000 : Sen. 3,309,064 : Cost 37.34133530 : Time 531.48s : 14683.33 words/s
[2019-07-26 17:44:01] Ep. 25 : Up. 466000 : Sen. 3,655,714 : Cost 37.56634903 : Time 528.01s : 14772.66 words/s
[2019-07-26 17:52:50] Ep. 25 : Up. 468000 : Sen. 4,004,528 : Cost 37.45955658 : Time 528.61s : 14788.12 words/s
[2019-07-26 17:58:34] Seen 4231167 samples
[2019-07-26 17:58:34] Starting epoch 26
[2019-07-26 17:58:34] [data] Shuffling data
[2019-07-26 17:58:40] [data] Done reading 4864128 sentences
[2019-07-26 17:59:27] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 18:02:38] Ep. 26 : Up. 470000 : Sen. 120,458 : Cost 37.43508148 : Time 588.74s : 13259.48 words/s
[2019-07-26 18:11:29] Ep. 26 : Up. 472000 : Sen. 467,699 : Cost 36.53647232 : Time 531.19s : 14662.44 words/s
[2019-07-26 18:20:17] Ep. 26 : Up. 474000 : Sen. 814,847 : Cost 36.69889450 : Time 527.06s : 14775.91 words/s
[2019-07-26 18:28:54] Ep. 26 : Up. 476000 : Sen. 1,161,729 : Cost 37.09318161 : Time 516.99s : 15104.14 words/s
[2019-07-26 18:37:00] Ep. 26 : Up. 478000 : Sen. 1,509,373 : Cost 36.93977356 : Time 486.42s : 16053.91 words/s
[2019-07-26 18:45:08] Ep. 26 : Up. 480000 : Sen. 1,856,438 : Cost 37.18579483 : Time 487.85s : 16000.38 words/s
[2019-07-26 18:45:08] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 18:45:16] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter480000.npz
[2019-07-26 18:45:19] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 18:45:28] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 18:45:55] [valid] Ep. 26 : Up. 480000 : cross-entropy : 38.5834 : new best
[2019-07-26 18:46:07] [valid] Ep. 26 : Up. 480000 : perplexity : 4.57059 : new best
[2019-07-26 18:47:22] [valid] Ep. 26 : Up. 480000 : translation : 31.23 : stalled 4 times (last best: 31.37)
[2019-07-26 18:55:34] Ep. 26 : Up. 482000 : Sen. 2,204,323 : Cost 36.88249969 : Time 626.30s : 12462.85 words/s
[2019-07-26 19:03:35] Ep. 26 : Up. 484000 : Sen. 2,552,200 : Cost 37.19636536 : Time 480.64s : 16252.02 words/s
[2019-07-26 19:11:32] Ep. 26 : Up. 486000 : Sen. 2,899,374 : Cost 37.17937851 : Time 477.24s : 16362.92 words/s
[2019-07-26 19:19:18] Ep. 26 : Up. 488000 : Sen. 3,246,038 : Cost 37.20043945 : Time 465.83s : 16733.80 words/s
[2019-07-26 19:27:01] Ep. 26 : Up. 490000 : Sen. 3,593,830 : Cost 37.37202072 : Time 463.43s : 16838.11 words/s
[2019-07-26 19:34:45] Ep. 26 : Up. 492000 : Sen. 3,938,959 : Cost 37.52538300 : Time 463.40s : 16756.72 words/s
[2019-07-26 19:41:16] Seen 4231167 samples
[2019-07-26 19:41:16] Starting epoch 27
[2019-07-26 19:41:16] [data] Shuffling data
[2019-07-26 19:41:20] [data] Done reading 4864128 sentences
[2019-07-26 19:41:48] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 19:43:04] Ep. 27 : Up. 494000 : Sen. 55,280 : Cost 37.28311157 : Time 499.82s : 15650.70 words/s
[2019-07-26 19:50:50] Ep. 27 : Up. 496000 : Sen. 403,200 : Cost 36.34389114 : Time 465.52s : 16776.71 words/s
[2019-07-26 19:58:33] Ep. 27 : Up. 498000 : Sen. 749,106 : Cost 36.71038818 : Time 462.75s : 16806.81 words/s
[2019-07-26 20:06:11] Ep. 27 : Up. 500000 : Sen. 1,096,402 : Cost 36.78559494 : Time 458.76s : 17006.46 words/s
[2019-07-26 20:06:11] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 20:06:17] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter500000.npz
[2019-07-26 20:06:19] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 20:06:25] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 20:06:44] [valid] Ep. 27 : Up. 500000 : cross-entropy : 38.513 : new best
[2019-07-26 20:06:51] [valid] Ep. 27 : Up. 500000 : perplexity : 4.55794 : new best
[2019-07-26 20:07:45] [valid] Ep. 27 : Up. 500000 : translation : 31.29 : stalled 5 times (last best: 31.37)
[2019-07-26 20:15:15] Ep. 27 : Up. 502000 : Sen. 1,444,521 : Cost 36.55997086 : Time 543.63s : 14348.87 words/s
[2019-07-26 20:22:40] Ep. 27 : Up. 504000 : Sen. 1,793,183 : Cost 37.00517654 : Time 444.99s : 17647.95 words/s
[2019-07-26 20:30:02] Ep. 27 : Up. 506000 : Sen. 2,140,773 : Cost 36.81431961 : Time 441.48s : 17664.90 words/s
[2019-07-26 20:37:22] Ep. 27 : Up. 508000 : Sen. 2,487,400 : Cost 37.06754684 : Time 440.78s : 17675.81 words/s
[2019-07-26 20:44:44] Ep. 27 : Up. 510000 : Sen. 2,834,970 : Cost 37.08559418 : Time 441.86s : 17675.15 words/s
[2019-07-26 20:52:04] Ep. 27 : Up. 512000 : Sen. 3,181,204 : Cost 37.07935333 : Time 440.06s : 17653.78 words/s
[2019-07-26 20:59:25] Ep. 27 : Up. 514000 : Sen. 3,528,069 : Cost 36.99768066 : Time 440.71s : 17680.00 words/s
[2019-07-26 21:06:46] Ep. 27 : Up. 516000 : Sen. 3,874,993 : Cost 37.22938538 : Time 441.07s : 17701.13 words/s
[2019-07-26 21:14:09] Ep. 27 : Up. 518000 : Sen. 4,224,000 : Cost 37.29735947 : Time 443.28s : 17703.07 words/s
[2019-07-26 21:14:19] Seen 4231167 samples
[2019-07-26 21:14:19] Starting epoch 28
[2019-07-26 21:14:19] [data] Shuffling data
[2019-07-26 21:14:21] [data] Done reading 4864128 sentences
[2019-07-26 21:14:43] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 21:21:54] Ep. 28 : Up. 520000 : Sen. 338,170 : Cost 36.42506790 : Time 465.04s : 16695.38 words/s
[2019-07-26 21:21:54] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 21:21:59] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter520000.npz
[2019-07-26 21:22:01] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 21:22:06] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 21:22:24] [valid] Ep. 28 : Up. 520000 : cross-entropy : 38.4777 : new best
[2019-07-26 21:22:30] [valid] Ep. 28 : Up. 520000 : perplexity : 4.5516 : new best
[2019-07-26 21:23:21] [valid] Ep. 28 : Up. 520000 : translation : 31.45 : new best
[2019-07-26 21:30:43] Ep. 28 : Up. 522000 : Sen. 684,494 : Cost 36.41238403 : Time 528.42s : 14726.13 words/s
[2019-07-26 21:38:05] Ep. 28 : Up. 524000 : Sen. 1,032,198 : Cost 36.81371689 : Time 442.10s : 17675.65 words/s
[2019-07-26 21:45:26] Ep. 28 : Up. 526000 : Sen. 1,379,924 : Cost 36.45716858 : Time 440.80s : 17724.36 words/s
[2019-07-26 21:52:45] Ep. 28 : Up. 528000 : Sen. 1,727,370 : Cost 36.58940506 : Time 438.94s : 17770.73 words/s
[2019-07-26 22:00:05] Ep. 28 : Up. 530000 : Sen. 2,074,923 : Cost 36.82119751 : Time 440.03s : 17731.92 words/s
[2019-07-26 22:07:24] Ep. 28 : Up. 532000 : Sen. 2,422,333 : Cost 37.02328491 : Time 439.35s : 17785.28 words/s
[2019-07-26 22:14:44] Ep. 28 : Up. 534000 : Sen. 2,769,951 : Cost 36.88505936 : Time 439.73s : 17749.47 words/s
[2019-07-26 22:22:03] Ep. 28 : Up. 536000 : Sen. 3,116,800 : Cost 37.05371475 : Time 439.74s : 17742.58 words/s
[2019-07-26 22:29:23] Ep. 28 : Up. 538000 : Sen. 3,464,945 : Cost 36.87874222 : Time 439.09s : 17807.01 words/s
[2019-07-26 22:36:42] Ep. 28 : Up. 540000 : Sen. 3,812,319 : Cost 36.94696045 : Time 439.48s : 17749.46 words/s
[2019-07-26 22:36:42] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 22:36:47] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter540000.npz
[2019-07-26 22:36:49] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 22:36:54] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 22:37:11] [valid] Ep. 28 : Up. 540000 : cross-entropy : 38.4404 : new best
[2019-07-26 22:37:17] [valid] Ep. 28 : Up. 540000 : perplexity : 4.54492 : new best
[2019-07-26 22:38:06] [valid] Ep. 28 : Up. 540000 : translation : 31.38 : stalled 1 times (last best: 31.45)
[2019-07-26 22:45:27] Ep. 28 : Up. 542000 : Sen. 4,160,396 : Cost 37.16047287 : Time 525.01s : 14911.04 words/s
[2019-07-26 22:46:56] Seen 4231167 samples
[2019-07-26 22:46:56] Starting epoch 29
[2019-07-26 22:46:56] [data] Shuffling data
[2019-07-26 22:46:59] [data] Done reading 4864128 sentences
[2019-07-26 22:47:18] [data] Done shuffling 4864128 sentences to temp files
[2019-07-26 22:53:09] Ep. 29 : Up. 544000 : Sen. 277,002 : Cost 36.55502319 : Time 461.92s : 16915.44 words/s
[2019-07-26 23:00:26] Ep. 29 : Up. 546000 : Sen. 624,762 : Cost 36.21746445 : Time 437.04s : 17835.59 words/s
[2019-07-26 23:07:42] Ep. 29 : Up. 548000 : Sen. 971,540 : Cost 36.59539413 : Time 435.67s : 17891.40 words/s
[2019-07-26 23:14:58] Ep. 29 : Up. 550000 : Sen. 1,317,264 : Cost 36.64914322 : Time 436.72s : 17846.12 words/s
[2019-07-26 23:22:17] Ep. 29 : Up. 552000 : Sen. 1,664,417 : Cost 36.66090775 : Time 438.47s : 17788.73 words/s
[2019-07-26 23:29:38] Ep. 29 : Up. 554000 : Sen. 2,012,423 : Cost 36.78491592 : Time 440.89s : 17742.85 words/s
[2019-07-26 23:36:58] Ep. 29 : Up. 556000 : Sen. 2,360,101 : Cost 36.68029022 : Time 439.94s : 17745.37 words/s
[2019-07-26 23:44:19] Ep. 29 : Up. 558000 : Sen. 2,707,374 : Cost 36.62131119 : Time 441.58s : 17647.14 words/s
[2019-07-26 23:51:41] Ep. 29 : Up. 560000 : Sen. 3,054,601 : Cost 36.87981033 : Time 441.25s : 17694.38 words/s
[2019-07-26 23:51:41] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-26 23:51:45] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter560000.npz
[2019-07-26 23:51:48] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-26 23:51:53] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-26 23:52:11] [valid] Ep. 29 : Up. 560000 : cross-entropy : 38.4594 : stalled 1 times (last best: 38.4404)
[2019-07-26 23:52:17] [valid] Ep. 29 : Up. 560000 : perplexity : 4.54831 : stalled 1 times (last best: 4.54492)
[2019-07-26 23:53:08] [valid] Ep. 29 : Up. 560000 : translation : 31.54 : new best
[2019-07-27 00:00:31] Ep. 29 : Up. 562000 : Sen. 3,401,729 : Cost 36.97340393 : Time 530.61s : 14698.55 words/s
[2019-07-27 00:07:53] Ep. 29 : Up. 564000 : Sen. 3,748,757 : Cost 37.02666092 : Time 442.23s : 17637.59 words/s
[2019-07-27 00:15:14] Ep. 29 : Up. 566000 : Sen. 4,096,230 : Cost 36.97578049 : Time 440.92s : 17702.67 words/s
[2019-07-27 00:18:05] Seen 4231167 samples
[2019-07-27 00:18:05] Starting epoch 30
[2019-07-27 00:18:05] [data] Shuffling data
[2019-07-27 00:18:08] [data] Done reading 4864128 sentences
[2019-07-27 00:18:27] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 00:22:58] Ep. 30 : Up. 568000 : Sen. 213,346 : Cost 36.34954834 : Time 464.02s : 16850.27 words/s
[2019-07-27 00:30:19] Ep. 30 : Up. 570000 : Sen. 560,312 : Cost 36.19717026 : Time 440.59s : 17691.26 words/s
[2019-07-27 00:37:41] Ep. 30 : Up. 572000 : Sen. 906,835 : Cost 36.16977692 : Time 441.80s : 17631.54 words/s
[2019-07-27 00:45:01] Ep. 30 : Up. 574000 : Sen. 1,253,169 : Cost 36.27313995 : Time 440.44s : 17656.74 words/s
[2019-07-27 00:52:21] Ep. 30 : Up. 576000 : Sen. 1,600,230 : Cost 36.47220230 : Time 439.75s : 17740.55 words/s
[2019-07-27 00:59:42] Ep. 30 : Up. 578000 : Sen. 1,947,569 : Cost 36.51017761 : Time 440.98s : 17714.05 words/s
[2019-07-27 01:07:02] Ep. 30 : Up. 580000 : Sen. 2,295,213 : Cost 36.44482040 : Time 440.09s : 17729.38 words/s
[2019-07-27 01:07:02] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 01:07:07] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter580000.npz
[2019-07-27 01:07:09] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 01:07:14] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 01:07:31] [valid] Ep. 30 : Up. 580000 : cross-entropy : 38.5075 : stalled 2 times (last best: 38.4404)
[2019-07-27 01:07:37] [valid] Ep. 30 : Up. 580000 : perplexity : 4.55695 : stalled 2 times (last best: 4.54492)
[2019-07-27 01:08:27] [valid] Ep. 30 : Up. 580000 : translation : 31.44 : stalled 1 times (last best: 31.54)
[2019-07-27 01:15:48] Ep. 30 : Up. 582000 : Sen. 2,642,455 : Cost 36.69072723 : Time 526.54s : 14823.00 words/s
[2019-07-27 01:23:08] Ep. 30 : Up. 584000 : Sen. 2,989,370 : Cost 36.58345413 : Time 439.17s : 17758.78 words/s
[2019-07-27 01:30:27] Ep. 30 : Up. 586000 : Sen. 3,338,097 : Cost 36.71349716 : Time 439.41s : 17789.99 words/s
[2019-07-27 01:37:47] Ep. 30 : Up. 588000 : Sen. 3,685,481 : Cost 36.90553284 : Time 439.48s : 17782.84 words/s
[2019-07-27 01:45:05] Ep. 30 : Up. 590000 : Sen. 4,031,073 : Cost 36.82632446 : Time 438.23s : 17758.19 words/s
[2019-07-27 01:49:17] Seen 4231167 samples
[2019-07-27 01:49:17] Starting epoch 31
[2019-07-27 01:49:17] [data] Shuffling data
[2019-07-27 01:49:20] [data] Done reading 4864128 sentences
[2019-07-27 01:49:39] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 01:52:48] Ep. 31 : Up. 592000 : Sen. 148,098 : Cost 36.43001556 : Time 463.08s : 16876.09 words/s
[2019-07-27 02:00:07] Ep. 31 : Up. 594000 : Sen. 495,131 : Cost 35.90195847 : Time 439.00s : 17742.26 words/s
[2019-07-27 02:07:25] Ep. 31 : Up. 596000 : Sen. 841,516 : Cost 36.11056900 : Time 438.41s : 17755.05 words/s
[2019-07-27 02:14:45] Ep. 31 : Up. 598000 : Sen. 1,187,929 : Cost 36.36270905 : Time 440.19s : 17739.17 words/s
[2019-07-27 02:22:06] Ep. 31 : Up. 600000 : Sen. 1,535,868 : Cost 36.19631195 : Time 440.07s : 17741.40 words/s
[2019-07-27 02:22:06] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 02:22:11] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter600000.npz
[2019-07-27 02:22:13] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 02:22:17] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 02:22:34] [valid] Ep. 31 : Up. 600000 : cross-entropy : 38.506 : stalled 3 times (last best: 38.4404)
[2019-07-27 02:22:41] [valid] Ep. 31 : Up. 600000 : perplexity : 4.55668 : stalled 3 times (last best: 4.54492)
[2019-07-27 02:23:31] [valid] Ep. 31 : Up. 600000 : translation : 31.46 : stalled 2 times (last best: 31.54)
[2019-07-27 02:30:52] Ep. 31 : Up. 602000 : Sen. 1,882,400 : Cost 36.58055878 : Time 526.76s : 14796.08 words/s
[2019-07-27 02:38:10] Ep. 31 : Up. 604000 : Sen. 2,228,600 : Cost 36.37216568 : Time 438.08s : 17738.50 words/s
[2019-07-27 02:45:32] Ep. 31 : Up. 606000 : Sen. 2,577,360 : Cost 36.42287064 : Time 441.92s : 17737.79 words/s
[2019-07-27 02:52:52] Ep. 31 : Up. 608000 : Sen. 2,925,685 : Cost 36.56801605 : Time 439.87s : 17775.69 words/s
[2019-07-27 03:00:13] Ep. 31 : Up. 610000 : Sen. 3,273,646 : Cost 36.80205917 : Time 440.46s : 17788.66 words/s
[2019-07-27 03:07:32] Ep. 31 : Up. 612000 : Sen. 3,620,965 : Cost 36.68881989 : Time 439.37s : 17761.86 words/s
[2019-07-27 03:14:51] Ep. 31 : Up. 614000 : Sen. 3,967,868 : Cost 36.79756546 : Time 438.64s : 17754.38 words/s
[2019-07-27 03:20:24] Seen 4231167 samples
[2019-07-27 03:20:24] Starting epoch 32
[2019-07-27 03:20:24] [data] Shuffling data
[2019-07-27 03:20:26] [data] Done reading 4864128 sentences
[2019-07-27 03:20:46] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 03:22:32] Ep. 32 : Up. 616000 : Sen. 83,779 : Cost 36.44754028 : Time 461.37s : 16873.81 words/s
[2019-07-27 03:29:53] Ep. 32 : Up. 618000 : Sen. 431,694 : Cost 35.94524384 : Time 440.84s : 17721.66 words/s
[2019-07-27 03:37:14] Ep. 32 : Up. 620000 : Sen. 779,314 : Cost 36.15891647 : Time 440.82s : 17714.01 words/s
[2019-07-27 03:37:14] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 03:37:18] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter620000.npz
[2019-07-27 03:37:20] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 03:37:25] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 03:37:42] [valid] Ep. 32 : Up. 620000 : cross-entropy : 38.4302 : new best
[2019-07-27 03:37:49] [valid] Ep. 32 : Up. 620000 : perplexity : 4.5431 : new best
[2019-07-27 03:38:38] [valid] Ep. 32 : Up. 620000 : translation : 31.63 : new best
[2019-07-27 03:46:01] Ep. 32 : Up. 622000 : Sen. 1,127,356 : Cost 35.96152115 : Time 527.69s : 14821.38 words/s
[2019-07-27 03:53:24] Ep. 32 : Up. 624000 : Sen. 1,476,443 : Cost 36.15510941 : Time 442.87s : 17743.54 words/s
[2019-07-27 04:00:46] Ep. 32 : Up. 626000 : Sen. 1,824,396 : Cost 36.45532608 : Time 441.88s : 17729.07 words/s
[2019-07-27 04:08:07] Ep. 32 : Up. 628000 : Sen. 2,173,056 : Cost 36.34618759 : Time 441.35s : 17716.93 words/s
[2019-07-27 04:15:28] Ep. 32 : Up. 630000 : Sen. 2,519,677 : Cost 36.44552231 : Time 440.32s : 17693.44 words/s
[2019-07-27 04:22:48] Ep. 32 : Up. 632000 : Sen. 2,867,549 : Cost 36.49550247 : Time 440.67s : 17725.30 words/s
[2019-07-27 04:30:11] Ep. 32 : Up. 634000 : Sen. 3,215,547 : Cost 36.58165741 : Time 442.57s : 17716.98 words/s
[2019-07-27 04:37:31] Ep. 32 : Up. 636000 : Sen. 3,562,358 : Cost 36.64160919 : Time 439.78s : 17681.78 words/s
[2019-07-27 04:44:52] Ep. 32 : Up. 638000 : Sen. 3,909,473 : Cost 36.74437332 : Time 440.84s : 17704.65 words/s
[2019-07-27 04:51:40] Seen 4231167 samples
[2019-07-27 04:51:40] Starting epoch 33
[2019-07-27 04:51:40] [data] Shuffling data
[2019-07-27 04:51:42] [data] Done reading 4864128 sentences
[2019-07-27 04:52:02] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 04:52:35] Ep. 33 : Up. 640000 : Sen. 26,136 : Cost 36.62312317 : Time 463.65s : 16836.15 words/s
[2019-07-27 04:52:35] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 04:52:40] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter640000.npz
[2019-07-27 04:52:42] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 04:52:47] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 04:53:04] [valid] Ep. 33 : Up. 640000 : cross-entropy : 38.4004 : new best
[2019-07-27 04:53:10] [valid] Ep. 33 : Up. 640000 : perplexity : 4.53776 : new best
[2019-07-27 04:54:00] [valid] Ep. 33 : Up. 640000 : translation : 31.78 : new best
[2019-07-27 05:01:23] Ep. 33 : Up. 642000 : Sen. 373,789 : Cost 35.75214767 : Time 527.74s : 14805.54 words/s
[2019-07-27 05:08:45] Ep. 33 : Up. 644000 : Sen. 721,668 : Cost 35.99706650 : Time 441.70s : 17698.67 words/s
[2019-07-27 05:16:06] Ep. 33 : Up. 646000 : Sen. 1,068,570 : Cost 35.95887756 : Time 440.97s : 17670.80 words/s
[2019-07-27 05:23:27] Ep. 33 : Up. 648000 : Sen. 1,416,296 : Cost 36.20465851 : Time 441.36s : 17721.90 words/s
[2019-07-27 05:30:47] Ep. 33 : Up. 650000 : Sen. 1,762,249 : Cost 35.99912643 : Time 439.97s : 17669.86 words/s
[2019-07-27 05:38:06] Ep. 33 : Up. 652000 : Sen. 2,109,569 : Cost 36.03733826 : Time 439.39s : 17706.86 words/s
[2019-07-27 05:45:29] Ep. 33 : Up. 654000 : Sen. 2,457,204 : Cost 36.31293869 : Time 442.11s : 17692.95 words/s
[2019-07-27 05:52:49] Ep. 33 : Up. 656000 : Sen. 2,804,572 : Cost 36.32443237 : Time 440.72s : 17717.06 words/s
[2019-07-27 06:00:10] Ep. 33 : Up. 658000 : Sen. 3,151,307 : Cost 36.50180817 : Time 440.52s : 17680.57 words/s
[2019-07-27 06:07:31] Ep. 33 : Up. 660000 : Sen. 3,497,842 : Cost 36.49932480 : Time 441.67s : 17689.24 words/s
[2019-07-27 06:07:31] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 06:07:36] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter660000.npz
[2019-07-27 06:07:38] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 06:07:43] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 06:08:01] [valid] Ep. 33 : Up. 660000 : cross-entropy : 38.4234 : stalled 1 times (last best: 38.4004)
[2019-07-27 06:08:07] [valid] Ep. 33 : Up. 660000 : perplexity : 4.54187 : stalled 1 times (last best: 4.53776)
[2019-07-27 06:08:58] [valid] Ep. 33 : Up. 660000 : translation : 31.63 : stalled 1 times (last best: 31.78)
[2019-07-27 06:16:17] Ep. 33 : Up. 662000 : Sen. 3,843,915 : Cost 36.51794815 : Time 525.32s : 14776.84 words/s
[2019-07-27 06:23:36] Ep. 33 : Up. 664000 : Sen. 4,192,349 : Cost 36.49763107 : Time 439.73s : 17774.76 words/s
[2019-07-27 06:24:26] Seen 4231167 samples
[2019-07-27 06:24:26] Starting epoch 34
[2019-07-27 06:24:26] [data] Shuffling data
[2019-07-27 06:24:28] [data] Done reading 4864128 sentences
[2019-07-27 06:24:53] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 06:31:23] Ep. 34 : Up. 666000 : Sen. 307,332 : Cost 35.75922012 : Time 466.76s : 16681.94 words/s
[2019-07-27 06:38:45] Ep. 34 : Up. 668000 : Sen. 654,770 : Cost 35.88191986 : Time 441.78s : 17689.14 words/s
[2019-07-27 06:46:07] Ep. 34 : Up. 670000 : Sen. 1,002,166 : Cost 35.85058975 : Time 442.46s : 17640.64 words/s
[2019-07-27 06:53:28] Ep. 34 : Up. 672000 : Sen. 1,348,144 : Cost 35.93205643 : Time 440.39s : 17668.22 words/s
[2019-07-27 07:00:49] Ep. 34 : Up. 674000 : Sen. 1,695,800 : Cost 36.12721634 : Time 441.41s : 17705.99 words/s
[2019-07-27 07:08:11] Ep. 34 : Up. 676000 : Sen. 2,043,737 : Cost 36.12807083 : Time 441.75s : 17681.60 words/s
[2019-07-27 07:15:32] Ep. 34 : Up. 678000 : Sen. 2,390,790 : Cost 36.32235718 : Time 440.59s : 17674.89 words/s
[2019-07-27 07:22:52] Ep. 34 : Up. 680000 : Sen. 2,737,919 : Cost 36.14488983 : Time 440.03s : 17728.74 words/s
[2019-07-27 07:22:52] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 07:22:58] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter680000.npz
[2019-07-27 07:23:00] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 07:23:05] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 07:23:23] [valid] Ep. 34 : Up. 680000 : cross-entropy : 38.4768 : stalled 2 times (last best: 38.4004)
[2019-07-27 07:23:29] [valid] Ep. 34 : Up. 680000 : perplexity : 4.55143 : stalled 2 times (last best: 4.53776)
[2019-07-27 07:24:19] [valid] Ep. 34 : Up. 680000 : translation : 31.54 : stalled 2 times (last best: 31.78)
[2019-07-27 07:31:40] Ep. 34 : Up. 682000 : Sen. 3,085,249 : Cost 36.22961426 : Time 528.47s : 14755.47 words/s
[2019-07-27 07:39:01] Ep. 34 : Up. 684000 : Sen. 3,434,185 : Cost 36.43824005 : Time 440.98s : 17794.99 words/s
[2019-07-27 07:46:20] Ep. 34 : Up. 686000 : Sen. 3,782,400 : Cost 36.34278488 : Time 439.29s : 17795.72 words/s
[2019-07-27 07:53:37] Ep. 34 : Up. 688000 : Sen. 4,129,055 : Cost 36.53578568 : Time 436.79s : 17824.47 words/s
[2019-07-27 07:55:46] Seen 4231167 samples
[2019-07-27 07:55:46] Starting epoch 35
[2019-07-27 07:55:46] [data] Shuffling data
[2019-07-27 07:55:51] [data] Done reading 4864128 sentences
[2019-07-27 07:56:10] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 08:01:19] Ep. 35 : Up. 690000 : Sen. 245,499 : Cost 35.68229675 : Time 462.00s : 16876.33 words/s
[2019-07-27 08:08:37] Ep. 35 : Up. 692000 : Sen. 592,846 : Cost 35.75303268 : Time 438.13s : 17843.59 words/s
[2019-07-27 08:15:56] Ep. 35 : Up. 694000 : Sen. 941,106 : Cost 35.78786469 : Time 438.33s : 17821.41 words/s
[2019-07-27 08:23:13] Ep. 35 : Up. 696000 : Sen. 1,288,105 : Cost 35.85206985 : Time 437.10s : 17844.95 words/s
[2019-07-27 08:30:31] Ep. 35 : Up. 698000 : Sen. 1,635,568 : Cost 35.99801636 : Time 438.30s : 17843.98 words/s
[2019-07-27 08:37:48] Ep. 35 : Up. 700000 : Sen. 1,984,000 : Cost 35.91323090 : Time 437.26s : 17885.68 words/s
[2019-07-27 08:37:48] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 08:37:53] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter700000.npz
[2019-07-27 08:37:55] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 08:37:59] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 08:38:17] [valid] Ep. 35 : Up. 700000 : cross-entropy : 38.4531 : stalled 3 times (last best: 38.4004)
[2019-07-27 08:38:23] [valid] Ep. 35 : Up. 700000 : perplexity : 4.54719 : stalled 3 times (last best: 4.53776)
[2019-07-27 08:39:11] [valid] Ep. 35 : Up. 700000 : translation : 31.56 : stalled 3 times (last best: 31.78)
[2019-07-27 08:46:30] Ep. 35 : Up. 702000 : Sen. 2,330,268 : Cost 36.09348679 : Time 521.53s : 14920.51 words/s
[2019-07-27 08:53:49] Ep. 35 : Up. 704000 : Sen. 2,678,416 : Cost 36.02769470 : Time 438.78s : 17831.21 words/s
[2019-07-27 09:01:07] Ep. 35 : Up. 706000 : Sen. 3,026,804 : Cost 36.22035980 : Time 438.34s : 17851.52 words/s
[2019-07-27 09:08:27] Ep. 35 : Up. 708000 : Sen. 3,374,688 : Cost 36.49732971 : Time 440.55s : 17771.16 words/s
[2019-07-27 09:15:48] Ep. 35 : Up. 710000 : Sen. 3,722,475 : Cost 36.36669159 : Time 440.75s : 17740.98 words/s
[2019-07-27 09:23:09] Ep. 35 : Up. 712000 : Sen. 4,070,762 : Cost 36.16755676 : Time 440.99s : 17732.50 words/s
[2019-07-27 09:26:32] Seen 4231167 samples
[2019-07-27 09:26:32] Starting epoch 36
[2019-07-27 09:26:32] [data] Shuffling data
[2019-07-27 09:26:35] [data] Done reading 4864128 sentences
[2019-07-27 09:26:55] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 09:30:52] Ep. 36 : Up. 714000 : Sen. 186,499 : Cost 35.80837250 : Time 463.26s : 16832.25 words/s
[2019-07-27 09:38:14] Ep. 36 : Up. 716000 : Sen. 533,905 : Cost 35.65937424 : Time 441.54s : 17729.24 words/s
[2019-07-27 09:45:34] Ep. 36 : Up. 718000 : Sen. 881,087 : Cost 35.62559128 : Time 440.19s : 17738.71 words/s
[2019-07-27 09:52:53] Ep. 36 : Up. 720000 : Sen. 1,228,208 : Cost 35.78012466 : Time 439.02s : 17765.67 words/s
[2019-07-27 09:52:53] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 09:52:59] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter720000.npz
[2019-07-27 09:53:01] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 09:53:07] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 09:53:25] [valid] Ep. 36 : Up. 720000 : cross-entropy : 38.4521 : stalled 4 times (last best: 38.4004)
[2019-07-27 09:53:32] [valid] Ep. 36 : Up. 720000 : perplexity : 4.54701 : stalled 4 times (last best: 4.53776)
[2019-07-27 09:54:22] [valid] Ep. 36 : Up. 720000 : translation : 31.51 : stalled 4 times (last best: 31.78)
[2019-07-27 10:01:44] Ep. 36 : Up. 722000 : Sen. 1,576,833 : Cost 35.60699844 : Time 530.37s : 14739.68 words/s
[2019-07-27 10:09:01] Ep. 36 : Up. 724000 : Sen. 1,923,104 : Cost 35.82852554 : Time 437.75s : 17759.40 words/s
[2019-07-27 10:16:22] Ep. 36 : Up. 726000 : Sen. 2,271,242 : Cost 35.97074509 : Time 441.03s : 17715.21 words/s
[2019-07-27 10:23:45] Ep. 36 : Up. 728000 : Sen. 2,618,720 : Cost 36.24483490 : Time 443.07s : 17647.60 words/s
[2019-07-27 10:31:09] Ep. 36 : Up. 730000 : Sen. 2,967,561 : Cost 35.89886856 : Time 443.22s : 17642.50 words/s
[2019-07-27 10:38:33] Ep. 36 : Up. 732000 : Sen. 3,315,200 : Cost 36.19784164 : Time 443.96s : 17615.90 words/s
[2019-07-27 10:45:56] Ep. 36 : Up. 734000 : Sen. 3,663,011 : Cost 36.21936035 : Time 443.40s : 17620.59 words/s
[2019-07-27 10:53:18] Ep. 36 : Up. 736000 : Sen. 4,010,580 : Cost 36.25047302 : Time 442.36s : 17678.11 words/s
[2019-07-27 10:57:57] Seen 4231167 samples
[2019-07-27 10:57:57] Starting epoch 37
[2019-07-27 10:57:57] [data] Shuffling data
[2019-07-27 10:58:00] [data] Done reading 4864128 sentences
[2019-07-27 10:58:20] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 11:01:02] Ep. 37 : Up. 738000 : Sen. 126,766 : Cost 35.77415466 : Time 463.24s : 16853.14 words/s
[2019-07-27 11:08:21] Ep. 37 : Up. 740000 : Sen. 473,783 : Cost 35.52799606 : Time 439.35s : 17710.38 words/s
[2019-07-27 11:08:21] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 11:08:26] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter740000.npz
[2019-07-27 11:08:28] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 11:08:33] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 11:08:51] [valid] Ep. 37 : Up. 740000 : cross-entropy : 38.389 : new best
[2019-07-27 11:08:57] [valid] Ep. 37 : Up. 740000 : perplexity : 4.53572 : new best
[2019-07-27 11:09:48] [valid] Ep. 37 : Up. 740000 : translation : 31.69 : stalled 5 times (last best: 31.78)
[2019-07-27 11:17:09] Ep. 37 : Up. 742000 : Sen. 820,679 : Cost 35.48386383 : Time 528.47s : 14721.51 words/s
[2019-07-27 11:24:31] Ep. 37 : Up. 744000 : Sen. 1,168,123 : Cost 35.81200027 : Time 441.22s : 17706.98 words/s
[2019-07-27 11:31:51] Ep. 37 : Up. 746000 : Sen. 1,515,723 : Cost 35.54160690 : Time 440.71s : 17713.34 words/s
[2019-07-27 11:39:12] Ep. 37 : Up. 748000 : Sen. 1,862,936 : Cost 35.69531631 : Time 440.59s : 17701.22 words/s
[2019-07-27 11:46:32] Ep. 37 : Up. 750000 : Sen. 2,210,306 : Cost 35.92755127 : Time 440.27s : 17694.16 words/s
[2019-07-27 11:53:53] Ep. 37 : Up. 752000 : Sen. 2,557,654 : Cost 35.99399185 : Time 441.01s : 17703.73 words/s
[2019-07-27 12:01:16] Ep. 37 : Up. 754000 : Sen. 2,905,732 : Cost 36.04586792 : Time 442.49s : 17705.34 words/s
[2019-07-27 12:08:37] Ep. 37 : Up. 756000 : Sen. 3,253,858 : Cost 35.90936279 : Time 441.77s : 17681.68 words/s
[2019-07-27 12:16:00] Ep. 37 : Up. 758000 : Sen. 3,601,517 : Cost 36.45698929 : Time 442.49s : 17720.83 words/s
[2019-07-27 12:23:22] Ep. 37 : Up. 760000 : Sen. 3,949,238 : Cost 36.18180466 : Time 442.04s : 17688.56 words/s
[2019-07-27 12:23:22] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 12:23:27] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter760000.npz
[2019-07-27 12:23:29] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 12:23:34] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 12:23:53] [valid] Ep. 37 : Up. 760000 : cross-entropy : 38.4144 : stalled 1 times (last best: 38.389)
[2019-07-27 12:23:59] [valid] Ep. 37 : Up. 760000 : perplexity : 4.54026 : stalled 1 times (last best: 4.53572)
[2019-07-27 12:24:50] [valid] Ep. 37 : Up. 760000 : translation : 31.59 : stalled 6 times (last best: 31.78)
[2019-07-27 12:30:51] Seen 4231167 samples
[2019-07-27 12:30:51] Starting epoch 38
[2019-07-27 12:30:51] [data] Shuffling data
[2019-07-27 12:30:53] [data] Done reading 4864128 sentences
[2019-07-27 12:31:13] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 12:32:35] Ep. 38 : Up. 762000 : Sen. 63,826 : Cost 36.06101608 : Time 553.21s : 14029.18 words/s
[2019-07-27 12:39:57] Ep. 38 : Up. 764000 : Sen. 412,806 : Cost 35.24563980 : Time 442.22s : 17717.52 words/s
[2019-07-27 12:47:19] Ep. 38 : Up. 766000 : Sen. 761,238 : Cost 35.50921631 : Time 442.03s : 17706.78 words/s
[2019-07-27 12:54:39] Ep. 38 : Up. 768000 : Sen. 1,108,506 : Cost 35.32449722 : Time 439.77s : 17715.72 words/s
[2019-07-27 13:02:02] Ep. 38 : Up. 770000 : Sen. 1,456,546 : Cost 35.79279709 : Time 442.82s : 17675.04 words/s
[2019-07-27 13:09:25] Ep. 38 : Up. 772000 : Sen. 1,804,367 : Cost 35.80992126 : Time 442.57s : 17683.57 words/s
[2019-07-27 13:16:45] Ep. 38 : Up. 774000 : Sen. 2,150,976 : Cost 35.94387817 : Time 440.71s : 17679.00 words/s
[2019-07-27 13:24:06] Ep. 38 : Up. 776000 : Sen. 2,497,936 : Cost 35.69417953 : Time 440.60s : 17685.58 words/s
[2019-07-27 13:31:27] Ep. 38 : Up. 778000 : Sen. 2,845,044 : Cost 35.99962997 : Time 441.43s : 17680.08 words/s
[2019-07-27 13:38:50] Ep. 38 : Up. 780000 : Sen. 3,192,440 : Cost 35.88270950 : Time 442.46s : 17672.99 words/s
[2019-07-27 13:38:50] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 13:38:55] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter780000.npz
[2019-07-27 13:38:57] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 13:39:02] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 13:39:20] [valid] Ep. 38 : Up. 780000 : cross-entropy : 38.4317 : stalled 2 times (last best: 38.389)
[2019-07-27 13:39:27] [valid] Ep. 38 : Up. 780000 : perplexity : 4.54335 : stalled 2 times (last best: 4.53572)
[2019-07-27 13:40:17] [valid] Ep. 38 : Up. 780000 : translation : 31.78 : stalled 7 times (last best: 31.78)
[2019-07-27 13:47:40] Ep. 38 : Up. 782000 : Sen. 3,540,250 : Cost 36.11338043 : Time 530.62s : 14707.72 words/s
[2019-07-27 13:55:00] Ep. 38 : Up. 784000 : Sen. 3,886,439 : Cost 35.82232285 : Time 439.16s : 17685.01 words/s
[2019-07-27 14:02:19] Seen 4231167 samples
[2019-07-27 14:02:19] Starting epoch 39
[2019-07-27 14:02:19] [data] Shuffling data
[2019-07-27 14:02:22] [data] Done reading 4864128 sentences
[2019-07-27 14:02:43] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 14:02:46] Ep. 39 : Up. 786000 : Sen. 1,775 : Cost 36.17633438 : Time 466.50s : 16703.78 words/s
[2019-07-27 14:10:08] Ep. 39 : Up. 788000 : Sen. 350,545 : Cost 35.20612717 : Time 441.98s : 17731.76 words/s
[2019-07-27 14:17:30] Ep. 39 : Up. 790000 : Sen. 697,600 : Cost 35.14683151 : Time 442.00s : 17644.48 words/s
[2019-07-27 14:24:51] Ep. 39 : Up. 792000 : Sen. 1,044,449 : Cost 35.50513458 : Time 440.91s : 17689.02 words/s
[2019-07-27 14:32:12] Ep. 39 : Up. 794000 : Sen. 1,391,622 : Cost 35.75630188 : Time 441.17s : 17655.95 words/s
[2019-07-27 14:39:33] Ep. 39 : Up. 796000 : Sen. 1,738,894 : Cost 35.65797806 : Time 440.73s : 17702.15 words/s
[2019-07-27 14:46:54] Ep. 39 : Up. 798000 : Sen. 2,086,226 : Cost 35.60673141 : Time 441.60s : 17671.60 words/s
[2019-07-27 14:54:17] Ep. 39 : Up. 800000 : Sen. 2,434,366 : Cost 35.88187408 : Time 442.08s : 17695.82 words/s
[2019-07-27 14:54:17] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 14:54:22] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter800000.npz
[2019-07-27 14:54:24] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 14:54:29] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 14:54:47] [valid] Ep. 39 : Up. 800000 : cross-entropy : 38.4146 : stalled 3 times (last best: 38.389)
[2019-07-27 14:54:53] [valid] Ep. 39 : Up. 800000 : perplexity : 4.5403 : stalled 3 times (last best: 4.53572)
[2019-07-27 14:55:45] [valid] Ep. 39 : Up. 800000 : translation : 31.69 : stalled 8 times (last best: 31.78)
[2019-07-27 15:03:09] Ep. 39 : Up. 802000 : Sen. 2,783,026 : Cost 35.77826309 : Time 532.25s : 14708.89 words/s
[2019-07-27 15:10:30] Ep. 39 : Up. 804000 : Sen. 3,130,268 : Cost 35.95116425 : Time 441.01s : 17696.07 words/s
[2019-07-27 15:17:50] Ep. 39 : Up. 806000 : Sen. 3,476,983 : Cost 36.02275467 : Time 440.34s : 17678.00 words/s
[2019-07-27 15:25:13] Ep. 39 : Up. 808000 : Sen. 3,825,341 : Cost 35.92384720 : Time 442.83s : 17709.05 words/s
[2019-07-27 15:32:35] Ep. 39 : Up. 810000 : Sen. 4,173,338 : Cost 35.96779251 : Time 441.53s : 17716.62 words/s
[2019-07-27 15:33:48] Seen 4231167 samples
[2019-07-27 15:33:48] Starting epoch 40
[2019-07-27 15:33:48] [data] Shuffling data
[2019-07-27 15:33:51] [data] Done reading 4864128 sentences
[2019-07-27 15:34:11] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 15:40:19] Ep. 40 : Up. 812000 : Sen. 288,668 : Cost 35.29604721 : Time 464.44s : 16773.98 words/s
[2019-07-27 15:47:41] Ep. 40 : Up. 814000 : Sen. 635,922 : Cost 35.29680634 : Time 441.70s : 17683.52 words/s
[2019-07-27 15:55:02] Ep. 40 : Up. 816000 : Sen. 983,405 : Cost 35.41057968 : Time 441.39s : 17660.13 words/s
[2019-07-27 16:02:24] Ep. 40 : Up. 818000 : Sen. 1,331,068 : Cost 35.36313629 : Time 442.00s : 17674.71 words/s
[2019-07-27 16:09:46] Ep. 40 : Up. 820000 : Sen. 1,678,742 : Cost 35.53768158 : Time 442.39s : 17656.27 words/s
[2019-07-27 16:09:46] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 16:09:51] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter820000.npz
[2019-07-27 16:09:53] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 16:09:58] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 16:10:17] [valid] Ep. 40 : Up. 820000 : cross-entropy : 38.4268 : stalled 4 times (last best: 38.389)
[2019-07-27 16:10:23] [valid] Ep. 40 : Up. 820000 : perplexity : 4.54249 : stalled 4 times (last best: 4.53572)
[2019-07-27 16:11:14] [valid] Ep. 40 : Up. 820000 : translation : 31.62 : stalled 9 times (last best: 31.78)
[2019-07-27 16:18:37] Ep. 40 : Up. 822000 : Sen. 2,026,608 : Cost 35.51513290 : Time 530.65s : 14714.84 words/s
[2019-07-27 16:25:58] Ep. 40 : Up. 824000 : Sen. 2,374,400 : Cost 35.59093094 : Time 441.18s : 17710.26 words/s
[2019-07-27 16:33:15] Ep. 40 : Up. 826000 : Sen. 2,721,034 : Cost 35.72352600 : Time 436.71s : 17846.27 words/s
[2019-07-27 16:40:32] Ep. 40 : Up. 828000 : Sen. 3,068,537 : Cost 35.59392166 : Time 437.14s : 17859.86 words/s
[2019-07-27 16:47:49] Ep. 40 : Up. 830000 : Sen. 3,416,570 : Cost 35.87968445 : Time 436.91s : 17875.83 words/s
[2019-07-27 16:55:07] Ep. 40 : Up. 832000 : Sen. 3,763,924 : Cost 35.86744690 : Time 437.65s : 17855.97 words/s
[2019-07-27 17:02:24] Ep. 40 : Up. 834000 : Sen. 4,110,566 : Cost 36.02482224 : Time 436.82s : 17863.76 words/s
[2019-07-27 17:04:55] Seen 4231167 samples
[2019-07-27 17:04:55] Starting epoch 41
[2019-07-27 17:04:55] [data] Shuffling data
[2019-07-27 17:04:58] [data] Done reading 4864128 sentences
[2019-07-27 17:05:17] [data] Done shuffling 4864128 sentences to temp files
[2019-07-27 17:10:01] Ep. 41 : Up. 836000 : Sen. 225,345 : Cost 35.23927689 : Time 457.88s : 16972.63 words/s
[2019-07-27 17:17:22] Ep. 41 : Up. 838000 : Sen. 573,561 : Cost 35.07456970 : Time 440.26s : 17777.88 words/s
[2019-07-27 17:24:43] Ep. 41 : Up. 840000 : Sen. 922,838 : Cost 35.25664902 : Time 441.69s : 17760.34 words/s
[2019-07-27 17:24:43] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 17:24:48] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.iter840000.npz
[2019-07-27 17:24:50] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 17:24:56] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
[2019-07-27 17:25:15] [valid] Ep. 41 : Up. 840000 : cross-entropy : 38.4125 : stalled 5 times (last best: 38.389)
[2019-07-27 17:25:21] [valid] Ep. 41 : Up. 840000 : perplexity : 4.53992 : stalled 5 times (last best: 4.53572)
[2019-07-27 17:26:11] [valid] Ep. 41 : Up. 840000 : translation : 31.79 : new best
[2019-07-27 17:26:13] Training finished
[2019-07-27 17:26:17] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz.orig.npz
[2019-07-27 17:26:22] Saving model to ../experiments/100M_bicleaner_st_lm/model/model.npz
[2019-07-27 17:26:28] Saving Adam parameters to ../experiments/100M_bicleaner_st_lm/model/model.npz.optimizer.npz
