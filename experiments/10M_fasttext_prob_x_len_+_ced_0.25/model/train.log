[2019-07-16 18:42:34] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-16 18:42:34] [marian] Running on dagr as process 35722 with command line:
[2019-07-16 18:42:34] [marian] /fs/bil0/abdel/marian-dev/build/marian --sync-sgd --model ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz -T . --devices 2 --train-sets ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.de ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.en --vocabs ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.de.json ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.en.json --mini-batch-fit -w 5000 --dim-vocabs 50000 50000 --layer-normalization --dropout-rnn 0.2 --dropout-src 0.1 --dropout-trg 0.1 --learn-rate 0.0001 --after-epochs 0 --early-stopping 5 --valid-freq 20000 --save-freq 20000 --disp-freq 2000 --valid-mini-batch 8 --valid-sets ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/dev.bpe.de ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/dev.bpe.en --valid-metrics cross-entropy perplexity translation --valid-translation-output ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/dev.out --valid-script-path ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/score-dev.sh --seed 1111 --exponential-smoothing --normalize=1 --beam-size=12 --quiet-translation --log ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/train.log --valid-log ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/valid.log
[2019-07-16 18:42:34] [config] after-batches: 0
[2019-07-16 18:42:34] [config] after-epochs: 0
[2019-07-16 18:42:34] [config] allow-unk: false
[2019-07-16 18:42:34] [config] beam-size: 12
[2019-07-16 18:42:34] [config] bert-class-symbol: "[CLS]"
[2019-07-16 18:42:34] [config] bert-mask-symbol: "[MASK]"
[2019-07-16 18:42:34] [config] bert-masking-fraction: 0.15
[2019-07-16 18:42:34] [config] bert-sep-symbol: "[SEP]"
[2019-07-16 18:42:34] [config] bert-train-type-embeddings: true
[2019-07-16 18:42:34] [config] bert-type-vocab-size: 2
[2019-07-16 18:42:34] [config] best-deep: false
[2019-07-16 18:42:34] [config] clip-gemm: 0
[2019-07-16 18:42:34] [config] clip-norm: 1
[2019-07-16 18:42:34] [config] cost-type: ce-mean
[2019-07-16 18:42:34] [config] cpu-threads: 0
[2019-07-16 18:42:34] [config] data-weighting: ""
[2019-07-16 18:42:34] [config] data-weighting-type: sentence
[2019-07-16 18:42:34] [config] dec-cell: gru
[2019-07-16 18:42:34] [config] dec-cell-base-depth: 2
[2019-07-16 18:42:34] [config] dec-cell-high-depth: 1
[2019-07-16 18:42:34] [config] dec-depth: 1
[2019-07-16 18:42:34] [config] devices:
[2019-07-16 18:42:34] [config]   - 2
[2019-07-16 18:42:34] [config] dim-emb: 512
[2019-07-16 18:42:34] [config] dim-rnn: 1024
[2019-07-16 18:42:34] [config] dim-vocabs:
[2019-07-16 18:42:34] [config]   - 50000
[2019-07-16 18:42:34] [config]   - 50000
[2019-07-16 18:42:34] [config] disp-first: 0
[2019-07-16 18:42:34] [config] disp-freq: 2000
[2019-07-16 18:42:34] [config] disp-label-counts: false
[2019-07-16 18:42:34] [config] dropout-rnn: 0.2
[2019-07-16 18:42:34] [config] dropout-src: 0.1
[2019-07-16 18:42:34] [config] dropout-trg: 0.1
[2019-07-16 18:42:34] [config] dump-config: ""
[2019-07-16 18:42:34] [config] early-stopping: 5
[2019-07-16 18:42:34] [config] embedding-fix-src: false
[2019-07-16 18:42:34] [config] embedding-fix-trg: false
[2019-07-16 18:42:34] [config] embedding-normalization: false
[2019-07-16 18:42:34] [config] embedding-vectors:
[2019-07-16 18:42:34] [config]   []
[2019-07-16 18:42:34] [config] enc-cell: gru
[2019-07-16 18:42:34] [config] enc-cell-depth: 1
[2019-07-16 18:42:34] [config] enc-depth: 1
[2019-07-16 18:42:34] [config] enc-type: bidirectional
[2019-07-16 18:42:34] [config] exponential-smoothing: 0.0001
[2019-07-16 18:42:34] [config] grad-dropping-momentum: 0
[2019-07-16 18:42:34] [config] grad-dropping-rate: 0
[2019-07-16 18:42:34] [config] grad-dropping-warmup: 100
[2019-07-16 18:42:34] [config] guided-alignment: none
[2019-07-16 18:42:34] [config] guided-alignment-cost: mse
[2019-07-16 18:42:34] [config] guided-alignment-weight: 0.1
[2019-07-16 18:42:34] [config] ignore-model-config: false
[2019-07-16 18:42:34] [config] input-types:
[2019-07-16 18:42:34] [config]   []
[2019-07-16 18:42:34] [config] interpolate-env-vars: false
[2019-07-16 18:42:34] [config] keep-best: false
[2019-07-16 18:42:34] [config] label-smoothing: 0
[2019-07-16 18:42:34] [config] layer-normalization: true
[2019-07-16 18:42:34] [config] learn-rate: 0.0001
[2019-07-16 18:42:34] [config] log: ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/train.log
[2019-07-16 18:42:34] [config] log-level: info
[2019-07-16 18:42:34] [config] log-time-zone: ""
[2019-07-16 18:42:34] [config] lr-decay: 0
[2019-07-16 18:42:34] [config] lr-decay-freq: 50000
[2019-07-16 18:42:34] [config] lr-decay-inv-sqrt:
[2019-07-16 18:42:34] [config]   - 0
[2019-07-16 18:42:34] [config] lr-decay-repeat-warmup: false
[2019-07-16 18:42:34] [config] lr-decay-reset-optimizer: false
[2019-07-16 18:42:34] [config] lr-decay-start:
[2019-07-16 18:42:34] [config]   - 10
[2019-07-16 18:42:34] [config]   - 1
[2019-07-16 18:42:34] [config] lr-decay-strategy: epoch+stalled
[2019-07-16 18:42:34] [config] lr-report: false
[2019-07-16 18:42:34] [config] lr-warmup: 0
[2019-07-16 18:42:34] [config] lr-warmup-at-reload: false
[2019-07-16 18:42:34] [config] lr-warmup-cycle: false
[2019-07-16 18:42:34] [config] lr-warmup-start-rate: 0
[2019-07-16 18:42:34] [config] max-length: 50
[2019-07-16 18:42:34] [config] max-length-crop: false
[2019-07-16 18:42:34] [config] max-length-factor: 3
[2019-07-16 18:42:34] [config] maxi-batch: 100
[2019-07-16 18:42:34] [config] maxi-batch-sort: trg
[2019-07-16 18:42:34] [config] mini-batch: 64
[2019-07-16 18:42:34] [config] mini-batch-fit: true
[2019-07-16 18:42:34] [config] mini-batch-fit-step: 10
[2019-07-16 18:42:34] [config] mini-batch-overstuff: 1
[2019-07-16 18:42:34] [config] mini-batch-track-lr: false
[2019-07-16 18:42:34] [config] mini-batch-understuff: 1
[2019-07-16 18:42:34] [config] mini-batch-warmup: 0
[2019-07-16 18:42:34] [config] mini-batch-words: 0
[2019-07-16 18:42:34] [config] mini-batch-words-ref: 0
[2019-07-16 18:42:34] [config] model: ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz
[2019-07-16 18:42:34] [config] multi-loss-type: sum
[2019-07-16 18:42:34] [config] multi-node: false
[2019-07-16 18:42:34] [config] multi-node-overlap: true
[2019-07-16 18:42:34] [config] n-best: false
[2019-07-16 18:42:34] [config] no-nccl: false
[2019-07-16 18:42:34] [config] no-reload: false
[2019-07-16 18:42:34] [config] no-restore-corpus: false
[2019-07-16 18:42:34] [config] no-shuffle: false
[2019-07-16 18:42:34] [config] normalize: 1
[2019-07-16 18:42:34] [config] num-devices: 0
[2019-07-16 18:42:34] [config] optimizer: adam
[2019-07-16 18:42:34] [config] optimizer-delay: 1
[2019-07-16 18:42:34] [config] optimizer-params:
[2019-07-16 18:42:34] [config]   []
[2019-07-16 18:42:34] [config] overwrite: false
[2019-07-16 18:42:34] [config] pretrained-model: ""
[2019-07-16 18:42:34] [config] quiet: false
[2019-07-16 18:42:34] [config] quiet-translation: true
[2019-07-16 18:42:34] [config] relative-paths: false
[2019-07-16 18:42:34] [config] right-left: false
[2019-07-16 18:42:34] [config] save-freq: 20000
[2019-07-16 18:42:34] [config] seed: 1111
[2019-07-16 18:42:34] [config] shuffle-in-ram: false
[2019-07-16 18:42:34] [config] skip: false
[2019-07-16 18:42:34] [config] sqlite: ""
[2019-07-16 18:42:34] [config] sqlite-drop: false
[2019-07-16 18:42:34] [config] sync-sgd: true
[2019-07-16 18:42:34] [config] tempdir: .
[2019-07-16 18:42:34] [config] tied-embeddings: false
[2019-07-16 18:42:34] [config] tied-embeddings-all: false
[2019-07-16 18:42:34] [config] tied-embeddings-src: false
[2019-07-16 18:42:34] [config] train-sets:
[2019-07-16 18:42:34] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.de
[2019-07-16 18:42:34] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.en
[2019-07-16 18:42:34] [config] transformer-aan-activation: swish
[2019-07-16 18:42:34] [config] transformer-aan-depth: 2
[2019-07-16 18:42:34] [config] transformer-aan-nogate: false
[2019-07-16 18:42:34] [config] transformer-decoder-autoreg: self-attention
[2019-07-16 18:42:34] [config] transformer-dim-aan: 2048
[2019-07-16 18:42:34] [config] transformer-dim-ffn: 2048
[2019-07-16 18:42:34] [config] transformer-dropout: 0
[2019-07-16 18:42:34] [config] transformer-dropout-attention: 0
[2019-07-16 18:42:34] [config] transformer-dropout-ffn: 0
[2019-07-16 18:42:34] [config] transformer-ffn-activation: swish
[2019-07-16 18:42:34] [config] transformer-ffn-depth: 2
[2019-07-16 18:42:34] [config] transformer-guided-alignment-layer: last
[2019-07-16 18:42:34] [config] transformer-heads: 8
[2019-07-16 18:42:34] [config] transformer-no-projection: false
[2019-07-16 18:42:34] [config] transformer-postprocess: dan
[2019-07-16 18:42:34] [config] transformer-postprocess-emb: d
[2019-07-16 18:42:34] [config] transformer-preprocess: ""
[2019-07-16 18:42:34] [config] transformer-tied-layers:
[2019-07-16 18:42:34] [config]   []
[2019-07-16 18:42:34] [config] transformer-train-position-embeddings: false
[2019-07-16 18:42:34] [config] type: amun
[2019-07-16 18:42:34] [config] ulr: false
[2019-07-16 18:42:34] [config] ulr-dim-emb: 0
[2019-07-16 18:42:34] [config] ulr-dropout: 0
[2019-07-16 18:42:34] [config] ulr-keys-vectors: ""
[2019-07-16 18:42:34] [config] ulr-query-vectors: ""
[2019-07-16 18:42:34] [config] ulr-softmax-temperature: 1
[2019-07-16 18:42:34] [config] ulr-trainable-transformation: false
[2019-07-16 18:42:34] [config] valid-freq: 20000
[2019-07-16 18:42:34] [config] valid-log: ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/valid.log
[2019-07-16 18:42:34] [config] valid-max-length: 1000
[2019-07-16 18:42:34] [config] valid-metrics:
[2019-07-16 18:42:34] [config]   - cross-entropy
[2019-07-16 18:42:34] [config]   - perplexity
[2019-07-16 18:42:34] [config]   - translation
[2019-07-16 18:42:34] [config] valid-mini-batch: 8
[2019-07-16 18:42:34] [config] valid-script-path: ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/score-dev.sh
[2019-07-16 18:42:34] [config] valid-sets:
[2019-07-16 18:42:34] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/dev.bpe.de
[2019-07-16 18:42:34] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/dev.bpe.en
[2019-07-16 18:42:34] [config] valid-translation-output: ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/dev.out
[2019-07-16 18:42:34] [config] vocabs:
[2019-07-16 18:42:34] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.de.json
[2019-07-16 18:42:34] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.en.json
[2019-07-16 18:42:34] [config] word-penalty: 0
[2019-07-16 18:42:34] [config] workspace: 5000
[2019-07-16 18:42:34] [config] Model is being created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-16 18:42:34] Using synchronous training
[2019-07-16 18:42:34] [data] Loading vocabulary from JSON/Yaml file ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.de.json
[2019-07-16 18:42:34] [data] Using unused word id eos for 0
[2019-07-16 18:42:34] [data] Using unused word id UNK for 1
[2019-07-16 18:42:34] [data] Setting vocabulary size for input 0 to 50000
[2019-07-16 18:42:34] [data] Loading vocabulary from JSON/Yaml file ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/data/train.bpe.en.json
[2019-07-16 18:42:34] [data] Using unused word id eos for 0
[2019-07-16 18:42:34] [data] Using unused word id UNK for 1
[2019-07-16 18:42:34] [data] Setting vocabulary size for input 1 to 50000
[2019-07-16 18:42:34] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-16 18:42:34] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-16 18:42:36] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-16 18:42:36] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-16 18:42:36] [comm] NCCLCommunicator constructed successfully.
[2019-07-16 18:42:36] [training] Using 1 GPUs
[2019-07-16 18:42:36] [memory] Reserving 422 MB, device gpu2
[2019-07-16 18:42:36] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-16 18:42:36] [memory] Reserving 422 MB, device gpu2
[2019-07-16 18:42:45] [batching] Done. Typical MB size is 6880 target words
[2019-07-16 18:42:45] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-16 18:42:45] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-16 18:42:45] [comm] NCCLCommunicator constructed successfully.
[2019-07-16 18:42:45] [training] Using 1 GPUs
[2019-07-16 18:42:45] Training started
[2019-07-16 18:42:45] [data] Shuffling data
[2019-07-16 18:42:54] [data] Done reading 13926791 sentences
[2019-07-16 18:44:09] [data] Done shuffling 13926791 sentences to temp files
[2019-07-16 18:44:15] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-07-16 18:44:15] [memory] Reserving 422 MB, device gpu2
[2019-07-16 18:44:15] [memory] Reserving 422 MB, device gpu2
[2019-07-16 18:44:15] [memory] Reserving 422 MB, device gpu2
[2019-07-16 18:44:15] [memory] Reserving 844 MB, device gpu2
[2019-07-16 18:58:03] Ep. 1 : Up. 2000 : Sen. 288,980 : Cost 136.75701904 : Time 928.84s : 6553.05 words/s
[2019-07-16 19:12:04] Ep. 1 : Up. 4000 : Sen. 578,204 : Cost 119.14662933 : Time 841.01s : 7281.78 words/s
[2019-07-16 19:26:05] Ep. 1 : Up. 6000 : Sen. 867,303 : Cost 110.50666046 : Time 840.49s : 7258.97 words/s
[2019-07-16 19:40:06] Ep. 1 : Up. 8000 : Sen. 1,156,777 : Cost 104.86098480 : Time 840.94s : 7257.45 words/s
[2019-07-16 19:54:06] Ep. 1 : Up. 10000 : Sen. 1,446,723 : Cost 101.05861664 : Time 840.48s : 7288.99 words/s
[2019-07-16 20:08:05] Ep. 1 : Up. 12000 : Sen. 1,735,724 : Cost 97.64866638 : Time 839.05s : 7271.03 words/s
[2019-07-16 20:22:07] Ep. 1 : Up. 14000 : Sen. 2,024,932 : Cost 95.12207031 : Time 841.60s : 7256.94 words/s
[2019-07-16 20:36:07] Ep. 1 : Up. 16000 : Sen. 2,313,959 : Cost 92.84384918 : Time 840.65s : 7258.39 words/s
[2019-07-16 20:50:11] Ep. 1 : Up. 18000 : Sen. 2,603,653 : Cost 91.15547943 : Time 843.19s : 7264.05 words/s
[2019-07-16 21:04:14] Ep. 1 : Up. 20000 : Sen. 2,893,238 : Cost 89.63602448 : Time 843.39s : 7258.18 words/s
[2019-07-16 21:04:14] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.orig.npz
[2019-07-16 21:04:23] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.iter20000.npz
[2019-07-16 21:04:30] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz
[2019-07-16 21:04:39] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.optimizer.npz
[2019-07-16 21:05:04] [valid] Ep. 1 : Up. 20000 : cross-entropy : 90.8601 : new best
[2019-07-16 21:05:11] [valid] Ep. 1 : Up. 20000 : perplexity : 36.6078 : new best
[2019-07-16 21:06:36] [valid] Ep. 1 : Up. 20000 : translation : 12.17 : new best
[2019-07-16 21:20:40] Ep. 1 : Up. 22000 : Sen. 3,182,468 : Cost 88.00970459 : Time 986.40s : 6188.45 words/s
[2019-07-16 21:34:39] Ep. 1 : Up. 24000 : Sen. 3,469,988 : Cost 87.03524017 : Time 838.90s : 7244.76 words/s
[2019-07-16 21:48:42] Ep. 1 : Up. 26000 : Sen. 3,758,814 : Cost 85.73952484 : Time 842.44s : 7240.17 words/s
[2019-07-16 22:02:39] Ep. 1 : Up. 28000 : Sen. 4,047,578 : Cost 84.69268036 : Time 837.38s : 7262.29 words/s
[2019-07-16 22:16:43] Ep. 1 : Up. 30000 : Sen. 4,336,226 : Cost 84.44776917 : Time 843.80s : 7253.87 words/s
[2019-07-16 22:30:46] Ep. 1 : Up. 32000 : Sen. 4,626,630 : Cost 82.86391449 : Time 843.09s : 7263.53 words/s
[2019-07-16 22:44:49] Ep. 1 : Up. 34000 : Sen. 4,916,289 : Cost 82.65140533 : Time 842.78s : 7258.69 words/s
[2019-07-16 22:58:51] Ep. 1 : Up. 36000 : Sen. 5,205,535 : Cost 81.85598755 : Time 842.10s : 7249.95 words/s
[2019-07-16 23:12:47] Ep. 1 : Up. 38000 : Sen. 5,492,817 : Cost 81.16600037 : Time 836.28s : 7247.86 words/s
[2019-07-16 23:26:50] Ep. 1 : Up. 40000 : Sen. 5,781,877 : Cost 80.56988525 : Time 843.18s : 7243.94 words/s
[2019-07-16 23:26:50] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.orig.npz
[2019-07-16 23:26:59] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.iter40000.npz
[2019-07-16 23:27:06] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz
[2019-07-16 23:27:16] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.optimizer.npz
[2019-07-16 23:27:40] [valid] Ep. 1 : Up. 40000 : cross-entropy : 73.3961 : new best
[2019-07-16 23:27:48] [valid] Ep. 1 : Up. 40000 : perplexity : 18.3249 : new best
[2019-07-16 23:29:02] [valid] Ep. 1 : Up. 40000 : translation : 19.55 : new best
[2019-07-16 23:43:01] Ep. 1 : Up. 42000 : Sen. 6,070,179 : Cost 79.79265594 : Time 971.01s : 6255.03 words/s
[2019-07-16 23:57:03] Ep. 1 : Up. 44000 : Sen. 6,358,711 : Cost 79.79811859 : Time 841.26s : 7249.47 words/s
[2019-07-17 00:11:01] Ep. 1 : Up. 46000 : Sen. 6,646,578 : Cost 79.21067810 : Time 837.84s : 7252.49 words/s
[2019-07-17 00:25:01] Ep. 1 : Up. 48000 : Sen. 6,935,181 : Cost 78.89852142 : Time 840.47s : 7250.41 words/s
[2019-07-17 00:39:02] Ep. 1 : Up. 50000 : Sen. 7,224,299 : Cost 78.09456635 : Time 841.11s : 7244.83 words/s
[2019-07-17 00:53:08] Ep. 1 : Up. 52000 : Sen. 7,514,104 : Cost 78.21887207 : Time 845.57s : 7253.63 words/s
[2019-07-17 01:07:10] Ep. 1 : Up. 54000 : Sen. 7,803,587 : Cost 77.47589874 : Time 841.87s : 7250.49 words/s
[2019-07-17 01:21:11] Ep. 1 : Up. 56000 : Sen. 8,092,507 : Cost 77.51848602 : Time 841.77s : 7261.16 words/s
[2019-07-17 01:35:13] Ep. 1 : Up. 58000 : Sen. 8,381,422 : Cost 76.90616608 : Time 841.24s : 7247.35 words/s
[2019-07-17 01:49:16] Ep. 1 : Up. 60000 : Sen. 8,670,242 : Cost 76.79135895 : Time 843.88s : 7236.10 words/s
[2019-07-17 01:49:16] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.orig.npz
[2019-07-17 01:49:26] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.iter60000.npz
[2019-07-17 01:49:33] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz
[2019-07-17 01:49:42] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.optimizer.npz
[2019-07-17 01:50:07] [valid] Ep. 1 : Up. 60000 : cross-entropy : 66.4769 : new best
[2019-07-17 01:50:14] [valid] Ep. 1 : Up. 60000 : perplexity : 13.9307 : new best
[2019-07-17 01:51:25] [valid] Ep. 1 : Up. 60000 : translation : 21.92 : new best
[2019-07-17 02:05:30] Ep. 1 : Up. 62000 : Sen. 8,958,748 : Cost 76.37639618 : Time 974.02s : 6271.37 words/s
[2019-07-17 02:19:35] Ep. 1 : Up. 64000 : Sen. 9,247,562 : Cost 76.25269318 : Time 844.13s : 7224.68 words/s
[2019-07-17 02:33:38] Ep. 1 : Up. 66000 : Sen. 9,536,854 : Cost 75.97511292 : Time 843.12s : 7249.68 words/s
[2019-07-17 02:47:40] Ep. 1 : Up. 68000 : Sen. 9,826,627 : Cost 75.20000458 : Time 841.90s : 7241.54 words/s
[2019-07-17 03:01:46] Ep. 1 : Up. 70000 : Sen. 10,117,566 : Cost 75.50884247 : Time 846.78s : 7265.14 words/s
[2019-07-17 03:15:52] Ep. 1 : Up. 72000 : Sen. 10,407,642 : Cost 75.12265778 : Time 845.68s : 7237.48 words/s
[2019-07-17 03:29:55] Ep. 1 : Up. 74000 : Sen. 10,697,114 : Cost 74.85155487 : Time 842.90s : 7246.93 words/s
[2019-07-17 03:44:00] Ep. 1 : Up. 76000 : Sen. 10,987,024 : Cost 74.63420105 : Time 844.78s : 7246.76 words/s
[2019-07-17 03:58:07] Ep. 1 : Up. 78000 : Sen. 11,277,900 : Cost 74.65672302 : Time 847.61s : 7249.42 words/s
[2019-07-17 04:12:12] Ep. 1 : Up. 80000 : Sen. 11,567,749 : Cost 74.19778442 : Time 844.49s : 7252.56 words/s
[2019-07-17 04:12:12] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.orig.npz
[2019-07-17 04:12:21] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.iter80000.npz
[2019-07-17 04:12:28] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz
[2019-07-17 04:12:37] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.optimizer.npz
[2019-07-17 04:13:02] [valid] Ep. 1 : Up. 80000 : cross-entropy : 62.6195 : new best
[2019-07-17 04:13:09] [valid] Ep. 1 : Up. 80000 : perplexity : 11.9562 : new best
[2019-07-17 04:14:19] [valid] Ep. 1 : Up. 80000 : translation : 23.23 : new best
[2019-07-17 04:25:25] Seen 11795613 samples
[2019-07-17 04:25:25] Starting epoch 2
[2019-07-17 04:25:25] [data] Shuffling data
[2019-07-17 04:25:32] [data] Done reading 13926791 sentences
[2019-07-17 04:26:40] [data] Done shuffling 13926791 sentences to temp files
[2019-07-17 04:29:39] Ep. 2 : Up. 82000 : Sen. 60,413 : Cost 73.91910553 : Time 1047.05s : 5811.49 words/s
[2019-07-17 04:43:43] Ep. 2 : Up. 84000 : Sen. 349,577 : Cost 73.28546906 : Time 843.62s : 7241.19 words/s
[2019-07-17 04:57:45] Ep. 2 : Up. 86000 : Sen. 638,746 : Cost 73.02408600 : Time 842.43s : 7239.63 words/s
[2019-07-17 05:11:46] Ep. 2 : Up. 88000 : Sen. 927,110 : Cost 72.94388580 : Time 840.79s : 7250.45 words/s
[2019-07-17 05:25:48] Ep. 2 : Up. 90000 : Sen. 1,215,777 : Cost 72.96342468 : Time 841.82s : 7244.91 words/s
[2019-07-17 05:39:52] Ep. 2 : Up. 92000 : Sen. 1,505,130 : Cost 72.64910889 : Time 844.57s : 7238.31 words/s
[2019-07-17 05:53:56] Ep. 2 : Up. 94000 : Sen. 1,794,761 : Cost 72.33586121 : Time 843.86s : 7239.44 words/s
[2019-07-17 06:08:00] Ep. 2 : Up. 96000 : Sen. 2,083,543 : Cost 72.40852356 : Time 843.67s : 7238.63 words/s
[2019-07-17 06:22:03] Ep. 2 : Up. 98000 : Sen. 2,373,405 : Cost 72.06594086 : Time 843.50s : 7251.17 words/s
[2019-07-17 06:36:05] Ep. 2 : Up. 100000 : Sen. 2,662,664 : Cost 72.14463806 : Time 841.56s : 7255.79 words/s
[2019-07-17 06:36:05] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.orig.npz
[2019-07-17 06:36:15] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.iter100000.npz
[2019-07-17 06:36:22] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz
[2019-07-17 06:36:32] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.optimizer.npz
[2019-07-17 06:36:59] [valid] Ep. 2 : Up. 100000 : cross-entropy : 60.2333 : new best
[2019-07-17 06:37:06] [valid] Ep. 2 : Up. 100000 : perplexity : 10.8775 : new best
[2019-07-17 06:38:16] [valid] Ep. 2 : Up. 100000 : translation : 23.84 : new best
[2019-07-17 06:52:22] Ep. 2 : Up. 102000 : Sen. 2,951,999 : Cost 72.12772369 : Time 977.27s : 6255.11 words/s
[2019-07-17 07:06:27] Ep. 2 : Up. 104000 : Sen. 3,242,063 : Cost 71.91580963 : Time 844.71s : 7249.80 words/s
[2019-07-17 07:20:31] Ep. 2 : Up. 106000 : Sen. 3,530,802 : Cost 71.98169708 : Time 844.04s : 7241.86 words/s
[2019-07-17 07:34:37] Ep. 2 : Up. 108000 : Sen. 3,820,130 : Cost 71.72251892 : Time 845.91s : 7225.69 words/s
[2019-07-17 07:48:39] Ep. 2 : Up. 110000 : Sen. 4,109,911 : Cost 71.25982666 : Time 842.46s : 7249.02 words/s
[2019-07-17 08:02:41] Ep. 2 : Up. 112000 : Sen. 4,398,873 : Cost 71.47452545 : Time 841.42s : 7257.85 words/s
[2019-07-17 08:16:43] Ep. 2 : Up. 114000 : Sen. 4,687,364 : Cost 71.46831512 : Time 842.39s : 7239.25 words/s
[2019-07-17 08:30:45] Ep. 2 : Up. 116000 : Sen. 4,977,180 : Cost 70.87658691 : Time 842.09s : 7247.51 words/s
[2019-07-17 08:44:49] Ep. 2 : Up. 118000 : Sen. 5,266,268 : Cost 71.57441711 : Time 843.81s : 7263.21 words/s
[2019-07-17 08:58:52] Ep. 2 : Up. 120000 : Sen. 5,556,340 : Cost 71.16921234 : Time 843.51s : 7261.31 words/s
[2019-07-17 08:58:52] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.orig.npz
[2019-07-17 08:59:02] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.iter120000.npz
[2019-07-17 08:59:08] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz
[2019-07-17 08:59:18] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.optimizer.npz
[2019-07-17 08:59:43] [valid] Ep. 2 : Up. 120000 : cross-entropy : 58.4178 : new best
[2019-07-17 08:59:50] [valid] Ep. 2 : Up. 120000 : perplexity : 10.1225 : new best
[2019-07-17 09:01:00] [valid] Ep. 2 : Up. 120000 : translation : 24.07 : new best
[2019-07-17 09:15:05] Ep. 2 : Up. 122000 : Sen. 5,845,587 : Cost 70.95701599 : Time 972.62s : 6277.89 words/s
[2019-07-17 09:29:10] Ep. 2 : Up. 124000 : Sen. 6,135,076 : Cost 70.73576355 : Time 845.49s : 7226.14 words/s
[2019-07-17 09:43:12] Ep. 2 : Up. 126000 : Sen. 6,423,918 : Cost 70.59671783 : Time 841.22s : 7245.29 words/s
[2019-07-17 09:57:13] Ep. 2 : Up. 128000 : Sen. 6,712,686 : Cost 70.81116486 : Time 841.79s : 7256.17 words/s
[2019-07-17 10:11:19] Ep. 2 : Up. 130000 : Sen. 7,001,864 : Cost 70.49243164 : Time 845.71s : 7227.79 words/s
[2019-07-17 10:25:28] Ep. 2 : Up. 132000 : Sen. 7,292,765 : Cost 70.33087158 : Time 849.21s : 7229.77 words/s
[2019-07-17 10:39:31] Ep. 2 : Up. 134000 : Sen. 7,582,209 : Cost 70.16883087 : Time 842.25s : 7246.98 words/s
[2019-07-17 10:53:33] Ep. 2 : Up. 136000 : Sen. 7,870,860 : Cost 70.56362152 : Time 842.09s : 7246.45 words/s
[2019-07-17 11:07:37] Ep. 2 : Up. 138000 : Sen. 8,160,574 : Cost 70.25899506 : Time 844.14s : 7239.09 words/s
[2019-07-17 11:21:40] Ep. 2 : Up. 140000 : Sen. 8,449,524 : Cost 70.10806274 : Time 842.70s : 7224.78 words/s
[2019-07-17 11:21:40] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.orig.npz
[2019-07-17 11:21:49] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.iter140000.npz
[2019-07-17 11:21:56] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz
[2019-07-17 11:22:06] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.optimizer.npz
[2019-07-17 11:22:31] [valid] Ep. 2 : Up. 140000 : cross-entropy : 57.0989 : new best
[2019-07-17 11:22:39] [valid] Ep. 2 : Up. 140000 : perplexity : 9.60708 : new best
[2019-07-17 11:23:48] [valid] Ep. 2 : Up. 140000 : translation : 24.24 : new best
[2019-07-17 11:37:55] Ep. 2 : Up. 142000 : Sen. 8,739,147 : Cost 70.35657501 : Time 975.45s : 6288.75 words/s
[2019-07-17 11:52:00] Ep. 2 : Up. 144000 : Sen. 9,028,861 : Cost 69.77267456 : Time 845.08s : 7233.55 words/s
[2019-07-17 12:06:01] Ep. 2 : Up. 146000 : Sen. 9,316,784 : Cost 69.90512085 : Time 840.69s : 7236.32 words/s
[2019-07-17 12:20:06] Ep. 2 : Up. 148000 : Sen. 9,606,004 : Cost 69.72819519 : Time 845.25s : 7215.73 words/s
[2019-07-17 12:34:12] Ep. 2 : Up. 150000 : Sen. 9,894,796 : Cost 70.02679443 : Time 845.91s : 7226.62 words/s
[2019-07-17 12:48:16] Ep. 2 : Up. 152000 : Sen. 10,184,490 : Cost 69.33224487 : Time 843.59s : 7239.87 words/s
[2019-07-17 13:02:20] Ep. 2 : Up. 154000 : Sen. 10,473,597 : Cost 69.86600494 : Time 844.31s : 7241.48 words/s
[2019-07-17 13:16:24] Ep. 2 : Up. 156000 : Sen. 10,762,926 : Cost 69.25230408 : Time 843.88s : 7213.41 words/s
[2019-07-17 13:30:27] Ep. 2 : Up. 158000 : Sen. 11,052,008 : Cost 69.47306061 : Time 843.39s : 7241.74 words/s
[2019-07-17 13:44:29] Ep. 2 : Up. 160000 : Sen. 11,341,502 : Cost 69.20060730 : Time 842.35s : 7252.75 words/s
[2019-07-17 13:44:29] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.orig.npz
[2019-07-17 13:44:39] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.iter160000.npz
[2019-07-17 13:44:45] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz
[2019-07-17 13:44:55] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.25/model/model.npz.optimizer.npz
[2019-07-17 13:45:19] [valid] Ep. 2 : Up. 160000 : cross-entropy : 56.1797 : new best
[2019-07-17 13:45:26] [valid] Ep. 2 : Up. 160000 : perplexity : 9.26345 : new best
[2019-07-17 13:46:36] [valid] Ep. 2 : Up. 160000 : translation : 24.56 : new best
[2019-07-17 14:00:42] Ep. 2 : Up. 162000 : Sen. 11,631,397 : Cost 68.98062897 : Time 972.71s : 6276.65 words/s
[2019-07-17 14:08:41] Seen 11795613 samples
[2019-07-17 14:08:41] Starting epoch 3
[2019-07-17 14:08:41] [data] Shuffling data
[2019-07-17 14:08:49] [data] Done reading 13926791 sentences
[2019-07-17 14:09:56] [data] Done shuffling 13926791 sentences to temp files
[2019-07-17 14:16:04] Ep. 3 : Up. 164000 : Sen. 124,795 : Cost 68.69310760 : Time 921.76s : 6611.20 words/s
[2019-07-17 14:30:09] Ep. 3 : Up. 166000 : Sen. 413,408 : Cost 68.51149750 : Time 845.29s : 7227.13 words/s
[2019-07-17 14:44:12] Ep. 3 : Up. 168000 : Sen. 702,503 : Cost 68.33521271 : Time 842.90s : 7238.98 words/s
[2019-07-17 14:58:16] Ep. 3 : Up. 170000 : Sen. 991,256 : Cost 68.52637482 : Time 843.57s : 7249.99 words/s
[2019-07-17 15:12:21] Ep. 3 : Up. 172000 : Sen. 1,280,792 : Cost 68.15972137 : Time 845.57s : 7230.47 words/s
