[2019-07-16 19:18:55] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-16 19:18:55] [marian] Running on hodor as process 138411 with command line:
[2019-07-16 19:18:55] [marian] /fs/bil0/abdel/marian-dev/build/marian --sync-sgd --model ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz -T . --devices 2 --train-sets ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.de ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.en --vocabs ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.de.json ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.en.json --mini-batch-fit -w 5000 --dim-vocabs 50000 50000 --layer-normalization --dropout-rnn 0.2 --dropout-src 0.1 --dropout-trg 0.1 --learn-rate 0.0001 --after-epochs 0 --early-stopping 5 --valid-freq 20000 --save-freq 20000 --disp-freq 2000 --valid-mini-batch 8 --valid-sets ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/dev.bpe.de ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/dev.bpe.en --valid-metrics cross-entropy perplexity translation --valid-translation-output ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/dev.out --valid-script-path ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/score-dev.sh --seed 1111 --exponential-smoothing --normalize=1 --beam-size=12 --quiet-translation --log ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/train.log --valid-log ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/valid.log
[2019-07-16 19:18:55] [config] after-batches: 0
[2019-07-16 19:18:55] [config] after-epochs: 0
[2019-07-16 19:18:55] [config] allow-unk: false
[2019-07-16 19:18:55] [config] beam-size: 12
[2019-07-16 19:18:55] [config] bert-class-symbol: "[CLS]"
[2019-07-16 19:18:55] [config] bert-mask-symbol: "[MASK]"
[2019-07-16 19:18:55] [config] bert-masking-fraction: 0.15
[2019-07-16 19:18:55] [config] bert-sep-symbol: "[SEP]"
[2019-07-16 19:18:55] [config] bert-train-type-embeddings: true
[2019-07-16 19:18:55] [config] bert-type-vocab-size: 2
[2019-07-16 19:18:55] [config] best-deep: false
[2019-07-16 19:18:55] [config] clip-gemm: 0
[2019-07-16 19:18:55] [config] clip-norm: 1
[2019-07-16 19:18:55] [config] cost-type: ce-mean
[2019-07-16 19:18:55] [config] cpu-threads: 0
[2019-07-16 19:18:55] [config] data-weighting: ""
[2019-07-16 19:18:55] [config] data-weighting-type: sentence
[2019-07-16 19:18:55] [config] dec-cell: gru
[2019-07-16 19:18:55] [config] dec-cell-base-depth: 2
[2019-07-16 19:18:55] [config] dec-cell-high-depth: 1
[2019-07-16 19:18:55] [config] dec-depth: 1
[2019-07-16 19:18:55] [config] devices:
[2019-07-16 19:18:55] [config]   - 2
[2019-07-16 19:18:55] [config] dim-emb: 512
[2019-07-16 19:18:55] [config] dim-rnn: 1024
[2019-07-16 19:18:55] [config] dim-vocabs:
[2019-07-16 19:18:55] [config]   - 50000
[2019-07-16 19:18:55] [config]   - 50000
[2019-07-16 19:18:55] [config] disp-first: 0
[2019-07-16 19:18:55] [config] disp-freq: 2000
[2019-07-16 19:18:55] [config] disp-label-counts: false
[2019-07-16 19:18:55] [config] dropout-rnn: 0.2
[2019-07-16 19:18:55] [config] dropout-src: 0.1
[2019-07-16 19:18:55] [config] dropout-trg: 0.1
[2019-07-16 19:18:55] [config] dump-config: ""
[2019-07-16 19:18:55] [config] early-stopping: 5
[2019-07-16 19:18:55] [config] embedding-fix-src: false
[2019-07-16 19:18:55] [config] embedding-fix-trg: false
[2019-07-16 19:18:55] [config] embedding-normalization: false
[2019-07-16 19:18:55] [config] embedding-vectors:
[2019-07-16 19:18:55] [config]   []
[2019-07-16 19:18:55] [config] enc-cell: gru
[2019-07-16 19:18:55] [config] enc-cell-depth: 1
[2019-07-16 19:18:55] [config] enc-depth: 1
[2019-07-16 19:18:55] [config] enc-type: bidirectional
[2019-07-16 19:18:55] [config] exponential-smoothing: 0.0001
[2019-07-16 19:18:55] [config] grad-dropping-momentum: 0
[2019-07-16 19:18:55] [config] grad-dropping-rate: 0
[2019-07-16 19:18:55] [config] grad-dropping-warmup: 100
[2019-07-16 19:18:55] [config] guided-alignment: none
[2019-07-16 19:18:55] [config] guided-alignment-cost: mse
[2019-07-16 19:18:55] [config] guided-alignment-weight: 0.1
[2019-07-16 19:18:55] [config] ignore-model-config: false
[2019-07-16 19:18:55] [config] input-types:
[2019-07-16 19:18:55] [config]   []
[2019-07-16 19:18:55] [config] interpolate-env-vars: false
[2019-07-16 19:18:55] [config] keep-best: false
[2019-07-16 19:18:55] [config] label-smoothing: 0
[2019-07-16 19:18:55] [config] layer-normalization: true
[2019-07-16 19:18:55] [config] learn-rate: 0.0001
[2019-07-16 19:18:55] [config] log: ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/train.log
[2019-07-16 19:18:55] [config] log-level: info
[2019-07-16 19:18:55] [config] log-time-zone: ""
[2019-07-16 19:18:55] [config] lr-decay: 0
[2019-07-16 19:18:55] [config] lr-decay-freq: 50000
[2019-07-16 19:18:55] [config] lr-decay-inv-sqrt:
[2019-07-16 19:18:55] [config]   - 0
[2019-07-16 19:18:55] [config] lr-decay-repeat-warmup: false
[2019-07-16 19:18:55] [config] lr-decay-reset-optimizer: false
[2019-07-16 19:18:55] [config] lr-decay-start:
[2019-07-16 19:18:55] [config]   - 10
[2019-07-16 19:18:55] [config]   - 1
[2019-07-16 19:18:55] [config] lr-decay-strategy: epoch+stalled
[2019-07-16 19:18:55] [config] lr-report: false
[2019-07-16 19:18:55] [config] lr-warmup: 0
[2019-07-16 19:18:55] [config] lr-warmup-at-reload: false
[2019-07-16 19:18:55] [config] lr-warmup-cycle: false
[2019-07-16 19:18:55] [config] lr-warmup-start-rate: 0
[2019-07-16 19:18:55] [config] max-length: 50
[2019-07-16 19:18:55] [config] max-length-crop: false
[2019-07-16 19:18:55] [config] max-length-factor: 3
[2019-07-16 19:18:55] [config] maxi-batch: 100
[2019-07-16 19:18:55] [config] maxi-batch-sort: trg
[2019-07-16 19:18:55] [config] mini-batch: 64
[2019-07-16 19:18:55] [config] mini-batch-fit: true
[2019-07-16 19:18:55] [config] mini-batch-fit-step: 10
[2019-07-16 19:18:55] [config] mini-batch-overstuff: 1
[2019-07-16 19:18:55] [config] mini-batch-track-lr: false
[2019-07-16 19:18:55] [config] mini-batch-understuff: 1
[2019-07-16 19:18:55] [config] mini-batch-warmup: 0
[2019-07-16 19:18:55] [config] mini-batch-words: 0
[2019-07-16 19:18:55] [config] mini-batch-words-ref: 0
[2019-07-16 19:18:55] [config] model: ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz
[2019-07-16 19:18:55] [config] multi-loss-type: sum
[2019-07-16 19:18:55] [config] multi-node: false
[2019-07-16 19:18:55] [config] multi-node-overlap: true
[2019-07-16 19:18:55] [config] n-best: false
[2019-07-16 19:18:55] [config] no-nccl: false
[2019-07-16 19:18:55] [config] no-reload: false
[2019-07-16 19:18:55] [config] no-restore-corpus: false
[2019-07-16 19:18:55] [config] no-shuffle: false
[2019-07-16 19:18:55] [config] normalize: 1
[2019-07-16 19:18:55] [config] num-devices: 0
[2019-07-16 19:18:55] [config] optimizer: adam
[2019-07-16 19:18:55] [config] optimizer-delay: 1
[2019-07-16 19:18:55] [config] optimizer-params:
[2019-07-16 19:18:55] [config]   []
[2019-07-16 19:18:55] [config] overwrite: false
[2019-07-16 19:18:55] [config] pretrained-model: ""
[2019-07-16 19:18:55] [config] quiet: false
[2019-07-16 19:18:55] [config] quiet-translation: true
[2019-07-16 19:18:55] [config] relative-paths: false
[2019-07-16 19:18:55] [config] right-left: false
[2019-07-16 19:18:55] [config] save-freq: 20000
[2019-07-16 19:18:55] [config] seed: 1111
[2019-07-16 19:18:55] [config] shuffle-in-ram: false
[2019-07-16 19:18:55] [config] skip: false
[2019-07-16 19:18:55] [config] sqlite: ""
[2019-07-16 19:18:55] [config] sqlite-drop: false
[2019-07-16 19:18:55] [config] sync-sgd: true
[2019-07-16 19:18:55] [config] tempdir: .
[2019-07-16 19:18:55] [config] tied-embeddings: false
[2019-07-16 19:18:55] [config] tied-embeddings-all: false
[2019-07-16 19:18:55] [config] tied-embeddings-src: false
[2019-07-16 19:18:55] [config] train-sets:
[2019-07-16 19:18:55] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.de
[2019-07-16 19:18:55] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.en
[2019-07-16 19:18:55] [config] transformer-aan-activation: swish
[2019-07-16 19:18:55] [config] transformer-aan-depth: 2
[2019-07-16 19:18:55] [config] transformer-aan-nogate: false
[2019-07-16 19:18:55] [config] transformer-decoder-autoreg: self-attention
[2019-07-16 19:18:55] [config] transformer-dim-aan: 2048
[2019-07-16 19:18:55] [config] transformer-dim-ffn: 2048
[2019-07-16 19:18:55] [config] transformer-dropout: 0
[2019-07-16 19:18:55] [config] transformer-dropout-attention: 0
[2019-07-16 19:18:55] [config] transformer-dropout-ffn: 0
[2019-07-16 19:18:55] [config] transformer-ffn-activation: swish
[2019-07-16 19:18:55] [config] transformer-ffn-depth: 2
[2019-07-16 19:18:55] [config] transformer-guided-alignment-layer: last
[2019-07-16 19:18:55] [config] transformer-heads: 8
[2019-07-16 19:18:55] [config] transformer-no-projection: false
[2019-07-16 19:18:55] [config] transformer-postprocess: dan
[2019-07-16 19:18:55] [config] transformer-postprocess-emb: d
[2019-07-16 19:18:55] [config] transformer-preprocess: ""
[2019-07-16 19:18:55] [config] transformer-tied-layers:
[2019-07-16 19:18:55] [config]   []
[2019-07-16 19:18:55] [config] transformer-train-position-embeddings: false
[2019-07-16 19:18:55] [config] type: amun
[2019-07-16 19:18:55] [config] ulr: false
[2019-07-16 19:18:55] [config] ulr-dim-emb: 0
[2019-07-16 19:18:55] [config] ulr-dropout: 0
[2019-07-16 19:18:55] [config] ulr-keys-vectors: ""
[2019-07-16 19:18:55] [config] ulr-query-vectors: ""
[2019-07-16 19:18:55] [config] ulr-softmax-temperature: 1
[2019-07-16 19:18:55] [config] ulr-trainable-transformation: false
[2019-07-16 19:18:55] [config] valid-freq: 20000
[2019-07-16 19:18:55] [config] valid-log: ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/valid.log
[2019-07-16 19:18:55] [config] valid-max-length: 1000
[2019-07-16 19:18:55] [config] valid-metrics:
[2019-07-16 19:18:55] [config]   - cross-entropy
[2019-07-16 19:18:55] [config]   - perplexity
[2019-07-16 19:18:55] [config]   - translation
[2019-07-16 19:18:55] [config] valid-mini-batch: 8
[2019-07-16 19:18:55] [config] valid-script-path: ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/score-dev.sh
[2019-07-16 19:18:55] [config] valid-sets:
[2019-07-16 19:18:55] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/dev.bpe.de
[2019-07-16 19:18:55] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/dev.bpe.en
[2019-07-16 19:18:55] [config] valid-translation-output: ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/dev.out
[2019-07-16 19:18:55] [config] vocabs:
[2019-07-16 19:18:55] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.de.json
[2019-07-16 19:18:55] [config]   - ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.en.json
[2019-07-16 19:18:55] [config] word-penalty: 0
[2019-07-16 19:18:55] [config] workspace: 5000
[2019-07-16 19:18:55] [config] Model is being created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-16 19:18:55] Using synchronous training
[2019-07-16 19:18:55] [data] Loading vocabulary from JSON/Yaml file ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.de.json
[2019-07-16 19:18:56] [data] Using unused word id eos for 0
[2019-07-16 19:18:56] [data] Using unused word id UNK for 1
[2019-07-16 19:18:56] [data] Setting vocabulary size for input 0 to 50000
[2019-07-16 19:18:56] [data] Loading vocabulary from JSON/Yaml file ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/data/train.bpe.en.json
[2019-07-16 19:18:56] [data] Using unused word id eos for 0
[2019-07-16 19:18:56] [data] Using unused word id UNK for 1
[2019-07-16 19:18:56] [data] Setting vocabulary size for input 1 to 50000
[2019-07-16 19:18:56] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-16 19:18:56] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-16 19:18:57] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-16 19:18:57] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-16 19:18:57] [comm] NCCLCommunicator constructed successfully.
[2019-07-16 19:18:57] [training] Using 1 GPUs
[2019-07-16 19:18:57] [memory] Reserving 422 MB, device gpu2
[2019-07-16 19:18:57] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-16 19:18:57] [memory] Reserving 422 MB, device gpu2
[2019-07-16 19:19:06] [batching] Done. Typical MB size is 6880 target words
[2019-07-16 19:19:06] [memory] Extending reserved space to 5120 MB (device gpu2)
[2019-07-16 19:19:06] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-16 19:19:06] [comm] NCCLCommunicator constructed successfully.
[2019-07-16 19:19:06] [training] Using 1 GPUs
[2019-07-16 19:19:06] Training started
[2019-07-16 19:19:06] [data] Shuffling data
[2019-07-16 19:19:15] [data] Done reading 13926791 sentences
[2019-07-16 19:20:34] [data] Done shuffling 13926791 sentences to temp files
[2019-07-16 19:28:35] [training] Batches are processed as 1 process(es) x 1 devices/process
[2019-07-16 19:28:35] [memory] Reserving 422 MB, device gpu2
[2019-07-16 19:28:35] [memory] Reserving 422 MB, device gpu2
[2019-07-16 19:28:35] [memory] Reserving 422 MB, device gpu2
[2019-07-16 19:28:35] [memory] Reserving 844 MB, device gpu2
[2019-07-16 19:42:12] Ep. 1 : Up. 2000 : Sen. 288,980 : Cost 136.62452698 : Time 1396.09s : 4359.68 words/s
[2019-07-16 19:55:59] Ep. 1 : Up. 4000 : Sen. 578,204 : Cost 119.16773224 : Time 826.45s : 7409.96 words/s
[2019-07-16 20:09:45] Ep. 1 : Up. 6000 : Sen. 867,195 : Cost 110.65970612 : Time 826.13s : 7384.02 words/s
[2019-07-16 20:23:32] Ep. 1 : Up. 8000 : Sen. 1,156,773 : Cost 104.72293091 : Time 826.99s : 7376.25 words/s
[2019-07-16 20:37:16] Ep. 1 : Up. 10000 : Sen. 1,446,400 : Cost 101.09782410 : Time 824.53s : 7428.55 words/s
[2019-07-16 20:50:58] Ep. 1 : Up. 12000 : Sen. 1,735,540 : Cost 97.63807678 : Time 821.85s : 7426.62 words/s
[2019-07-16 21:04:40] Ep. 1 : Up. 14000 : Sen. 2,024,932 : Cost 94.92061615 : Time 821.84s : 7434.00 words/s
[2019-07-16 21:18:22] Ep. 1 : Up. 16000 : Sen. 2,313,850 : Cost 92.86993408 : Time 821.53s : 7432.15 words/s
[2019-07-16 21:32:05] Ep. 1 : Up. 18000 : Sen. 2,603,358 : Cost 90.79431152 : Time 823.03s : 7430.78 words/s
[2019-07-16 21:45:48] Ep. 1 : Up. 20000 : Sen. 2,893,081 : Cost 89.36719513 : Time 823.73s : 7435.59 words/s
[2019-07-16 21:45:48] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.orig.npz
[2019-07-16 21:45:58] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.iter20000.npz
[2019-07-16 21:46:04] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz
[2019-07-16 21:46:13] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.optimizer.npz
[2019-07-16 21:46:36] [valid] Ep. 1 : Up. 20000 : cross-entropy : 90.349 : new best
[2019-07-16 21:46:44] [valid] Ep. 1 : Up. 20000 : perplexity : 35.8756 : new best
[2019-07-16 21:48:06] [valid] Ep. 1 : Up. 20000 : translation : 12.28 : new best
[2019-07-16 22:01:50] Ep. 1 : Up. 22000 : Sen. 3,182,468 : Cost 87.83905029 : Time 961.27s : 6351.98 words/s
[2019-07-16 22:15:29] Ep. 1 : Up. 24000 : Sen. 3,470,162 : Cost 86.87938690 : Time 819.05s : 7426.85 words/s
[2019-07-16 22:29:08] Ep. 1 : Up. 26000 : Sen. 3,759,351 : Cost 85.33298492 : Time 818.85s : 7448.48 words/s
[2019-07-16 22:42:47] Ep. 1 : Up. 28000 : Sen. 4,047,723 : Cost 84.48619843 : Time 819.06s : 7428.28 words/s
[2019-07-16 22:56:27] Ep. 1 : Up. 30000 : Sen. 4,336,492 : Cost 84.10002136 : Time 820.67s : 7453.49 words/s
[2019-07-16 23:10:10] Ep. 1 : Up. 32000 : Sen. 4,626,630 : Cost 82.78398895 : Time 823.07s : 7434.91 words/s
[2019-07-16 23:23:52] Ep. 1 : Up. 34000 : Sen. 4,916,290 : Cost 82.29768372 : Time 821.94s : 7442.62 words/s
[2019-07-16 23:37:34] Ep. 1 : Up. 36000 : Sen. 5,205,537 : Cost 81.53311920 : Time 821.89s : 7428.39 words/s
[2019-07-16 23:51:12] Ep. 1 : Up. 38000 : Sen. 5,492,818 : Cost 80.97732544 : Time 817.76s : 7411.89 words/s
[2019-07-17 00:04:54] Ep. 1 : Up. 40000 : Sen. 5,782,148 : Cost 80.28613281 : Time 821.86s : 7436.37 words/s
[2019-07-17 00:04:54] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.orig.npz
[2019-07-17 00:05:03] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.iter40000.npz
[2019-07-17 00:05:10] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz
[2019-07-17 00:05:20] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.optimizer.npz
[2019-07-17 00:05:43] [valid] Ep. 1 : Up. 40000 : cross-entropy : 72.8121 : new best
[2019-07-17 00:05:51] [valid] Ep. 1 : Up. 40000 : perplexity : 17.9065 : new best
[2019-07-17 00:07:01] [valid] Ep. 1 : Up. 40000 : translation : 19.71 : new best
[2019-07-17 00:20:42] Ep. 1 : Up. 42000 : Sen. 6,070,775 : Cost 79.88948822 : Time 948.23s : 6423.72 words/s
[2019-07-17 00:34:22] Ep. 1 : Up. 44000 : Sen. 6,359,473 : Cost 79.44146729 : Time 819.41s : 7433.84 words/s
[2019-07-17 00:48:00] Ep. 1 : Up. 46000 : Sen. 6,646,841 : Cost 79.19536591 : Time 818.01s : 7424.91 words/s
[2019-07-17 01:01:39] Ep. 1 : Up. 48000 : Sen. 6,936,012 : Cost 78.38624573 : Time 819.66s : 7428.86 words/s
[2019-07-17 01:15:20] Ep. 1 : Up. 50000 : Sen. 7,225,030 : Cost 78.05609894 : Time 821.05s : 7432.34 words/s
[2019-07-17 01:29:02] Ep. 1 : Up. 52000 : Sen. 7,514,444 : Cost 77.93701935 : Time 821.98s : 7452.72 words/s
[2019-07-17 01:42:42] Ep. 1 : Up. 54000 : Sen. 7,804,046 : Cost 77.27222443 : Time 819.38s : 7449.49 words/s
[2019-07-17 01:56:23] Ep. 1 : Up. 56000 : Sen. 8,092,764 : Cost 77.37424469 : Time 821.19s : 7447.21 words/s
[2019-07-17 02:10:01] Ep. 1 : Up. 58000 : Sen. 8,382,362 : Cost 76.54847717 : Time 818.44s : 7456.86 words/s
[2019-07-17 02:23:43] Ep. 1 : Up. 60000 : Sen. 8,670,900 : Cost 76.43180084 : Time 821.27s : 7428.53 words/s
[2019-07-17 02:23:43] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.orig.npz
[2019-07-17 02:23:51] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.iter60000.npz
[2019-07-17 02:23:58] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz
[2019-07-17 02:24:07] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.optimizer.npz
[2019-07-17 02:24:30] [valid] Ep. 1 : Up. 60000 : cross-entropy : 65.8003 : new best
[2019-07-17 02:24:38] [valid] Ep. 1 : Up. 60000 : perplexity : 13.5627 : new best
[2019-07-17 02:25:46] [valid] Ep. 1 : Up. 60000 : translation : 22.01 : new best
[2019-07-17 02:39:28] Ep. 1 : Up. 62000 : Sen. 8,959,398 : Cost 76.28929138 : Time 945.93s : 6457.26 words/s
[2019-07-17 02:53:12] Ep. 1 : Up. 64000 : Sen. 9,248,575 : Cost 75.99856567 : Time 823.21s : 7417.13 words/s
[2019-07-17 03:06:55] Ep. 1 : Up. 66000 : Sen. 9,537,777 : Cost 75.94011688 : Time 823.27s : 7434.61 words/s
[2019-07-17 03:20:34] Ep. 1 : Up. 68000 : Sen. 9,827,260 : Cost 75.09325409 : Time 819.29s : 7428.85 words/s
[2019-07-17 03:34:21] Ep. 1 : Up. 70000 : Sen. 10,118,400 : Cost 75.31320953 : Time 827.09s : 7444.13 words/s
[2019-07-17 03:48:04] Ep. 1 : Up. 72000 : Sen. 10,408,567 : Cost 74.94480896 : Time 822.58s : 7438.55 words/s
[2019-07-17 04:01:43] Ep. 1 : Up. 74000 : Sen. 10,697,490 : Cost 74.56430817 : Time 819.46s : 7437.79 words/s
[2019-07-17 04:15:28] Ep. 1 : Up. 76000 : Sen. 10,987,778 : Cost 74.48430634 : Time 824.91s : 7438.07 words/s
[2019-07-17 04:29:14] Ep. 1 : Up. 78000 : Sen. 11,278,400 : Cost 74.19329071 : Time 825.64s : 7425.85 words/s
[2019-07-17 04:42:57] Ep. 1 : Up. 80000 : Sen. 11,567,590 : Cost 74.13132477 : Time 823.34s : 7430.83 words/s
[2019-07-17 04:42:57] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.orig.npz
[2019-07-17 04:43:08] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.iter80000.npz
[2019-07-17 04:43:14] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz
[2019-07-17 04:43:23] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.optimizer.npz
[2019-07-17 04:43:47] [valid] Ep. 1 : Up. 80000 : cross-entropy : 61.9437 : new best
[2019-07-17 04:43:55] [valid] Ep. 1 : Up. 80000 : perplexity : 11.6407 : new best
[2019-07-17 04:45:01] [valid] Ep. 1 : Up. 80000 : translation : 23.2 : new best
[2019-07-17 04:55:51] Seen 11795642 samples
[2019-07-17 04:55:51] Starting epoch 2
[2019-07-17 04:55:51] [data] Shuffling data
[2019-07-17 04:55:58] [data] Done reading 13926791 sentences
[2019-07-17 04:56:54] [data] Done shuffling 13926791 sentences to temp files
[2019-07-17 04:59:51] Ep. 2 : Up. 82000 : Sen. 60,410 : Cost 73.61170197 : Time 1013.50s : 6003.09 words/s
[2019-07-17 05:13:36] Ep. 2 : Up. 84000 : Sen. 349,577 : Cost 73.21417236 : Time 825.18s : 7403.09 words/s
[2019-07-17 05:27:19] Ep. 2 : Up. 86000 : Sen. 638,748 : Cost 72.79898071 : Time 822.84s : 7412.00 words/s
[2019-07-17 05:41:02] Ep. 2 : Up. 88000 : Sen. 927,107 : Cost 72.82216644 : Time 823.37s : 7403.56 words/s
[2019-07-17 05:54:45] Ep. 2 : Up. 90000 : Sen. 1,215,778 : Cost 72.65888977 : Time 822.79s : 7412.49 words/s
[2019-07-17 06:08:32] Ep. 2 : Up. 92000 : Sen. 1,505,435 : Cost 72.47507477 : Time 827.31s : 7396.29 words/s
[2019-07-17 06:22:17] Ep. 2 : Up. 94000 : Sen. 1,795,155 : Cost 72.24263763 : Time 824.88s : 7415.26 words/s
[2019-07-17 06:36:00] Ep. 2 : Up. 96000 : Sen. 2,083,865 : Cost 72.03442383 : Time 823.24s : 7409.62 words/s
[2019-07-17 06:49:43] Ep. 2 : Up. 98000 : Sen. 2,373,740 : Cost 71.76277161 : Time 822.86s : 7427.57 words/s
[2019-07-17 07:03:29] Ep. 2 : Up. 100000 : Sen. 2,662,865 : Cost 72.10218048 : Time 825.30s : 7403.67 words/s
[2019-07-17 07:03:29] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.orig.npz
[2019-07-17 07:03:41] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.iter100000.npz
[2019-07-17 07:03:49] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz
[2019-07-17 07:03:59] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.optimizer.npz
[2019-07-17 07:04:22] [valid] Ep. 2 : Up. 100000 : cross-entropy : 59.5456 : new best
[2019-07-17 07:04:30] [valid] Ep. 2 : Up. 100000 : perplexity : 10.5854 : new best
[2019-07-17 07:05:37] [valid] Ep. 2 : Up. 100000 : translation : 23.75 : new best
[2019-07-17 07:19:21] Ep. 2 : Up. 102000 : Sen. 2,951,874 : Cost 72.00003815 : Time 952.22s : 6410.96 words/s
[2019-07-17 07:33:04] Ep. 2 : Up. 104000 : Sen. 3,241,797 : Cost 71.74525452 : Time 823.43s : 7435.20 words/s
[2019-07-17 07:46:45] Ep. 2 : Up. 106000 : Sen. 3,530,776 : Cost 71.67417145 : Time 821.18s : 7442.29 words/s
[2019-07-17 08:00:28] Ep. 2 : Up. 108000 : Sen. 3,819,694 : Cost 71.70529175 : Time 822.60s : 7427.63 words/s
[2019-07-17 08:14:10] Ep. 2 : Up. 110000 : Sen. 4,109,642 : Cost 71.12937927 : Time 822.11s : 7430.69 words/s
[2019-07-17 08:28:14] Ep. 2 : Up. 112000 : Sen. 4,398,482 : Cost 71.20397186 : Time 843.90s : 7238.45 words/s
[2019-07-17 08:41:54] Ep. 2 : Up. 114000 : Sen. 4,687,361 : Cost 71.20008087 : Time 820.23s : 7439.11 words/s
[2019-07-17 08:55:37] Ep. 2 : Up. 116000 : Sen. 4,977,397 : Cost 70.83625793 : Time 822.67s : 7427.96 words/s
[2019-07-17 09:09:19] Ep. 2 : Up. 118000 : Sen. 5,266,804 : Cost 71.16506195 : Time 822.34s : 7445.79 words/s
[2019-07-17 09:23:03] Ep. 2 : Up. 120000 : Sen. 5,556,778 : Cost 71.16334534 : Time 823.86s : 7439.15 words/s
[2019-07-17 09:23:03] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.orig.npz
[2019-07-17 09:23:12] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.iter120000.npz
[2019-07-17 09:23:18] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz
[2019-07-17 09:23:27] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.optimizer.npz
[2019-07-17 09:23:57] [valid] Ep. 2 : Up. 120000 : cross-entropy : 57.7754 : new best
[2019-07-17 09:24:05] [valid] Ep. 2 : Up. 120000 : perplexity : 9.8684 : new best
[2019-07-17 09:25:12] [valid] Ep. 2 : Up. 120000 : translation : 23.93 : new best
[2019-07-17 09:38:55] Ep. 2 : Up. 122000 : Sen. 5,846,025 : Cost 70.73973083 : Time 951.77s : 6420.82 words/s
[2019-07-17 09:52:34] Ep. 2 : Up. 124000 : Sen. 6,135,253 : Cost 70.66624451 : Time 819.63s : 7444.73 words/s
[2019-07-17 10:06:14] Ep. 2 : Up. 126000 : Sen. 6,423,918 : Cost 70.30908966 : Time 819.60s : 7432.06 words/s
[2019-07-17 10:19:58] Ep. 2 : Up. 128000 : Sen. 6,712,684 : Cost 70.56787109 : Time 824.12s : 7411.73 words/s
[2019-07-17 10:33:42] Ep. 2 : Up. 130000 : Sen. 7,001,732 : Cost 70.39695740 : Time 824.08s : 7409.84 words/s
[2019-07-17 10:47:29] Ep. 2 : Up. 132000 : Sen. 7,292,187 : Cost 70.23033905 : Time 826.35s : 7425.57 words/s
[2019-07-17 11:01:14] Ep. 2 : Up. 134000 : Sen. 7,581,804 : Cost 70.00615692 : Time 825.32s : 7398.84 words/s
[2019-07-17 11:14:58] Ep. 2 : Up. 136000 : Sen. 7,870,488 : Cost 70.28710938 : Time 823.69s : 7406.58 words/s
[2019-07-17 11:28:41] Ep. 2 : Up. 138000 : Sen. 8,160,536 : Cost 69.84732056 : Time 823.79s : 7423.37 words/s
[2019-07-17 11:42:23] Ep. 2 : Up. 140000 : Sen. 8,449,374 : Cost 69.94077301 : Time 821.60s : 7410.44 words/s
[2019-07-17 11:42:23] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.orig.npz
[2019-07-17 11:42:31] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.iter140000.npz
[2019-07-17 11:42:38] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz
[2019-07-17 11:42:48] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.optimizer.npz
[2019-07-17 11:43:11] [valid] Ep. 2 : Up. 140000 : cross-entropy : 56.4733 : new best
[2019-07-17 11:43:19] [valid] Ep. 2 : Up. 140000 : perplexity : 9.37212 : new best
[2019-07-17 11:44:27] [valid] Ep. 2 : Up. 140000 : translation : 24.18 : new best
[2019-07-17 11:58:12] Ep. 2 : Up. 142000 : Sen. 8,739,039 : Cost 69.85530090 : Time 948.76s : 6460.31 words/s
[2019-07-17 12:11:54] Ep. 2 : Up. 144000 : Sen. 9,028,525 : Cost 69.70816040 : Time 822.25s : 7432.05 words/s
[2019-07-17 12:25:34] Ep. 2 : Up. 146000 : Sen. 9,316,786 : Cost 69.75769043 : Time 820.19s : 7429.88 words/s
[2019-07-17 12:39:16] Ep. 2 : Up. 148000 : Sen. 9,605,740 : Cost 69.60867310 : Time 821.43s : 7413.12 words/s
[2019-07-17 12:52:59] Ep. 2 : Up. 150000 : Sen. 9,894,532 : Cost 69.97401428 : Time 823.31s : 7426.90 words/s
[2019-07-17 13:06:42] Ep. 2 : Up. 152000 : Sen. 10,184,073 : Cost 69.37326050 : Time 822.85s : 7424.37 words/s
[2019-07-17 13:20:25] Ep. 2 : Up. 154000 : Sen. 10,473,354 : Cost 69.59738922 : Time 823.34s : 7424.72 words/s
[2019-07-17 13:34:09] Ep. 2 : Up. 156000 : Sen. 10,762,665 : Cost 68.97890472 : Time 823.70s : 7393.31 words/s
[2019-07-17 13:47:54] Ep. 2 : Up. 158000 : Sen. 11,052,008 : Cost 69.12144470 : Time 824.95s : 7409.24 words/s
[2019-07-17 14:01:40] Ep. 2 : Up. 160000 : Sen. 11,341,996 : Cost 69.12372589 : Time 826.15s : 7406.57 words/s
[2019-07-17 14:01:40] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.orig.npz
[2019-07-17 14:01:48] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.iter160000.npz
[2019-07-17 14:01:55] Saving model to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz
[2019-07-17 14:02:04] Saving Adam parameters to ../experiments/10M_fasttext_prob_x_len_+_ced_0.5/model/model.npz.optimizer.npz
[2019-07-17 14:02:29] [valid] Ep. 2 : Up. 160000 : cross-entropy : 55.4458 : new best
[2019-07-17 14:02:36] [valid] Ep. 2 : Up. 160000 : perplexity : 8.99822 : new best
[2019-07-17 14:03:44] [valid] Ep. 2 : Up. 160000 : translation : 24.46 : new best
[2019-07-17 14:17:28] Ep. 2 : Up. 162000 : Sen. 11,631,223 : Cost 69.00500488 : Time 948.35s : 6429.61 words/s
[2019-07-17 14:25:17] Seen 11795642 samples
[2019-07-17 14:25:17] Starting epoch 3
[2019-07-17 14:25:17] [data] Shuffling data
[2019-07-17 14:25:25] [data] Done reading 13926791 sentences
[2019-07-17 14:26:25] [data] Done shuffling 13926791 sentences to temp files
[2019-07-17 14:32:23] Ep. 3 : Up. 164000 : Sen. 124,299 : Cost 68.56048584 : Time 894.71s : 6804.14 words/s
[2019-07-17 14:46:08] Ep. 3 : Up. 166000 : Sen. 412,997 : Cost 68.38823700 : Time 825.18s : 7400.06 words/s
[2019-07-17 14:59:52] Ep. 3 : Up. 168000 : Sen. 701,894 : Cost 68.12295532 : Time 824.08s : 7404.20 words/s
