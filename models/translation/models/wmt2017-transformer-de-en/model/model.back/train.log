[2019-07-07 17:08:02] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-07 17:08:02] [marian] Running on fulla as process 400699 with command line:
[2019-07-07 17:08:02] [marian] /fs/bil0/abdel/marian-dev/build/marian --model models/wmt2017-transformer-de-en/model/model.back/model.npz --type s2s --train-sets models/wmt2017-transformer-de-en/data/corpus.bpe.en models/wmt2017-transformer-de-en/data/corpus.bpe.de --max-length 100 --vocabs models/wmt2017-transformer-de-en/model/vocab.ende.yml models/wmt2017-transformer-de-en/model/vocab.ende.yml --mini-batch-fit -w 6000 --maxi-batch 1000 --valid-freq 10000 --save-freq 10000 --disp-freq 1000 --valid-metrics ce-mean-words perplexity translation --valid-script-path 'bash ./models/wmt2017-transformer-de-en/validate.de.sh' --valid-translation-output models/wmt2017-transformer-de-en/data/valid.bpe.en.output --quiet-translation --valid-sets models/wmt2017-transformer-de-en/data/valid.bpe.en models/wmt2017-transformer-de-en/data/valid.bpe.de --valid-mini-batch 64 --beam-size 12 --normalize=1 --overwrite --keep-best --early-stopping 5 --after-epochs 10 --cost-type=ce-mean-words --log models/wmt2017-transformer-de-en/model/model.back/train.log --valid-log models/wmt2017-transformer-de-en/model/model.back/valid.log --tied-embeddings-all --layer-normalization --devices 5 6 --seed 1111 --exponential-smoothing
[2019-07-07 17:08:02] [config] after-batches: 0
[2019-07-07 17:08:02] [config] after-epochs: 10
[2019-07-07 17:08:02] [config] allow-unk: false
[2019-07-07 17:08:02] [config] beam-size: 12
[2019-07-07 17:08:02] [config] bert-class-symbol: "[CLS]"
[2019-07-07 17:08:02] [config] bert-mask-symbol: "[MASK]"
[2019-07-07 17:08:02] [config] bert-masking-fraction: 0.15
[2019-07-07 17:08:02] [config] bert-sep-symbol: "[SEP]"
[2019-07-07 17:08:02] [config] bert-train-type-embeddings: true
[2019-07-07 17:08:02] [config] bert-type-vocab-size: 2
[2019-07-07 17:08:02] [config] best-deep: false
[2019-07-07 17:08:02] [config] clip-gemm: 0
[2019-07-07 17:08:02] [config] clip-norm: 1
[2019-07-07 17:08:02] [config] cost-type: ce-mean-words
[2019-07-07 17:08:02] [config] cpu-threads: 0
[2019-07-07 17:08:02] [config] data-weighting: ""
[2019-07-07 17:08:02] [config] data-weighting-type: sentence
[2019-07-07 17:08:02] [config] dec-cell: gru
[2019-07-07 17:08:02] [config] dec-cell-base-depth: 2
[2019-07-07 17:08:02] [config] dec-cell-high-depth: 1
[2019-07-07 17:08:02] [config] dec-depth: 1
[2019-07-07 17:08:02] [config] devices:
[2019-07-07 17:08:02] [config]   - 5
[2019-07-07 17:08:02] [config]   - 6
[2019-07-07 17:08:02] [config] dim-emb: 512
[2019-07-07 17:08:02] [config] dim-rnn: 1024
[2019-07-07 17:08:02] [config] dim-vocabs:
[2019-07-07 17:08:02] [config]   - 0
[2019-07-07 17:08:02] [config]   - 0
[2019-07-07 17:08:02] [config] disp-first: 0
[2019-07-07 17:08:02] [config] disp-freq: 1000
[2019-07-07 17:08:02] [config] disp-label-counts: false
[2019-07-07 17:08:02] [config] dropout-rnn: 0
[2019-07-07 17:08:02] [config] dropout-src: 0
[2019-07-07 17:08:02] [config] dropout-trg: 0
[2019-07-07 17:08:02] [config] dump-config: ""
[2019-07-07 17:08:02] [config] early-stopping: 5
[2019-07-07 17:08:02] [config] embedding-fix-src: false
[2019-07-07 17:08:02] [config] embedding-fix-trg: false
[2019-07-07 17:08:02] [config] embedding-normalization: false
[2019-07-07 17:08:02] [config] embedding-vectors:
[2019-07-07 17:08:02] [config]   []
[2019-07-07 17:08:02] [config] enc-cell: gru
[2019-07-07 17:08:02] [config] enc-cell-depth: 1
[2019-07-07 17:08:02] [config] enc-depth: 1
[2019-07-07 17:08:02] [config] enc-type: bidirectional
[2019-07-07 17:08:02] [config] exponential-smoothing: 0.0001
[2019-07-07 17:08:02] [config] grad-dropping-momentum: 0
[2019-07-07 17:08:02] [config] grad-dropping-rate: 0
[2019-07-07 17:08:02] [config] grad-dropping-warmup: 100
[2019-07-07 17:08:02] [config] guided-alignment: none
[2019-07-07 17:08:02] [config] guided-alignment-cost: mse
[2019-07-07 17:08:02] [config] guided-alignment-weight: 0.1
[2019-07-07 17:08:02] [config] ignore-model-config: false
[2019-07-07 17:08:02] [config] input-types:
[2019-07-07 17:08:02] [config]   []
[2019-07-07 17:08:02] [config] interpolate-env-vars: false
[2019-07-07 17:08:02] [config] keep-best: true
[2019-07-07 17:08:02] [config] label-smoothing: 0
[2019-07-07 17:08:02] [config] layer-normalization: true
[2019-07-07 17:08:02] [config] learn-rate: 0.0001
[2019-07-07 17:08:02] [config] log: models/wmt2017-transformer-de-en/model/model.back/train.log
[2019-07-07 17:08:02] [config] log-level: info
[2019-07-07 17:08:02] [config] log-time-zone: ""
[2019-07-07 17:08:02] [config] lr-decay: 0
[2019-07-07 17:08:02] [config] lr-decay-freq: 50000
[2019-07-07 17:08:02] [config] lr-decay-inv-sqrt:
[2019-07-07 17:08:02] [config]   - 0
[2019-07-07 17:08:02] [config] lr-decay-repeat-warmup: false
[2019-07-07 17:08:02] [config] lr-decay-reset-optimizer: false
[2019-07-07 17:08:02] [config] lr-decay-start:
[2019-07-07 17:08:02] [config]   - 10
[2019-07-07 17:08:02] [config]   - 1
[2019-07-07 17:08:02] [config] lr-decay-strategy: epoch+stalled
[2019-07-07 17:08:02] [config] lr-report: false
[2019-07-07 17:08:02] [config] lr-warmup: 0
[2019-07-07 17:08:02] [config] lr-warmup-at-reload: false
[2019-07-07 17:08:02] [config] lr-warmup-cycle: false
[2019-07-07 17:08:02] [config] lr-warmup-start-rate: 0
[2019-07-07 17:08:02] [config] max-length: 100
[2019-07-07 17:08:02] [config] max-length-crop: false
[2019-07-07 17:08:02] [config] max-length-factor: 3
[2019-07-07 17:08:02] [config] maxi-batch: 1000
[2019-07-07 17:08:02] [config] maxi-batch-sort: trg
[2019-07-07 17:08:02] [config] mini-batch: 64
[2019-07-07 17:08:02] [config] mini-batch-fit: true
[2019-07-07 17:08:02] [config] mini-batch-fit-step: 10
[2019-07-07 17:08:02] [config] mini-batch-overstuff: 1
[2019-07-07 17:08:02] [config] mini-batch-track-lr: false
[2019-07-07 17:08:02] [config] mini-batch-understuff: 1
[2019-07-07 17:08:02] [config] mini-batch-warmup: 0
[2019-07-07 17:08:02] [config] mini-batch-words: 0
[2019-07-07 17:08:02] [config] mini-batch-words-ref: 0
[2019-07-07 17:08:02] [config] model: models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 17:08:02] [config] multi-loss-type: sum
[2019-07-07 17:08:02] [config] multi-node: false
[2019-07-07 17:08:02] [config] multi-node-overlap: true
[2019-07-07 17:08:02] [config] n-best: false
[2019-07-07 17:08:02] [config] no-nccl: false
[2019-07-07 17:08:02] [config] no-reload: false
[2019-07-07 17:08:02] [config] no-restore-corpus: false
[2019-07-07 17:08:02] [config] no-shuffle: false
[2019-07-07 17:08:02] [config] normalize: 1
[2019-07-07 17:08:02] [config] num-devices: 0
[2019-07-07 17:08:02] [config] optimizer: adam
[2019-07-07 17:08:02] [config] optimizer-delay: 1
[2019-07-07 17:08:02] [config] optimizer-params:
[2019-07-07 17:08:02] [config]   []
[2019-07-07 17:08:02] [config] overwrite: true
[2019-07-07 17:08:02] [config] pretrained-model: ""
[2019-07-07 17:08:02] [config] quiet: false
[2019-07-07 17:08:02] [config] quiet-translation: true
[2019-07-07 17:08:02] [config] relative-paths: false
[2019-07-07 17:08:02] [config] right-left: false
[2019-07-07 17:08:02] [config] save-freq: 10000
[2019-07-07 17:08:02] [config] seed: 1111
[2019-07-07 17:08:02] [config] shuffle-in-ram: false
[2019-07-07 17:08:02] [config] skip: false
[2019-07-07 17:08:02] [config] sqlite: ""
[2019-07-07 17:08:02] [config] sqlite-drop: false
[2019-07-07 17:08:02] [config] sync-sgd: false
[2019-07-07 17:08:02] [config] tempdir: /tmp
[2019-07-07 17:08:02] [config] tied-embeddings: false
[2019-07-07 17:08:02] [config] tied-embeddings-all: true
[2019-07-07 17:08:02] [config] tied-embeddings-src: false
[2019-07-07 17:08:02] [config] train-sets:
[2019-07-07 17:08:02] [config]   - models/wmt2017-transformer-de-en/data/corpus.bpe.en
[2019-07-07 17:08:02] [config]   - models/wmt2017-transformer-de-en/data/corpus.bpe.de
[2019-07-07 17:08:02] [config] transformer-aan-activation: swish
[2019-07-07 17:08:02] [config] transformer-aan-depth: 2
[2019-07-07 17:08:02] [config] transformer-aan-nogate: false
[2019-07-07 17:08:02] [config] transformer-decoder-autoreg: self-attention
[2019-07-07 17:08:02] [config] transformer-dim-aan: 2048
[2019-07-07 17:08:02] [config] transformer-dim-ffn: 2048
[2019-07-07 17:08:02] [config] transformer-dropout: 0
[2019-07-07 17:08:02] [config] transformer-dropout-attention: 0
[2019-07-07 17:08:02] [config] transformer-dropout-ffn: 0
[2019-07-07 17:08:02] [config] transformer-ffn-activation: swish
[2019-07-07 17:08:02] [config] transformer-ffn-depth: 2
[2019-07-07 17:08:02] [config] transformer-guided-alignment-layer: last
[2019-07-07 17:08:02] [config] transformer-heads: 8
[2019-07-07 17:08:02] [config] transformer-no-projection: false
[2019-07-07 17:08:02] [config] transformer-postprocess: dan
[2019-07-07 17:08:02] [config] transformer-postprocess-emb: d
[2019-07-07 17:08:02] [config] transformer-preprocess: ""
[2019-07-07 17:08:02] [config] transformer-tied-layers:
[2019-07-07 17:08:02] [config]   []
[2019-07-07 17:08:02] [config] transformer-train-position-embeddings: false
[2019-07-07 17:08:02] [config] type: s2s
[2019-07-07 17:08:02] [config] ulr: false
[2019-07-07 17:08:02] [config] ulr-dim-emb: 0
[2019-07-07 17:08:02] [config] ulr-dropout: 0
[2019-07-07 17:08:02] [config] ulr-keys-vectors: ""
[2019-07-07 17:08:02] [config] ulr-query-vectors: ""
[2019-07-07 17:08:02] [config] ulr-softmax-temperature: 1
[2019-07-07 17:08:02] [config] ulr-trainable-transformation: false
[2019-07-07 17:08:02] [config] valid-freq: 10000
[2019-07-07 17:08:02] [config] valid-log: models/wmt2017-transformer-de-en/model/model.back/valid.log
[2019-07-07 17:08:02] [config] valid-max-length: 1000
[2019-07-07 17:08:02] [config] valid-metrics:
[2019-07-07 17:08:02] [config]   - ce-mean-words
[2019-07-07 17:08:02] [config]   - perplexity
[2019-07-07 17:08:02] [config]   - translation
[2019-07-07 17:08:02] [config] valid-mini-batch: 64
[2019-07-07 17:08:02] [config] valid-script-path: bash ./models/wmt2017-transformer-de-en/validate.de.sh
[2019-07-07 17:08:02] [config] valid-sets:
[2019-07-07 17:08:02] [config]   - models/wmt2017-transformer-de-en/data/valid.bpe.en
[2019-07-07 17:08:02] [config]   - models/wmt2017-transformer-de-en/data/valid.bpe.de
[2019-07-07 17:08:02] [config] valid-translation-output: models/wmt2017-transformer-de-en/data/valid.bpe.en.output
[2019-07-07 17:08:02] [config] vocabs:
[2019-07-07 17:08:02] [config]   - models/wmt2017-transformer-de-en/model/vocab.ende.yml
[2019-07-07 17:08:02] [config]   - models/wmt2017-transformer-de-en/model/vocab.ende.yml
[2019-07-07 17:08:02] [config] word-penalty: 0
[2019-07-07 17:08:02] [config] workspace: 6000
[2019-07-07 17:08:02] [config] Model is being created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-07 17:08:02] Using asynchronous training
[2019-07-07 17:08:02] [data] Loading vocabulary from JSON/Yaml file models/wmt2017-transformer-de-en/model/vocab.ende.yml
[2019-07-07 17:08:03] [data] Setting vocabulary size for input 0 to 36000
[2019-07-07 17:08:03] [data] Loading vocabulary from JSON/Yaml file models/wmt2017-transformer-de-en/model/vocab.ende.yml
[2019-07-07 17:08:03] [data] Setting vocabulary size for input 1 to 36000
[2019-07-07 17:08:03] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-07 17:08:03] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-07 17:08:04] [memory] Extending reserved space to 6016 MB (device gpu5)
[2019-07-07 17:08:06] [memory] Extending reserved space to 6016 MB (device gpu6)
[2019-07-07 17:08:06] [memory] Reserving 199 MB, device gpu5
[2019-07-07 17:08:06] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-07 17:08:06] [memory] Reserving 199 MB, device gpu5
[2019-07-07 17:08:20] [batching] Done. Typical MB size is 10153 target words
[2019-07-07 17:08:20] [memory] Extending reserved space to 6016 MB (device gpu5)
[2019-07-07 17:08:20] [memory] Extending reserved space to 6016 MB (device gpu6)
[2019-07-07 17:08:20] Training started
[2019-07-07 17:08:20] [data] Shuffling data
[2019-07-07 17:08:23] [data] Done reading 4561263 sentences
[2019-07-07 17:08:46] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 17:08:49] [memory] Reserving 199 MB, device gpu6
[2019-07-07 17:08:49] [memory] Reserving 199 MB, device gpu5
[2019-07-07 17:08:49] [memory] Reserving 99 MB, device gpu5
[2019-07-07 17:08:49] [memory] Reserving 99 MB, device gpu6
[2019-07-07 17:08:49] [memory] Reserving 99 MB, device gpu5
[2019-07-07 17:08:49] [memory] Reserving 99 MB, device gpu6
[2019-07-07 17:08:49] [memory] Reserving 99 MB, device gpu5
[2019-07-07 17:08:49] [memory] Reserving 99 MB, device gpu6
[2019-07-07 17:08:49] [memory] Reserving 199 MB, device gpu5
[2019-07-07 17:08:50] [memory] Reserving 199 MB, device gpu6
[2019-07-07 17:08:50] [memory] Reserving 199 MB, device gpu5
[2019-07-07 17:08:50] [memory] Reserving 199 MB, device gpu6
[2019-07-07 17:11:55] Ep. 1 : Up. 1000 : Sen. 220,873 : Cost 6.80438614 : Time 231.67s : 29182.53 words/s
[2019-07-07 17:15:02] Ep. 1 : Up. 2000 : Sen. 444,275 : Cost 5.74331522 : Time 186.87s : 36378.37 words/s
[2019-07-07 17:18:10] Ep. 1 : Up. 3000 : Sen. 667,922 : Cost 5.26913834 : Time 188.12s : 36140.31 words/s
[2019-07-07 17:21:26] Ep. 1 : Up. 4000 : Sen. 893,082 : Cost 4.96830368 : Time 196.24s : 34899.10 words/s
[2019-07-07 17:24:32] Ep. 1 : Up. 5000 : Sen. 1,116,486 : Cost 4.75925493 : Time 185.66s : 36457.61 words/s
[2019-07-07 17:27:41] Ep. 1 : Up. 6000 : Sen. 1,341,548 : Cost 4.59092712 : Time 189.00s : 36101.41 words/s
[2019-07-07 17:30:55] Ep. 1 : Up. 7000 : Sen. 1,562,593 : Cost 4.47805929 : Time 194.87s : 34861.69 words/s
[2019-07-07 17:34:02] Ep. 1 : Up. 8000 : Sen. 1,786,073 : Cost 4.37421656 : Time 187.01s : 36353.18 words/s
[2019-07-07 17:37:10] Ep. 1 : Up. 9000 : Sen. 2,010,552 : Cost 4.27122164 : Time 187.19s : 36450.39 words/s
[2019-07-07 17:40:17] Ep. 1 : Up. 10000 : Sen. 2,233,097 : Cost 4.19577837 : Time 187.51s : 36458.16 words/s
[2019-07-07 17:40:19] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 17:40:26] [valid] Ep. 1 : Up. 10000 : ce-mean-words : 4.59927 : new best
[2019-07-07 17:40:28] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 17:40:39] [valid] Ep. 1 : Up. 10000 : perplexity : 99.4113 : new best
[2019-07-07 17:41:50] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 17:41:58] [valid] Ep. 1 : Up. 10000 : translation : 1.74 : new best
[2019-07-07 17:41:59] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 17:42:08] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 17:45:34] Ep. 1 : Up. 11000 : Sen. 2,456,227 : Cost 4.11567163 : Time 316.88s : 21471.14 words/s
[2019-07-07 17:48:44] Ep. 1 : Up. 12000 : Sen. 2,679,054 : Cost 4.05837297 : Time 189.86s : 35598.76 words/s
[2019-07-07 17:51:57] Ep. 1 : Up. 13000 : Sen. 2,902,625 : Cost 3.86503172 : Time 192.57s : 35260.95 words/s
[2019-07-07 17:55:11] Ep. 1 : Up. 14000 : Sen. 3,126,129 : Cost 3.43964028 : Time 194.38s : 35116.08 words/s
[2019-07-07 17:58:22] Ep. 1 : Up. 15000 : Sen. 3,350,257 : Cost 3.20878005 : Time 191.33s : 35879.86 words/s
[2019-07-07 18:01:27] Ep. 1 : Up. 16000 : Sen. 3,573,516 : Cost 3.08372235 : Time 185.23s : 36419.88 words/s
[2019-07-07 18:04:36] Ep. 1 : Up. 17000 : Sen. 3,798,330 : Cost 2.97805572 : Time 188.26s : 36185.68 words/s
[2019-07-07 18:07:47] Ep. 1 : Up. 18000 : Sen. 4,021,771 : Cost 2.91614008 : Time 190.96s : 35791.55 words/s
[2019-07-07 18:10:56] Ep. 1 : Up. 19000 : Sen. 4,243,182 : Cost 2.85669422 : Time 189.40s : 35891.03 words/s
[2019-07-07 18:14:04] Ep. 1 : Up. 20000 : Sen. 4,468,906 : Cost 2.79438496 : Time 188.21s : 36138.26 words/s
[2019-07-07 18:14:06] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 18:14:16] [valid] Ep. 1 : Up. 20000 : ce-mean-words : 2.54703 : new best
[2019-07-07 18:14:19] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 18:14:31] [valid] Ep. 1 : Up. 20000 : perplexity : 12.7692 : new best
[2019-07-07 18:15:20] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 18:15:28] [valid] Ep. 1 : Up. 20000 : translation : 20.81 : new best
[2019-07-07 18:15:30] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 18:15:42] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 18:16:48] Seen 4531823 samples
[2019-07-07 18:16:48] Starting epoch 2
[2019-07-07 18:16:48] [data] Shuffling data
[2019-07-07 18:16:51] [data] Done reading 4561263 sentences
[2019-07-07 18:17:15] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 18:19:31] Ep. 2 : Up. 21000 : Sen. 156,633 : Cost 2.73994112 : Time 327.01s : 20563.08 words/s
[2019-07-07 18:22:40] Ep. 2 : Up. 22000 : Sen. 379,747 : Cost 2.69226623 : Time 188.45s : 36167.04 words/s
[2019-07-07 18:25:49] Ep. 2 : Up. 23000 : Sen. 603,849 : Cost 2.66009760 : Time 189.60s : 35969.25 words/s
[2019-07-07 18:28:58] Ep. 2 : Up. 24000 : Sen. 825,699 : Cost 2.64480877 : Time 188.52s : 35963.54 words/s
[2019-07-07 18:32:08] Ep. 2 : Up. 25000 : Sen. 1,048,914 : Cost 2.60747170 : Time 190.11s : 35853.64 words/s
[2019-07-07 18:35:16] Ep. 2 : Up. 26000 : Sen. 1,275,341 : Cost 2.57898450 : Time 187.62s : 36301.27 words/s
[2019-07-07 18:38:23] Ep. 2 : Up. 27000 : Sen. 1,497,069 : Cost 2.57183003 : Time 187.68s : 36177.90 words/s
[2019-07-07 18:41:32] Ep. 2 : Up. 28000 : Sen. 1,720,260 : Cost 2.55094886 : Time 188.76s : 36173.98 words/s
[2019-07-07 18:44:48] Ep. 2 : Up. 29000 : Sen. 1,942,866 : Cost 2.52975655 : Time 195.95s : 34539.72 words/s
[2019-07-07 18:47:56] Ep. 2 : Up. 30000 : Sen. 2,167,852 : Cost 2.51468039 : Time 188.36s : 36207.97 words/s
[2019-07-07 18:47:58] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 18:48:10] [valid] Ep. 2 : Up. 30000 : ce-mean-words : 2.1404 : new best
[2019-07-07 18:48:11] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 18:48:19] [valid] Ep. 2 : Up. 30000 : perplexity : 8.50287 : new best
[2019-07-07 18:49:04] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 18:49:15] [valid] Ep. 2 : Up. 30000 : translation : 24.94 : new best
[2019-07-07 18:49:15] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 18:49:24] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 18:52:49] Ep. 2 : Up. 31000 : Sen. 2,391,469 : Cost 2.49774647 : Time 292.80s : 23284.99 words/s
[2019-07-07 18:56:00] Ep. 2 : Up. 32000 : Sen. 2,618,138 : Cost 2.47433186 : Time 190.52s : 36169.19 words/s
[2019-07-07 18:59:09] Ep. 2 : Up. 33000 : Sen. 2,844,334 : Cost 2.46044755 : Time 188.99s : 36147.68 words/s
[2019-07-07 19:02:18] Ep. 2 : Up. 34000 : Sen. 3,064,749 : Cost 2.46230388 : Time 189.54s : 35817.63 words/s
[2019-07-07 19:05:26] Ep. 2 : Up. 35000 : Sen. 3,288,476 : Cost 2.43605208 : Time 188.11s : 35912.01 words/s
[2019-07-07 19:08:40] Ep. 2 : Up. 36000 : Sen. 3,510,789 : Cost 2.43335962 : Time 194.05s : 35205.52 words/s
[2019-07-07 19:11:52] Ep. 2 : Up. 37000 : Sen. 3,733,687 : Cost 2.41792679 : Time 191.20s : 35634.31 words/s
[2019-07-07 19:15:01] Ep. 2 : Up. 38000 : Sen. 3,958,375 : Cost 2.40786004 : Time 188.98s : 35977.76 words/s
[2019-07-07 19:18:10] Ep. 2 : Up. 39000 : Sen. 4,183,259 : Cost 2.39563823 : Time 189.14s : 36239.81 words/s
[2019-07-07 19:21:19] Ep. 2 : Up. 40000 : Sen. 4,405,583 : Cost 2.38316894 : Time 189.16s : 35809.17 words/s
[2019-07-07 19:21:20] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 19:21:31] [valid] Ep. 2 : Up. 40000 : ce-mean-words : 1.96228 : new best
[2019-07-07 19:21:32] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 19:21:41] [valid] Ep. 2 : Up. 40000 : perplexity : 7.11553 : new best
[2019-07-07 19:22:30] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 19:22:44] [valid] Ep. 2 : Up. 40000 : translation : 26.57 : new best
[2019-07-07 19:22:44] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 19:22:57] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 19:24:54] Seen 4532855 samples
[2019-07-07 19:24:54] Starting epoch 3
[2019-07-07 19:24:54] [data] Shuffling data
[2019-07-07 19:24:58] [data] Done reading 4561263 sentences
[2019-07-07 19:25:24] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 19:26:56] Ep. 3 : Up. 41000 : Sen. 95,768 : Cost 2.33882880 : Time 337.46s : 20110.21 words/s
[2019-07-07 19:30:05] Ep. 3 : Up. 42000 : Sen. 317,520 : Cost 2.31991911 : Time 188.26s : 35964.71 words/s
[2019-07-07 19:33:15] Ep. 3 : Up. 43000 : Sen. 541,758 : Cost 2.29944062 : Time 190.05s : 36007.68 words/s
[2019-07-07 19:36:23] Ep. 3 : Up. 44000 : Sen. 764,118 : Cost 2.30002785 : Time 188.00s : 36006.91 words/s
[2019-07-07 19:39:31] Ep. 3 : Up. 45000 : Sen. 989,889 : Cost 2.29004669 : Time 188.66s : 36141.61 words/s
[2019-07-07 19:42:41] Ep. 3 : Up. 46000 : Sen. 1,210,917 : Cost 2.29939675 : Time 189.93s : 35721.34 words/s
[2019-07-07 19:45:59] Ep. 3 : Up. 47000 : Sen. 1,435,093 : Cost 2.27363992 : Time 198.02s : 34401.14 words/s
[2019-07-07 19:49:08] Ep. 3 : Up. 48000 : Sen. 1,658,220 : Cost 2.27555490 : Time 188.68s : 36000.87 words/s
[2019-07-07 19:52:17] Ep. 3 : Up. 49000 : Sen. 1,883,071 : Cost 2.27170444 : Time 189.40s : 36087.33 words/s
[2019-07-07 19:55:27] Ep. 3 : Up. 50000 : Sen. 2,106,433 : Cost 2.28185987 : Time 189.33s : 35937.46 words/s
[2019-07-07 19:55:28] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 19:55:38] [valid] Ep. 3 : Up. 50000 : ce-mean-words : 1.86659 : new best
[2019-07-07 19:55:39] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 19:55:50] [valid] Ep. 3 : Up. 50000 : perplexity : 6.46622 : new best
[2019-07-07 19:56:40] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 19:56:53] [valid] Ep. 3 : Up. 50000 : translation : 27.61 : new best
[2019-07-07 19:56:53] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 19:57:04] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 20:00:26] Ep. 3 : Up. 51000 : Sen. 2,330,294 : Cost 2.26018143 : Time 299.32s : 22843.46 words/s
[2019-07-07 20:03:43] Ep. 3 : Up. 52000 : Sen. 2,552,984 : Cost 2.26001787 : Time 197.24s : 34389.72 words/s
[2019-07-07 20:06:51] Ep. 3 : Up. 53000 : Sen. 2,775,155 : Cost 2.26287746 : Time 188.16s : 35826.18 words/s
[2019-07-07 20:10:00] Ep. 3 : Up. 54000 : Sen. 2,999,497 : Cost 2.25700760 : Time 189.14s : 36066.15 words/s
[2019-07-07 20:13:10] Ep. 3 : Up. 55000 : Sen. 3,224,100 : Cost 2.24049735 : Time 189.71s : 36194.14 words/s
[2019-07-07 20:16:20] Ep. 3 : Up. 56000 : Sen. 3,446,449 : Cost 2.24293351 : Time 189.95s : 35813.01 words/s
[2019-07-07 20:19:38] Ep. 3 : Up. 57000 : Sen. 3,670,152 : Cost 2.24450588 : Time 197.58s : 34402.53 words/s
[2019-07-07 20:22:46] Ep. 3 : Up. 58000 : Sen. 3,891,284 : Cost 2.24251890 : Time 188.64s : 35911.62 words/s
[2019-07-07 20:25:56] Ep. 3 : Up. 59000 : Sen. 4,117,450 : Cost 2.22565055 : Time 189.88s : 36095.49 words/s
[2019-07-07 20:29:05] Ep. 3 : Up. 60000 : Sen. 4,341,045 : Cost 2.22888732 : Time 188.96s : 35930.54 words/s
[2019-07-07 20:29:06] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 20:29:12] [valid] Ep. 3 : Up. 60000 : ce-mean-words : 1.80009 : new best
[2019-07-07 20:29:15] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 20:29:26] [valid] Ep. 3 : Up. 60000 : perplexity : 6.05022 : new best
[2019-07-07 20:30:17] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 20:30:25] [valid] Ep. 3 : Up. 60000 : translation : 28.3 : new best
[2019-07-07 20:30:27] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 20:30:39] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 20:33:35] Seen 4532874 samples
[2019-07-07 20:33:35] Starting epoch 4
[2019-07-07 20:33:35] [data] Shuffling data
[2019-07-07 20:33:38] [data] Done reading 4561263 sentences
[2019-07-07 20:34:00] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 20:34:28] Ep. 4 : Up. 61000 : Sen. 31,312 : Cost 2.20892191 : Time 322.98s : 21075.40 words/s
[2019-07-07 20:37:44] Ep. 4 : Up. 62000 : Sen. 252,814 : Cost 2.15280366 : Time 195.73s : 34589.25 words/s
[2019-07-07 20:40:54] Ep. 4 : Up. 63000 : Sen. 474,125 : Cost 2.15838265 : Time 190.33s : 35757.16 words/s
[2019-07-07 20:44:03] Ep. 4 : Up. 64000 : Sen. 699,284 : Cost 2.15035748 : Time 189.15s : 36005.77 words/s
[2019-07-07 20:47:13] Ep. 4 : Up. 65000 : Sen. 924,531 : Cost 2.14318943 : Time 189.62s : 36088.53 words/s
[2019-07-07 20:50:22] Ep. 4 : Up. 66000 : Sen. 1,149,238 : Cost 2.15380645 : Time 189.16s : 36208.36 words/s
[2019-07-07 20:53:32] Ep. 4 : Up. 67000 : Sen. 1,374,535 : Cost 2.14081097 : Time 189.75s : 35997.78 words/s
[2019-07-07 20:56:49] Ep. 4 : Up. 68000 : Sen. 1,596,079 : Cost 2.16292620 : Time 196.92s : 34427.93 words/s
[2019-07-07 20:59:58] Ep. 4 : Up. 69000 : Sen. 1,818,553 : Cost 2.15014863 : Time 189.09s : 35930.78 words/s
[2019-07-07 21:03:06] Ep. 4 : Up. 70000 : Sen. 2,042,193 : Cost 2.13829184 : Time 188.35s : 35965.78 words/s
[2019-07-07 21:03:07] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 21:03:17] [valid] Ep. 4 : Up. 70000 : ce-mean-words : 1.75879 : new best
[2019-07-07 21:03:21] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 21:03:34] [valid] Ep. 4 : Up. 70000 : perplexity : 5.80543 : new best
[2019-07-07 21:04:24] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 21:04:31] [valid] Ep. 4 : Up. 70000 : translation : 28.56 : new best
[2019-07-07 21:04:32] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 21:04:49] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 21:08:13] Ep. 4 : Up. 71000 : Sen. 2,264,884 : Cost 2.15026951 : Time 306.59s : 22301.52 words/s
[2019-07-07 21:11:23] Ep. 4 : Up. 72000 : Sen. 2,490,458 : Cost 2.14667034 : Time 190.38s : 35700.35 words/s
[2019-07-07 21:14:38] Ep. 4 : Up. 73000 : Sen. 2,712,285 : Cost 2.14770532 : Time 194.38s : 35015.71 words/s
[2019-07-07 21:17:47] Ep. 4 : Up. 74000 : Sen. 2,936,312 : Cost 2.14983678 : Time 189.23s : 35949.57 words/s
[2019-07-07 21:20:57] Ep. 4 : Up. 75000 : Sen. 3,160,457 : Cost 2.13873744 : Time 189.87s : 36209.31 words/s
[2019-07-07 21:24:05] Ep. 4 : Up. 76000 : Sen. 3,382,967 : Cost 2.14624691 : Time 188.63s : 35824.26 words/s
[2019-07-07 21:27:14] Ep. 4 : Up. 77000 : Sen. 3,606,841 : Cost 2.14104009 : Time 188.79s : 36066.17 words/s
[2019-07-07 21:30:32] Ep. 4 : Up. 78000 : Sen. 3,830,344 : Cost 2.13422346 : Time 197.57s : 34570.87 words/s
[2019-07-07 21:33:41] Ep. 4 : Up. 79000 : Sen. 4,056,765 : Cost 2.12940216 : Time 189.55s : 36108.39 words/s
[2019-07-07 21:36:50] Ep. 4 : Up. 80000 : Sen. 4,280,348 : Cost 2.13383389 : Time 188.64s : 36077.59 words/s
[2019-07-07 21:36:51] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 21:37:06] [valid] Ep. 4 : Up. 80000 : ce-mean-words : 1.72369 : new best
[2019-07-07 21:37:08] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 21:37:21] [valid] Ep. 4 : Up. 80000 : perplexity : 5.60515 : new best
[2019-07-07 21:38:07] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 21:38:18] [valid] Ep. 4 : Up. 80000 : translation : 28.95 : new best
[2019-07-07 21:38:20] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 21:38:36] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 21:41:55] Ep. 4 : Up. 81000 : Sen. 4,502,467 : Cost 2.12877274 : Time 305.09s : 22309.17 words/s
[2019-07-07 21:42:20] Seen 4532861 samples
[2019-07-07 21:42:20] Starting epoch 5
[2019-07-07 21:42:20] [data] Shuffling data
[2019-07-07 21:42:23] [data] Done reading 4561263 sentences
[2019-07-07 21:42:46] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 21:45:34] Ep. 5 : Up. 82000 : Sen. 192,568 : Cost 2.06637812 : Time 218.59s : 30937.70 words/s
[2019-07-07 21:48:49] Ep. 5 : Up. 83000 : Sen. 416,305 : Cost 2.06259608 : Time 195.67s : 35035.53 words/s
[2019-07-07 21:51:57] Ep. 5 : Up. 84000 : Sen. 638,976 : Cost 2.05638313 : Time 187.87s : 35848.86 words/s
[2019-07-07 21:55:05] Ep. 5 : Up. 85000 : Sen. 861,906 : Cost 2.06433845 : Time 188.01s : 35932.50 words/s
[2019-07-07 21:58:15] Ep. 5 : Up. 86000 : Sen. 1,086,094 : Cost 2.06588674 : Time 189.71s : 36145.03 words/s
[2019-07-07 22:01:25] Ep. 5 : Up. 87000 : Sen. 1,308,938 : Cost 2.06380343 : Time 189.99s : 36038.57 words/s
[2019-07-07 22:04:42] Ep. 5 : Up. 88000 : Sen. 1,533,364 : Cost 2.07536364 : Time 197.21s : 34268.42 words/s
[2019-07-07 22:07:52] Ep. 5 : Up. 89000 : Sen. 1,754,468 : Cost 2.07480383 : Time 189.73s : 35839.43 words/s
[2019-07-07 22:11:01] Ep. 5 : Up. 90000 : Sen. 1,979,343 : Cost 2.05976057 : Time 189.11s : 36136.00 words/s
[2019-07-07 22:11:02] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 22:11:17] [valid] Ep. 5 : Up. 90000 : ce-mean-words : 1.70347 : new best
[2019-07-07 22:11:21] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 22:11:35] [valid] Ep. 5 : Up. 90000 : perplexity : 5.49296 : new best
[2019-07-07 22:12:22] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 22:12:29] [valid] Ep. 5 : Up. 90000 : translation : 28.98 : new best
[2019-07-07 22:12:29] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 22:12:43] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 22:16:02] Ep. 5 : Up. 91000 : Sen. 2,202,896 : Cost 2.06604934 : Time 301.19s : 22650.50 words/s
[2019-07-07 22:19:11] Ep. 5 : Up. 92000 : Sen. 2,427,168 : Cost 2.06389451 : Time 188.71s : 36028.48 words/s
[2019-07-07 22:22:28] Ep. 5 : Up. 93000 : Sen. 2,651,776 : Cost 2.06923318 : Time 196.71s : 34620.80 words/s
[2019-07-07 22:25:37] Ep. 5 : Up. 94000 : Sen. 2,876,037 : Cost 2.06850314 : Time 189.94s : 35985.02 words/s
[2019-07-07 22:28:48] Ep. 5 : Up. 95000 : Sen. 3,100,193 : Cost 2.07229066 : Time 190.40s : 35688.73 words/s
[2019-07-07 22:31:57] Ep. 5 : Up. 96000 : Sen. 3,320,906 : Cost 2.07273364 : Time 189.58s : 35900.55 words/s
[2019-07-07 22:35:07] Ep. 5 : Up. 97000 : Sen. 3,546,156 : Cost 2.06030798 : Time 189.06s : 36209.22 words/s
[2019-07-07 22:38:24] Ep. 5 : Up. 98000 : Sen. 3,770,327 : Cost 2.07486296 : Time 197.66s : 34513.01 words/s
[2019-07-07 22:41:32] Ep. 5 : Up. 99000 : Sen. 3,994,779 : Cost 2.06257606 : Time 188.10s : 36218.77 words/s
[2019-07-07 22:44:42] Ep. 5 : Up. 100000 : Sen. 4,219,672 : Cost 2.07080889 : Time 189.28s : 36036.47 words/s
[2019-07-07 22:44:43] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 22:44:51] [valid] Ep. 5 : Up. 100000 : ce-mean-words : 1.68176 : new best
[2019-07-07 22:44:52] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 22:45:05] [valid] Ep. 5 : Up. 100000 : perplexity : 5.37502 : new best
[2019-07-07 22:45:52] [valid] Ep. 5 : Up. 100000 : translation : 28.96 : stalled 1 times (last best: 28.98)
[2019-07-07 22:45:52] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 22:46:07] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 22:49:28] Ep. 5 : Up. 101000 : Sen. 4,440,455 : Cost 2.08016133 : Time 286.05s : 23738.94 words/s
[2019-07-07 22:50:46] Seen 4533002 samples
[2019-07-07 22:50:46] Starting epoch 6
[2019-07-07 22:50:46] [data] Shuffling data
[2019-07-07 22:50:49] [data] Done reading 4561263 sentences
[2019-07-07 22:51:13] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 22:53:06] Ep. 6 : Up. 102000 : Sen. 131,347 : Cost 2.01436853 : Time 218.31s : 31111.88 words/s
[2019-07-07 22:56:24] Ep. 6 : Up. 103000 : Sen. 353,377 : Cost 2.00087643 : Time 198.00s : 34332.14 words/s
[2019-07-07 22:59:32] Ep. 6 : Up. 104000 : Sen. 577,626 : Cost 1.98652339 : Time 188.35s : 36101.91 words/s
[2019-07-07 23:02:40] Ep. 6 : Up. 105000 : Sen. 800,462 : Cost 1.99169755 : Time 187.95s : 35992.13 words/s
[2019-07-07 23:05:50] Ep. 6 : Up. 106000 : Sen. 1,022,933 : Cost 2.01007390 : Time 189.81s : 35837.76 words/s
[2019-07-07 23:08:59] Ep. 6 : Up. 107000 : Sen. 1,245,981 : Cost 2.01328778 : Time 189.40s : 35951.20 words/s
[2019-07-07 23:12:14] Ep. 6 : Up. 108000 : Sen. 1,469,831 : Cost 2.00035596 : Time 194.95s : 34722.20 words/s
[2019-07-07 23:15:23] Ep. 6 : Up. 109000 : Sen. 1,691,847 : Cost 2.00820923 : Time 188.64s : 35990.99 words/s
[2019-07-07 23:18:32] Ep. 6 : Up. 110000 : Sen. 1,916,193 : Cost 2.00799298 : Time 189.24s : 35954.51 words/s
[2019-07-07 23:18:33] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 23:18:48] [valid] Ep. 6 : Up. 110000 : ce-mean-words : 1.66953 : new best
[2019-07-07 23:18:50] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 23:19:00] [valid] Ep. 6 : Up. 110000 : perplexity : 5.30967 : new best
[2019-07-07 23:19:50] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 23:19:58] [valid] Ep. 6 : Up. 110000 : translation : 29.06 : new best
[2019-07-07 23:19:58] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 23:20:14] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 23:23:33] Ep. 6 : Up. 111000 : Sen. 2,140,034 : Cost 2.01998568 : Time 300.51s : 22753.22 words/s
[2019-07-07 23:26:43] Ep. 6 : Up. 112000 : Sen. 2,363,483 : Cost 2.00921440 : Time 190.10s : 36005.00 words/s
[2019-07-07 23:29:59] Ep. 6 : Up. 113000 : Sen. 2,588,284 : Cost 2.02013803 : Time 196.12s : 34584.68 words/s
[2019-07-07 23:33:09] Ep. 6 : Up. 114000 : Sen. 2,812,760 : Cost 2.00885773 : Time 189.80s : 36222.73 words/s
[2019-07-07 23:36:17] Ep. 6 : Up. 115000 : Sen. 3,037,613 : Cost 2.00858092 : Time 188.64s : 36090.53 words/s
[2019-07-07 23:39:27] Ep. 6 : Up. 116000 : Sen. 3,260,201 : Cost 2.01967502 : Time 189.51s : 35889.11 words/s
[2019-07-07 23:42:37] Ep. 6 : Up. 117000 : Sen. 3,485,057 : Cost 2.02424955 : Time 190.46s : 36048.40 words/s
[2019-07-07 23:45:54] Ep. 6 : Up. 118000 : Sen. 3,708,763 : Cost 2.00830102 : Time 196.47s : 34635.08 words/s
[2019-07-07 23:49:03] Ep. 6 : Up. 119000 : Sen. 3,929,429 : Cost 2.03146100 : Time 189.20s : 35868.01 words/s
[2019-07-07 23:52:12] Ep. 6 : Up. 120000 : Sen. 4,156,187 : Cost 2.00692463 : Time 189.18s : 36155.60 words/s
[2019-07-07 23:52:13] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 23:52:24] [valid] Ep. 6 : Up. 120000 : ce-mean-words : 1.65472 : new best
[2019-07-07 23:52:29] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 23:52:38] [valid] Ep. 6 : Up. 120000 : perplexity : 5.23161 : new best
[2019-07-07 23:53:29] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-07 23:53:43] [valid] Ep. 6 : Up. 120000 : translation : 29.16 : new best
[2019-07-07 23:53:43] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-07 23:54:26] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-07 23:57:51] Ep. 6 : Up. 121000 : Sen. 4,377,495 : Cost 2.01789713 : Time 339.14s : 20003.67 words/s
[2019-07-08 00:00:03] Seen 4532577 samples
[2019-07-08 00:00:03] Starting epoch 7
[2019-07-08 00:00:03] [data] Shuffling data
[2019-07-08 00:00:07] [data] Done reading 4561263 sentences
[2019-07-08 00:00:32] [data] Done shuffling 4561263 sentences to temp files
[2019-07-08 00:01:32] Ep. 7 : Up. 122000 : Sen. 69,080 : Cost 1.99329031 : Time 220.28s : 30835.78 words/s
[2019-07-08 00:04:38] Ep. 7 : Up. 123000 : Sen. 290,652 : Cost 1.94460559 : Time 186.78s : 36355.65 words/s
[2019-07-08 00:07:52] Ep. 7 : Up. 124000 : Sen. 514,799 : Cost 1.94200683 : Time 194.01s : 35000.72 words/s
[2019-07-08 00:11:07] Ep. 7 : Up. 125000 : Sen. 736,955 : Cost 1.96017039 : Time 194.37s : 34941.44 words/s
[2019-07-08 00:14:20] Ep. 7 : Up. 126000 : Sen. 960,882 : Cost 1.94761074 : Time 193.16s : 35205.99 words/s
[2019-07-08 00:17:34] Ep. 7 : Up. 127000 : Sen. 1,184,917 : Cost 1.95422661 : Time 193.68s : 35026.18 words/s
[2019-07-08 00:20:42] Ep. 7 : Up. 128000 : Sen. 1,408,996 : Cost 1.96061873 : Time 188.76s : 36343.15 words/s
[2019-07-08 00:23:51] Ep. 7 : Up. 129000 : Sen. 1,634,561 : Cost 1.95778799 : Time 188.32s : 36469.20 words/s
[2019-07-08 00:27:02] Ep. 7 : Up. 130000 : Sen. 1,856,784 : Cost 1.96823764 : Time 190.85s : 35511.46 words/s
[2019-07-08 00:27:03] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-08 00:27:14] [valid] Ep. 7 : Up. 130000 : ce-mean-words : 1.65034 : new best
[2019-07-08 00:27:15] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-08 00:27:30] [valid] Ep. 7 : Up. 130000 : perplexity : 5.20874 : new best
[2019-07-08 00:28:15] [valid] Ep. 7 : Up. 130000 : translation : 29.06 : stalled 1 times (last best: 29.16)
[2019-07-08 00:28:15] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-08 00:28:39] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-08 00:32:01] Ep. 7 : Up. 131000 : Sen. 2,080,154 : Cost 1.96965480 : Time 299.81s : 22672.82 words/s
[2019-07-08 00:35:08] Ep. 7 : Up. 132000 : Sen. 2,302,498 : Cost 1.96645987 : Time 186.21s : 36412.66 words/s
[2019-07-08 00:38:14] Ep. 7 : Up. 133000 : Sen. 2,526,227 : Cost 1.97679639 : Time 186.68s : 36325.04 words/s
[2019-07-08 00:41:28] Ep. 7 : Up. 134000 : Sen. 2,750,041 : Cost 1.96419775 : Time 194.12s : 35198.34 words/s
[2019-07-08 00:44:35] Ep. 7 : Up. 135000 : Sen. 2,971,990 : Cost 1.97364962 : Time 186.53s : 36415.40 words/s
[2019-07-08 00:47:42] Ep. 7 : Up. 136000 : Sen. 3,199,390 : Cost 1.96563435 : Time 187.47s : 36762.08 words/s
[2019-07-08 00:50:49] Ep. 7 : Up. 137000 : Sen. 3,422,555 : Cost 1.97324908 : Time 186.16s : 36659.97 words/s
[2019-07-08 00:53:55] Ep. 7 : Up. 138000 : Sen. 3,646,599 : Cost 1.97657323 : Time 186.74s : 36432.77 words/s
[2019-07-08 00:57:02] Ep. 7 : Up. 139000 : Sen. 3,869,684 : Cost 1.98496330 : Time 186.74s : 36422.19 words/s
[2019-07-08 01:00:08] Ep. 7 : Up. 140000 : Sen. 4,094,339 : Cost 1.96851254 : Time 186.11s : 36709.69 words/s
[2019-07-08 01:00:09] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-08 01:00:18] [valid] Ep. 7 : Up. 140000 : ce-mean-words : 1.64151 : new best
[2019-07-08 01:00:20] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-08 01:00:27] [valid] Ep. 7 : Up. 140000 : perplexity : 5.16295 : new best
[2019-07-08 01:01:12] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-08 01:01:21] [valid] Ep. 7 : Up. 140000 : translation : 29.23 : new best
[2019-07-08 01:01:21] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-08 01:01:26] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-08 01:04:47] Ep. 7 : Up. 141000 : Sen. 4,319,686 : Cost 1.97591746 : Time 278.34s : 24582.12 words/s
[2019-07-08 01:07:46] Seen 4532866 samples
[2019-07-08 01:07:46] Starting epoch 8
[2019-07-08 01:07:46] [data] Shuffling data
[2019-07-08 01:07:49] [data] Done reading 4561263 sentences
[2019-07-08 01:08:09] [data] Done shuffling 4561263 sentences to temp files
[2019-07-08 01:08:18] Ep. 8 : Up. 142000 : Sen. 10,285 : Cost 1.97380590 : Time 211.70s : 31979.49 words/s
[2019-07-08 01:11:25] Ep. 8 : Up. 143000 : Sen. 231,623 : Cost 1.90877104 : Time 187.01s : 36499.19 words/s
[2019-07-08 01:14:32] Ep. 8 : Up. 144000 : Sen. 454,869 : Cost 1.90249455 : Time 186.49s : 36364.48 words/s
[2019-07-08 01:17:39] Ep. 8 : Up. 145000 : Sen. 677,719 : Cost 1.91129398 : Time 187.04s : 36535.95 words/s
[2019-07-08 01:20:45] Ep. 8 : Up. 146000 : Sen. 901,946 : Cost 1.91366935 : Time 186.33s : 36549.79 words/s
[2019-07-08 01:23:51] Ep. 8 : Up. 147000 : Sen. 1,126,179 : Cost 1.91376770 : Time 185.37s : 36575.90 words/s
[2019-07-08 01:26:57] Ep. 8 : Up. 148000 : Sen. 1,349,834 : Cost 1.92457819 : Time 186.34s : 36582.45 words/s
[2019-07-08 01:30:05] Ep. 8 : Up. 149000 : Sen. 1,571,371 : Cost 1.92938662 : Time 188.44s : 36410.63 words/s
[2019-07-08 01:33:11] Ep. 8 : Up. 150000 : Sen. 1,797,462 : Cost 1.92324281 : Time 185.62s : 36652.53 words/s
[2019-07-08 01:33:12] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-08 01:33:25] [valid] Ep. 8 : Up. 150000 : ce-mean-words : 1.64065 : new best
[2019-07-08 01:33:27] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-08 01:33:52] [valid] Ep. 8 : Up. 150000 : perplexity : 5.1585 : new best
[2019-07-08 01:34:49] [valid] Ep. 8 : Up. 150000 : translation : 29.04 : stalled 1 times (last best: 29.23)
[2019-07-08 01:34:49] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-08 01:35:31] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-08 01:38:57] Ep. 8 : Up. 151000 : Sen. 2,020,429 : Cost 1.92289901 : Time 346.40s : 19676.34 words/s
[2019-07-08 01:42:05] Ep. 8 : Up. 152000 : Sen. 2,242,656 : Cost 1.93804777 : Time 187.39s : 36056.95 words/s
[2019-07-08 01:45:13] Ep. 8 : Up. 153000 : Sen. 2,468,531 : Cost 1.92952907 : Time 188.71s : 36227.17 words/s
[2019-07-08 01:48:22] Ep. 8 : Up. 154000 : Sen. 2,690,326 : Cost 1.93250525 : Time 188.81s : 35953.22 words/s
[2019-07-08 01:51:31] Ep. 8 : Up. 155000 : Sen. 2,912,506 : Cost 1.93548656 : Time 188.30s : 35991.08 words/s
[2019-07-08 01:54:40] Ep. 8 : Up. 156000 : Sen. 3,136,358 : Cost 1.94142938 : Time 189.04s : 35917.49 words/s
[2019-07-08 01:57:49] Ep. 8 : Up. 157000 : Sen. 3,358,880 : Cost 1.93998456 : Time 188.93s : 36164.51 words/s
[2019-07-08 02:01:04] Ep. 8 : Up. 158000 : Sen. 3,581,947 : Cost 1.94147611 : Time 195.40s : 34672.11 words/s
[2019-07-08 02:04:14] Ep. 8 : Up. 159000 : Sen. 3,807,132 : Cost 1.93727064 : Time 189.75s : 36014.06 words/s
[2019-07-08 02:07:32] Ep. 8 : Up. 160000 : Sen. 4,029,585 : Cost 1.94137239 : Time 198.74s : 34131.74 words/s
[2019-07-08 02:07:34] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-08 02:07:42] [valid] Ep. 8 : Up. 160000 : ce-mean-words : 1.63555 : new best
[2019-07-08 02:07:44] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-perplexity.npz
[2019-07-08 02:07:56] [valid] Ep. 8 : Up. 160000 : perplexity : 5.13227 : new best
[2019-07-08 02:08:53] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.best-translation.npz
[2019-07-08 02:09:10] [valid] Ep. 8 : Up. 160000 : translation : 29.28 : new best
[2019-07-08 02:09:10] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-08 02:09:43] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-08 02:13:07] Ep. 8 : Up. 161000 : Sen. 4,253,150 : Cost 1.94215786 : Time 334.98s : 20135.38 words/s
[2019-07-08 02:16:17] Ep. 8 : Up. 162000 : Sen. 4,477,485 : Cost 1.94729364 : Time 190.03s : 36102.36 words/s
[2019-07-08 02:17:06] Seen 4532651 samples
[2019-07-08 02:17:06] Starting epoch 9
[2019-07-08 02:17:06] [data] Shuffling data
[2019-07-08 02:17:09] [data] Done reading 4561263 sentences
[2019-07-08 02:17:30] [data] Done shuffling 4561263 sentences to temp files
[2019-07-08 02:19:52] Ep. 9 : Up. 163000 : Sen. 165,258 : Cost 1.88890588 : Time 214.89s : 31253.38 words/s
[2019-07-08 02:23:02] Ep. 9 : Up. 164000 : Sen. 390,740 : Cost 1.86043119 : Time 189.86s : 36198.11 words/s
[2019-07-08 02:26:15] Ep. 9 : Up. 165000 : Sen. 612,304 : Cost 1.87631142 : Time 193.04s : 35146.61 words/s
[2019-07-08 02:29:30] Ep. 9 : Up. 166000 : Sen. 837,802 : Cost 1.87170351 : Time 194.31s : 35270.86 words/s
[2019-07-08 02:32:37] Ep. 9 : Up. 167000 : Sen. 1,059,169 : Cost 1.89075398 : Time 187.85s : 35936.47 words/s
[2019-07-08 02:35:47] Ep. 9 : Up. 168000 : Sen. 1,284,187 : Cost 1.88721633 : Time 189.29s : 36161.88 words/s
[2019-07-08 02:38:56] Ep. 9 : Up. 169000 : Sen. 1,507,756 : Cost 1.89558685 : Time 189.57s : 36029.28 words/s
[2019-07-08 02:42:05] Ep. 9 : Up. 170000 : Sen. 1,730,931 : Cost 1.88427985 : Time 188.83s : 35935.09 words/s
[2019-07-08 02:42:06] [valid] Ep. 9 : Up. 170000 : ce-mean-words : 1.63828 : stalled 1 times (last best: 1.63555)
[2019-07-08 02:42:08] [valid] Ep. 9 : Up. 170000 : perplexity : 5.14631 : stalled 1 times (last best: 5.13227)
[2019-07-08 02:42:54] [valid] Ep. 9 : Up. 170000 : translation : 29.17 : stalled 1 times (last best: 29.28)
[2019-07-08 02:42:54] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-08 02:43:22] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-08 02:46:53] Ep. 9 : Up. 171000 : Sen. 1,954,347 : Cost 1.90381384 : Time 288.42s : 23504.38 words/s
[2019-07-08 02:50:02] Ep. 9 : Up. 172000 : Sen. 2,177,996 : Cost 1.89182544 : Time 188.57s : 36121.29 words/s
[2019-07-08 02:53:11] Ep. 9 : Up. 173000 : Sen. 2,401,429 : Cost 1.90301597 : Time 188.48s : 36111.20 words/s
[2019-07-08 02:56:20] Ep. 9 : Up. 174000 : Sen. 2,625,801 : Cost 1.90562856 : Time 189.64s : 36080.86 words/s
[2019-07-08 02:59:29] Ep. 9 : Up. 175000 : Sen. 2,847,920 : Cost 1.90647912 : Time 188.63s : 35888.66 words/s
[2019-07-08 03:02:40] Ep. 9 : Up. 176000 : Sen. 3,072,589 : Cost 1.90382481 : Time 191.32s : 35765.46 words/s
[2019-07-08 03:05:51] Ep. 9 : Up. 177000 : Sen. 3,294,432 : Cost 1.91613531 : Time 191.24s : 35395.53 words/s
[2019-07-08 03:09:01] Ep. 9 : Up. 178000 : Sen. 3,518,698 : Cost 1.91139555 : Time 189.29s : 36057.08 words/s
[2019-07-08 03:12:09] Ep. 9 : Up. 179000 : Sen. 3,739,731 : Cost 1.90946543 : Time 188.81s : 35955.30 words/s
[2019-07-08 03:15:17] Ep. 9 : Up. 180000 : Sen. 3,963,921 : Cost 1.91659951 : Time 187.96s : 36089.02 words/s
[2019-07-08 03:15:19] [valid] Ep. 9 : Up. 180000 : ce-mean-words : 1.63596 : stalled 2 times (last best: 1.63555)
[2019-07-08 03:15:22] [valid] Ep. 9 : Up. 180000 : perplexity : 5.13436 : stalled 2 times (last best: 5.13227)
[2019-07-08 03:16:09] [valid] Ep. 9 : Up. 180000 : translation : 28.94 : stalled 2 times (last best: 29.28)
[2019-07-08 03:16:09] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-08 03:16:35] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-08 03:19:56] Ep. 9 : Up. 181000 : Sen. 4,188,534 : Cost 1.91610968 : Time 278.49s : 24525.29 words/s
[2019-07-08 03:23:13] Ep. 9 : Up. 182000 : Sen. 4,412,776 : Cost 1.91584384 : Time 196.88s : 34776.68 words/s
[2019-07-08 03:24:55] Seen 4532450 samples
[2019-07-08 03:24:55] Starting epoch 10
[2019-07-08 03:24:55] [data] Shuffling data
[2019-07-08 03:24:58] [data] Done reading 4561263 sentences
[2019-07-08 03:25:19] [data] Done shuffling 4561263 sentences to temp files
[2019-07-08 03:26:48] Ep. 10 : Up. 183000 : Sen. 101,407 : Cost 1.87885237 : Time 214.70s : 31352.35 words/s
[2019-07-08 03:29:56] Ep. 10 : Up. 184000 : Sen. 324,984 : Cost 1.83327210 : Time 188.51s : 35878.70 words/s
[2019-07-08 03:33:06] Ep. 10 : Up. 185000 : Sen. 548,909 : Cost 1.83570719 : Time 190.38s : 35930.54 words/s
[2019-07-08 03:36:15] Ep. 10 : Up. 186000 : Sen. 772,024 : Cost 1.85859132 : Time 188.65s : 35904.84 words/s
[2019-07-08 03:39:30] Ep. 10 : Up. 187000 : Sen. 995,207 : Cost 1.85800302 : Time 195.31s : 34917.57 words/s
[2019-07-08 03:42:40] Ep. 10 : Up. 188000 : Sen. 1,219,222 : Cost 1.86006272 : Time 189.76s : 35946.32 words/s
[2019-07-08 03:45:49] Ep. 10 : Up. 189000 : Sen. 1,443,345 : Cost 1.85952604 : Time 188.91s : 36059.15 words/s
[2019-07-08 03:48:59] Ep. 10 : Up. 190000 : Sen. 1,669,247 : Cost 1.86705446 : Time 190.13s : 36141.49 words/s
[2019-07-08 03:49:00] [valid] Ep. 10 : Up. 190000 : ce-mean-words : 1.64095 : stalled 3 times (last best: 1.63555)
[2019-07-08 03:49:07] [valid] Ep. 10 : Up. 190000 : perplexity : 5.16005 : stalled 3 times (last best: 5.13227)
[2019-07-08 03:49:57] [valid] Ep. 10 : Up. 190000 : translation : 28.97 : stalled 3 times (last best: 29.28)
[2019-07-08 03:49:57] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-08 03:50:39] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-08 03:54:10] Ep. 10 : Up. 191000 : Sen. 1,892,837 : Cost 1.87070775 : Time 311.19s : 21745.20 words/s
[2019-07-08 03:57:23] Ep. 10 : Up. 192000 : Sen. 2,115,796 : Cost 1.87535584 : Time 192.18s : 35638.91 words/s
[2019-07-08 04:00:35] Ep. 10 : Up. 193000 : Sen. 2,339,342 : Cost 1.87994075 : Time 192.43s : 35189.73 words/s
[2019-07-08 04:03:45] Ep. 10 : Up. 194000 : Sen. 2,563,218 : Cost 1.87190795 : Time 189.87s : 36139.19 words/s
[2019-07-08 04:06:54] Ep. 10 : Up. 195000 : Sen. 2,786,263 : Cost 1.87870729 : Time 189.19s : 35830.26 words/s
[2019-07-08 04:10:03] Ep. 10 : Up. 196000 : Sen. 3,008,624 : Cost 1.88355446 : Time 188.64s : 35932.70 words/s
[2019-07-08 04:13:13] Ep. 10 : Up. 197000 : Sen. 3,231,171 : Cost 1.88356090 : Time 189.89s : 35871.40 words/s
[2019-07-08 04:16:28] Ep. 10 : Up. 198000 : Sen. 3,454,435 : Cost 1.88918316 : Time 195.15s : 34770.25 words/s
[2019-07-08 04:19:37] Ep. 10 : Up. 199000 : Sen. 3,677,533 : Cost 1.87748194 : Time 189.48s : 35858.78 words/s
[2019-07-08 04:22:46] Ep. 10 : Up. 200000 : Sen. 3,900,508 : Cost 1.89206529 : Time 188.92s : 36007.20 words/s
[2019-07-08 04:22:47] [valid] Ep. 10 : Up. 200000 : ce-mean-words : 1.63852 : stalled 4 times (last best: 1.63555)
[2019-07-08 04:22:53] [valid] Ep. 10 : Up. 200000 : perplexity : 5.14754 : stalled 4 times (last best: 5.13227)
[2019-07-08 04:23:43] [valid] Ep. 10 : Up. 200000 : translation : 29.12 : stalled 4 times (last best: 29.28)
[2019-07-08 04:23:43] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-08 04:24:43] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
[2019-07-08 04:28:15] Ep. 10 : Up. 201000 : Sen. 4,123,127 : Cost 1.88871515 : Time 329.36s : 20535.93 words/s
[2019-07-08 04:31:25] Ep. 10 : Up. 202000 : Sen. 4,345,854 : Cost 1.89102030 : Time 189.82s : 35945.38 words/s
[2019-07-08 04:34:10] Seen 4533491 samples
[2019-07-08 04:34:10] Starting epoch 11
[2019-07-08 04:34:10] Training finished
[2019-07-08 04:34:11] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.orig.npz
[2019-07-08 04:34:24] [valid] Ep. 11 : Up. 202841 : ce-mean-words : 1.63594 : stalled 5 times (last best: 1.63555)
[2019-07-08 04:34:26] [valid] Ep. 11 : Up. 202841 : perplexity : 5.13427 : stalled 5 times (last best: 5.13227)
[2019-07-08 04:35:12] [valid] Ep. 11 : Up. 202841 : translation : 29.17 : stalled 5 times (last best: 29.28)
[2019-07-08 04:35:12] Saving model weights and runtime parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz
[2019-07-08 04:35:42] Saving Adam parameters to models/wmt2017-transformer-de-en/model/model.back/model.npz.optimizer.npz
