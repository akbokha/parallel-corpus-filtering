[2019-07-07 17:04:59] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-07 17:04:59] [marian] Running on fulla as process 397333 with command line:
[2019-07-07 17:04:59] [marian] /fs/bil0/abdel/marian-dev/build/marian --model models/wmt2017-transformer-en-de/model/model.back/model.npz --type s2s --train-sets models/wmt2017-transformer-en-de/data/corpus.bpe.de models/wmt2017-transformer-en-de/data/corpus.bpe.en --max-length 100 --vocabs models/wmt2017-transformer-en-de/model/vocab.ende.yml models/wmt2017-transformer-en-de/model/vocab.ende.yml --mini-batch-fit -w 6000 --maxi-batch 1000 --valid-freq 10000 --save-freq 10000 --disp-freq 1000 --valid-metrics ce-mean-words perplexity translation --valid-script-path 'bash ./models/wmt2017-transformer-en-de/validate.en.sh' --valid-translation-output models/wmt2017-transformer-en-de/data/valid.bpe.de.output --quiet-translation --valid-sets models/wmt2017-transformer-en-de/data/valid.bpe.de models/wmt2017-transformer-en-de/data/valid.bpe.en --valid-mini-batch 64 --beam-size 12 --normalize=1 --overwrite --keep-best --early-stopping 5 --after-epochs 10 --cost-type=ce-mean-words --log models/wmt2017-transformer-en-de/model/model.back/train.log --valid-log models/wmt2017-transformer-en-de/model/model.back/valid.log --tied-embeddings-all --layer-normalization --devices 1 3 --seed 1111 --exponential-smoothing
[2019-07-07 17:04:59] [config] after-batches: 0
[2019-07-07 17:04:59] [config] after-epochs: 10
[2019-07-07 17:04:59] [config] allow-unk: false
[2019-07-07 17:04:59] [config] beam-size: 12
[2019-07-07 17:04:59] [config] bert-class-symbol: "[CLS]"
[2019-07-07 17:04:59] [config] bert-mask-symbol: "[MASK]"
[2019-07-07 17:04:59] [config] bert-masking-fraction: 0.15
[2019-07-07 17:04:59] [config] bert-sep-symbol: "[SEP]"
[2019-07-07 17:04:59] [config] bert-train-type-embeddings: true
[2019-07-07 17:04:59] [config] bert-type-vocab-size: 2
[2019-07-07 17:04:59] [config] best-deep: false
[2019-07-07 17:04:59] [config] clip-gemm: 0
[2019-07-07 17:04:59] [config] clip-norm: 1
[2019-07-07 17:04:59] [config] cost-type: ce-mean-words
[2019-07-07 17:04:59] [config] cpu-threads: 0
[2019-07-07 17:04:59] [config] data-weighting: ""
[2019-07-07 17:04:59] [config] data-weighting-type: sentence
[2019-07-07 17:04:59] [config] dec-cell: gru
[2019-07-07 17:04:59] [config] dec-cell-base-depth: 2
[2019-07-07 17:04:59] [config] dec-cell-high-depth: 1
[2019-07-07 17:04:59] [config] dec-depth: 1
[2019-07-07 17:04:59] [config] devices:
[2019-07-07 17:04:59] [config]   - 1
[2019-07-07 17:04:59] [config]   - 3
[2019-07-07 17:04:59] [config] dim-emb: 512
[2019-07-07 17:04:59] [config] dim-rnn: 1024
[2019-07-07 17:04:59] [config] dim-vocabs:
[2019-07-07 17:04:59] [config]   - 0
[2019-07-07 17:04:59] [config]   - 0
[2019-07-07 17:04:59] [config] disp-first: 0
[2019-07-07 17:04:59] [config] disp-freq: 1000
[2019-07-07 17:04:59] [config] disp-label-counts: false
[2019-07-07 17:04:59] [config] dropout-rnn: 0
[2019-07-07 17:04:59] [config] dropout-src: 0
[2019-07-07 17:04:59] [config] dropout-trg: 0
[2019-07-07 17:04:59] [config] dump-config: ""
[2019-07-07 17:04:59] [config] early-stopping: 5
[2019-07-07 17:04:59] [config] embedding-fix-src: false
[2019-07-07 17:04:59] [config] embedding-fix-trg: false
[2019-07-07 17:04:59] [config] embedding-normalization: false
[2019-07-07 17:04:59] [config] embedding-vectors:
[2019-07-07 17:04:59] [config]   []
[2019-07-07 17:04:59] [config] enc-cell: gru
[2019-07-07 17:04:59] [config] enc-cell-depth: 1
[2019-07-07 17:04:59] [config] enc-depth: 1
[2019-07-07 17:04:59] [config] enc-type: bidirectional
[2019-07-07 17:04:59] [config] exponential-smoothing: 0.0001
[2019-07-07 17:04:59] [config] grad-dropping-momentum: 0
[2019-07-07 17:04:59] [config] grad-dropping-rate: 0
[2019-07-07 17:04:59] [config] grad-dropping-warmup: 100
[2019-07-07 17:04:59] [config] guided-alignment: none
[2019-07-07 17:04:59] [config] guided-alignment-cost: mse
[2019-07-07 17:04:59] [config] guided-alignment-weight: 0.1
[2019-07-07 17:04:59] [config] ignore-model-config: false
[2019-07-07 17:04:59] [config] input-types:
[2019-07-07 17:04:59] [config]   []
[2019-07-07 17:04:59] [config] interpolate-env-vars: false
[2019-07-07 17:04:59] [config] keep-best: true
[2019-07-07 17:04:59] [config] label-smoothing: 0
[2019-07-07 17:04:59] [config] layer-normalization: true
[2019-07-07 17:04:59] [config] learn-rate: 0.0001
[2019-07-07 17:04:59] [config] log: models/wmt2017-transformer-en-de/model/model.back/train.log
[2019-07-07 17:04:59] [config] log-level: info
[2019-07-07 17:04:59] [config] log-time-zone: ""
[2019-07-07 17:04:59] [config] lr-decay: 0
[2019-07-07 17:04:59] [config] lr-decay-freq: 50000
[2019-07-07 17:04:59] [config] lr-decay-inv-sqrt:
[2019-07-07 17:04:59] [config]   - 0
[2019-07-07 17:04:59] [config] lr-decay-repeat-warmup: false
[2019-07-07 17:04:59] [config] lr-decay-reset-optimizer: false
[2019-07-07 17:04:59] [config] lr-decay-start:
[2019-07-07 17:04:59] [config]   - 10
[2019-07-07 17:04:59] [config]   - 1
[2019-07-07 17:04:59] [config] lr-decay-strategy: epoch+stalled
[2019-07-07 17:04:59] [config] lr-report: false
[2019-07-07 17:04:59] [config] lr-warmup: 0
[2019-07-07 17:04:59] [config] lr-warmup-at-reload: false
[2019-07-07 17:04:59] [config] lr-warmup-cycle: false
[2019-07-07 17:04:59] [config] lr-warmup-start-rate: 0
[2019-07-07 17:04:59] [config] max-length: 100
[2019-07-07 17:04:59] [config] max-length-crop: false
[2019-07-07 17:04:59] [config] max-length-factor: 3
[2019-07-07 17:04:59] [config] maxi-batch: 1000
[2019-07-07 17:04:59] [config] maxi-batch-sort: trg
[2019-07-07 17:04:59] [config] mini-batch: 64
[2019-07-07 17:04:59] [config] mini-batch-fit: true
[2019-07-07 17:04:59] [config] mini-batch-fit-step: 10
[2019-07-07 17:04:59] [config] mini-batch-overstuff: 1
[2019-07-07 17:04:59] [config] mini-batch-track-lr: false
[2019-07-07 17:04:59] [config] mini-batch-understuff: 1
[2019-07-07 17:04:59] [config] mini-batch-warmup: 0
[2019-07-07 17:04:59] [config] mini-batch-words: 0
[2019-07-07 17:04:59] [config] mini-batch-words-ref: 0
[2019-07-07 17:04:59] [config] model: models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 17:04:59] [config] multi-loss-type: sum
[2019-07-07 17:04:59] [config] multi-node: false
[2019-07-07 17:04:59] [config] multi-node-overlap: true
[2019-07-07 17:04:59] [config] n-best: false
[2019-07-07 17:04:59] [config] no-nccl: false
[2019-07-07 17:04:59] [config] no-reload: false
[2019-07-07 17:04:59] [config] no-restore-corpus: false
[2019-07-07 17:04:59] [config] no-shuffle: false
[2019-07-07 17:04:59] [config] normalize: 1
[2019-07-07 17:04:59] [config] num-devices: 0
[2019-07-07 17:04:59] [config] optimizer: adam
[2019-07-07 17:04:59] [config] optimizer-delay: 1
[2019-07-07 17:04:59] [config] optimizer-params:
[2019-07-07 17:04:59] [config]   []
[2019-07-07 17:04:59] [config] overwrite: true
[2019-07-07 17:04:59] [config] pretrained-model: ""
[2019-07-07 17:04:59] [config] quiet: false
[2019-07-07 17:04:59] [config] quiet-translation: true
[2019-07-07 17:04:59] [config] relative-paths: false
[2019-07-07 17:04:59] [config] right-left: false
[2019-07-07 17:04:59] [config] save-freq: 10000
[2019-07-07 17:04:59] [config] seed: 1111
[2019-07-07 17:04:59] [config] shuffle-in-ram: false
[2019-07-07 17:04:59] [config] skip: false
[2019-07-07 17:04:59] [config] sqlite: ""
[2019-07-07 17:04:59] [config] sqlite-drop: false
[2019-07-07 17:04:59] [config] sync-sgd: false
[2019-07-07 17:04:59] [config] tempdir: /tmp
[2019-07-07 17:04:59] [config] tied-embeddings: false
[2019-07-07 17:04:59] [config] tied-embeddings-all: true
[2019-07-07 17:04:59] [config] tied-embeddings-src: false
[2019-07-07 17:04:59] [config] train-sets:
[2019-07-07 17:04:59] [config]   - models/wmt2017-transformer-en-de/data/corpus.bpe.de
[2019-07-07 17:04:59] [config]   - models/wmt2017-transformer-en-de/data/corpus.bpe.en
[2019-07-07 17:04:59] [config] transformer-aan-activation: swish
[2019-07-07 17:04:59] [config] transformer-aan-depth: 2
[2019-07-07 17:04:59] [config] transformer-aan-nogate: false
[2019-07-07 17:04:59] [config] transformer-decoder-autoreg: self-attention
[2019-07-07 17:04:59] [config] transformer-dim-aan: 2048
[2019-07-07 17:04:59] [config] transformer-dim-ffn: 2048
[2019-07-07 17:04:59] [config] transformer-dropout: 0
[2019-07-07 17:04:59] [config] transformer-dropout-attention: 0
[2019-07-07 17:04:59] [config] transformer-dropout-ffn: 0
[2019-07-07 17:04:59] [config] transformer-ffn-activation: swish
[2019-07-07 17:04:59] [config] transformer-ffn-depth: 2
[2019-07-07 17:04:59] [config] transformer-guided-alignment-layer: last
[2019-07-07 17:04:59] [config] transformer-heads: 8
[2019-07-07 17:04:59] [config] transformer-no-projection: false
[2019-07-07 17:04:59] [config] transformer-postprocess: dan
[2019-07-07 17:04:59] [config] transformer-postprocess-emb: d
[2019-07-07 17:04:59] [config] transformer-preprocess: ""
[2019-07-07 17:04:59] [config] transformer-tied-layers:
[2019-07-07 17:04:59] [config]   []
[2019-07-07 17:04:59] [config] transformer-train-position-embeddings: false
[2019-07-07 17:04:59] [config] type: s2s
[2019-07-07 17:04:59] [config] ulr: false
[2019-07-07 17:04:59] [config] ulr-dim-emb: 0
[2019-07-07 17:04:59] [config] ulr-dropout: 0
[2019-07-07 17:04:59] [config] ulr-keys-vectors: ""
[2019-07-07 17:04:59] [config] ulr-query-vectors: ""
[2019-07-07 17:04:59] [config] ulr-softmax-temperature: 1
[2019-07-07 17:04:59] [config] ulr-trainable-transformation: false
[2019-07-07 17:04:59] [config] valid-freq: 10000
[2019-07-07 17:04:59] [config] valid-log: models/wmt2017-transformer-en-de/model/model.back/valid.log
[2019-07-07 17:04:59] [config] valid-max-length: 1000
[2019-07-07 17:04:59] [config] valid-metrics:
[2019-07-07 17:04:59] [config]   - ce-mean-words
[2019-07-07 17:04:59] [config]   - perplexity
[2019-07-07 17:04:59] [config]   - translation
[2019-07-07 17:04:59] [config] valid-mini-batch: 64
[2019-07-07 17:04:59] [config] valid-script-path: bash ./models/wmt2017-transformer-en-de/validate.en.sh
[2019-07-07 17:04:59] [config] valid-sets:
[2019-07-07 17:04:59] [config]   - models/wmt2017-transformer-en-de/data/valid.bpe.de
[2019-07-07 17:04:59] [config]   - models/wmt2017-transformer-en-de/data/valid.bpe.en
[2019-07-07 17:04:59] [config] valid-translation-output: models/wmt2017-transformer-en-de/data/valid.bpe.de.output
[2019-07-07 17:04:59] [config] vocabs:
[2019-07-07 17:04:59] [config]   - models/wmt2017-transformer-en-de/model/vocab.ende.yml
[2019-07-07 17:04:59] [config]   - models/wmt2017-transformer-en-de/model/vocab.ende.yml
[2019-07-07 17:04:59] [config] word-penalty: 0
[2019-07-07 17:04:59] [config] workspace: 6000
[2019-07-07 17:04:59] [config] Model is being created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-07 17:04:59] Using asynchronous training
[2019-07-07 17:04:59] [data] Loading vocabulary from JSON/Yaml file models/wmt2017-transformer-en-de/model/vocab.ende.yml
[2019-07-07 17:05:00] [data] Setting vocabulary size for input 0 to 36000
[2019-07-07 17:05:00] [data] Loading vocabulary from JSON/Yaml file models/wmt2017-transformer-en-de/model/vocab.ende.yml
[2019-07-07 17:05:00] [data] Setting vocabulary size for input 1 to 36000
[2019-07-07 17:05:00] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-07 17:05:00] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-07 17:05:01] [memory] Extending reserved space to 6016 MB (device gpu1)
[2019-07-07 17:05:03] [memory] Extending reserved space to 6016 MB (device gpu3)
[2019-07-07 17:05:03] [memory] Reserving 199 MB, device gpu1
[2019-07-07 17:05:03] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-07 17:05:03] [memory] Reserving 199 MB, device gpu1
[2019-07-07 17:05:17] [batching] Done. Typical MB size is 10153 target words
[2019-07-07 17:05:17] [memory] Extending reserved space to 6016 MB (device gpu1)
[2019-07-07 17:05:17] [memory] Extending reserved space to 6016 MB (device gpu3)
[2019-07-07 17:05:17] Training started
[2019-07-07 17:05:17] [data] Shuffling data
[2019-07-07 17:05:20] [data] Done reading 4561263 sentences
[2019-07-07 17:05:42] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 17:05:45] [memory] Reserving 199 MB, device gpu1
[2019-07-07 17:05:45] [memory] Reserving 199 MB, device gpu3
[2019-07-07 17:05:45] [memory] Reserving 99 MB, device gpu1
[2019-07-07 17:05:45] [memory] Reserving 99 MB, device gpu3
[2019-07-07 17:05:46] [memory] Reserving 99 MB, device gpu1
[2019-07-07 17:05:46] [memory] Reserving 99 MB, device gpu3
[2019-07-07 17:05:46] [memory] Reserving 99 MB, device gpu1
[2019-07-07 17:05:46] [memory] Reserving 99 MB, device gpu3
[2019-07-07 17:05:46] [memory] Reserving 199 MB, device gpu1
[2019-07-07 17:05:46] [memory] Reserving 199 MB, device gpu3
[2019-07-07 17:05:46] [memory] Reserving 199 MB, device gpu1
[2019-07-07 17:05:46] [memory] Reserving 199 MB, device gpu3
[2019-07-07 17:08:53] Ep. 1 : Up. 1000 : Sen. 225,477 : Cost 6.19547844 : Time 232.93s : 28557.17 words/s
[2019-07-07 17:11:58] Ep. 1 : Up. 2000 : Sen. 447,025 : Cost 5.28317451 : Time 185.29s : 35316.88 words/s
[2019-07-07 17:15:04] Ep. 1 : Up. 3000 : Sen. 669,411 : Cost 4.91460323 : Time 186.47s : 35179.03 words/s
[2019-07-07 17:18:13] Ep. 1 : Up. 4000 : Sen. 894,040 : Cost 4.65947866 : Time 188.29s : 35211.73 words/s
[2019-07-07 17:21:19] Ep. 1 : Up. 5000 : Sen. 1,116,594 : Cost 4.48661518 : Time 186.04s : 35136.43 words/s
[2019-07-07 17:24:25] Ep. 1 : Up. 6000 : Sen. 1,341,397 : Cost 4.33862257 : Time 186.46s : 35489.51 words/s
[2019-07-07 17:27:34] Ep. 1 : Up. 7000 : Sen. 1,563,810 : Cost 4.23505735 : Time 188.82s : 34810.58 words/s
[2019-07-07 17:30:41] Ep. 1 : Up. 8000 : Sen. 1,788,030 : Cost 4.13930464 : Time 186.68s : 35416.24 words/s
[2019-07-07 17:33:48] Ep. 1 : Up. 9000 : Sen. 2,011,727 : Cost 4.05534935 : Time 187.23s : 35337.07 words/s
[2019-07-07 17:36:55] Ep. 1 : Up. 10000 : Sen. 2,235,270 : Cost 3.97974968 : Time 187.33s : 35395.86 words/s
[2019-07-07 17:36:56] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 17:37:04] [valid] Ep. 1 : Up. 10000 : ce-mean-words : 4.5445 : new best
[2019-07-07 17:37:07] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 17:37:16] [valid] Ep. 1 : Up. 10000 : perplexity : 94.113 : new best
[2019-07-07 17:38:13] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 17:38:24] [valid] Ep. 1 : Up. 10000 : translation : 2.27 : new best
[2019-07-07 17:38:24] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 17:38:31] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 17:41:52] Ep. 1 : Up. 11000 : Sen. 2,458,170 : Cost 3.90660191 : Time 296.54s : 22163.66 words/s
[2019-07-07 17:45:00] Ep. 1 : Up. 12000 : Sen. 2,683,326 : Cost 3.85243201 : Time 187.66s : 35271.58 words/s
[2019-07-07 17:48:05] Ep. 1 : Up. 13000 : Sen. 2,906,233 : Cost 3.79637885 : Time 185.75s : 35283.10 words/s
[2019-07-07 17:51:14] Ep. 1 : Up. 14000 : Sen. 3,128,077 : Cost 3.74940872 : Time 188.63s : 34958.37 words/s
[2019-07-07 17:54:24] Ep. 1 : Up. 15000 : Sen. 3,352,381 : Cost 3.69524240 : Time 189.67s : 35050.28 words/s
[2019-07-07 17:57:31] Ep. 1 : Up. 16000 : Sen. 3,573,889 : Cost 3.49254775 : Time 186.99s : 34796.28 words/s
[2019-07-07 18:00:38] Ep. 1 : Up. 17000 : Sen. 3,798,227 : Cost 3.12241292 : Time 186.99s : 35186.72 words/s
[2019-07-07 18:03:45] Ep. 1 : Up. 18000 : Sen. 4,021,730 : Cost 2.92030883 : Time 187.09s : 35315.36 words/s
[2019-07-07 18:06:54] Ep. 1 : Up. 19000 : Sen. 4,245,809 : Cost 2.81546307 : Time 189.67s : 34947.39 words/s
[2019-07-07 18:10:01] Ep. 1 : Up. 20000 : Sen. 4,469,219 : Cost 2.75154924 : Time 186.52s : 35252.71 words/s
[2019-07-07 18:10:02] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 18:10:08] [valid] Ep. 1 : Up. 20000 : ce-mean-words : 2.74827 : new best
[2019-07-07 18:10:10] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 18:10:21] [valid] Ep. 1 : Up. 20000 : perplexity : 15.6156 : new best
[2019-07-07 18:11:11] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 18:11:20] [valid] Ep. 1 : Up. 20000 : translation : 19.43 : new best
[2019-07-07 18:11:20] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 18:11:34] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 18:12:38] Seen 4532084 samples
[2019-07-07 18:12:38] Starting epoch 2
[2019-07-07 18:12:38] [data] Shuffling data
[2019-07-07 18:12:41] [data] Done reading 4561263 sentences
[2019-07-07 18:13:04] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 18:15:20] Ep. 2 : Up. 21000 : Sen. 160,450 : Cost 2.66021061 : Time 319.43s : 20632.06 words/s
[2019-07-07 18:18:27] Ep. 2 : Up. 22000 : Sen. 382,680 : Cost 2.61395264 : Time 186.95s : 35145.45 words/s
[2019-07-07 18:21:35] Ep. 2 : Up. 23000 : Sen. 606,655 : Cost 2.58315849 : Time 187.94s : 35301.70 words/s
[2019-07-07 18:24:42] Ep. 2 : Up. 24000 : Sen. 830,487 : Cost 2.55598688 : Time 186.58s : 35275.64 words/s
[2019-07-07 18:27:49] Ep. 2 : Up. 25000 : Sen. 1,054,563 : Cost 2.51774645 : Time 186.77s : 35241.34 words/s
[2019-07-07 18:30:55] Ep. 2 : Up. 26000 : Sen. 1,275,965 : Cost 2.50329065 : Time 186.91s : 35174.82 words/s
[2019-07-07 18:34:02] Ep. 2 : Up. 27000 : Sen. 1,501,505 : Cost 2.46640015 : Time 186.64s : 35453.96 words/s
[2019-07-07 18:37:10] Ep. 2 : Up. 28000 : Sen. 1,724,706 : Cost 2.45959091 : Time 187.51s : 35201.11 words/s
[2019-07-07 18:40:16] Ep. 2 : Up. 29000 : Sen. 1,945,970 : Cost 2.43578625 : Time 186.50s : 35315.37 words/s
[2019-07-07 18:43:23] Ep. 2 : Up. 30000 : Sen. 2,171,671 : Cost 2.41395617 : Time 187.30s : 35284.44 words/s
[2019-07-07 18:43:25] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 18:43:36] [valid] Ep. 2 : Up. 30000 : ce-mean-words : 2.11601 : new best
[2019-07-07 18:43:38] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 18:43:52] [valid] Ep. 2 : Up. 30000 : perplexity : 8.29794 : new best
[2019-07-07 18:44:38] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 18:44:46] [valid] Ep. 2 : Up. 30000 : translation : 27.36 : new best
[2019-07-07 18:44:46] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 18:44:55] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 18:48:13] Ep. 2 : Up. 31000 : Sen. 2,395,166 : Cost 2.40956545 : Time 289.83s : 22698.92 words/s
[2019-07-07 18:51:22] Ep. 2 : Up. 32000 : Sen. 2,618,673 : Cost 2.38849783 : Time 189.15s : 34871.34 words/s
[2019-07-07 18:54:28] Ep. 2 : Up. 33000 : Sen. 2,842,852 : Cost 2.36636615 : Time 185.35s : 35392.45 words/s
[2019-07-07 18:57:35] Ep. 2 : Up. 34000 : Sen. 3,063,671 : Cost 2.36500883 : Time 187.11s : 35161.90 words/s
[2019-07-07 19:00:41] Ep. 2 : Up. 35000 : Sen. 3,287,128 : Cost 2.34086084 : Time 185.92s : 35315.71 words/s
[2019-07-07 19:03:47] Ep. 2 : Up. 36000 : Sen. 3,509,916 : Cost 2.32914972 : Time 186.38s : 35238.03 words/s
[2019-07-07 19:06:55] Ep. 2 : Up. 37000 : Sen. 3,732,139 : Cost 2.33442998 : Time 187.46s : 35206.58 words/s
[2019-07-07 19:10:03] Ep. 2 : Up. 38000 : Sen. 3,956,212 : Cost 2.29999804 : Time 188.66s : 34724.96 words/s
[2019-07-07 19:13:11] Ep. 2 : Up. 39000 : Sen. 4,177,608 : Cost 2.31266642 : Time 187.63s : 35237.75 words/s
[2019-07-07 19:16:17] Ep. 2 : Up. 40000 : Sen. 4,400,321 : Cost 2.30219746 : Time 185.98s : 35158.77 words/s
[2019-07-07 19:16:18] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 19:16:26] [valid] Ep. 2 : Up. 40000 : ce-mean-words : 1.91485 : new best
[2019-07-07 19:16:29] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 19:16:40] [valid] Ep. 2 : Up. 40000 : perplexity : 6.7859 : new best
[2019-07-07 19:17:26] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 19:17:36] [valid] Ep. 2 : Up. 40000 : translation : 29.65 : new best
[2019-07-07 19:17:36] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 19:17:46] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 19:19:53] Seen 4532483 samples
[2019-07-07 19:19:53] Starting epoch 3
[2019-07-07 19:19:53] [data] Shuffling data
[2019-07-07 19:19:57] [data] Done reading 4561263 sentences
[2019-07-07 19:20:19] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 19:21:37] Ep. 3 : Up. 41000 : Sen. 92,187 : Cost 2.24218750 : Time 320.09s : 20601.24 words/s
[2019-07-07 19:24:44] Ep. 3 : Up. 42000 : Sen. 312,538 : Cost 2.23509097 : Time 186.89s : 34977.60 words/s
[2019-07-07 19:27:53] Ep. 3 : Up. 43000 : Sen. 538,488 : Cost 2.19536924 : Time 188.67s : 35130.20 words/s
[2019-07-07 19:30:59] Ep. 3 : Up. 44000 : Sen. 761,838 : Cost 2.21248460 : Time 186.84s : 35133.31 words/s
[2019-07-07 19:34:06] Ep. 3 : Up. 45000 : Sen. 984,406 : Cost 2.20768881 : Time 186.13s : 35306.58 words/s
[2019-07-07 19:37:13] Ep. 3 : Up. 46000 : Sen. 1,206,106 : Cost 2.20539045 : Time 187.39s : 35181.29 words/s
[2019-07-07 19:40:20] Ep. 3 : Up. 47000 : Sen. 1,428,855 : Cost 2.18833208 : Time 187.09s : 35177.15 words/s
[2019-07-07 19:43:30] Ep. 3 : Up. 48000 : Sen. 1,653,123 : Cost 2.19320226 : Time 189.94s : 34850.69 words/s
[2019-07-07 19:46:37] Ep. 3 : Up. 49000 : Sen. 1,877,484 : Cost 2.18976355 : Time 186.72s : 35269.99 words/s
[2019-07-07 19:49:45] Ep. 3 : Up. 50000 : Sen. 2,100,953 : Cost 2.19341207 : Time 187.98s : 35290.79 words/s
[2019-07-07 19:49:46] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 19:49:55] [valid] Ep. 3 : Up. 50000 : ce-mean-words : 1.81254 : new best
[2019-07-07 19:49:59] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 19:50:10] [valid] Ep. 3 : Up. 50000 : perplexity : 6.12597 : new best
[2019-07-07 19:50:56] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 19:51:03] [valid] Ep. 3 : Up. 50000 : translation : 30.71 : new best
[2019-07-07 19:51:04] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 19:51:23] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 19:54:39] Ep. 3 : Up. 51000 : Sen. 2,325,491 : Cost 2.17736435 : Time 294.74s : 22378.30 words/s
[2019-07-07 19:57:46] Ep. 3 : Up. 52000 : Sen. 2,549,615 : Cost 2.17085338 : Time 186.44s : 35489.10 words/s
[2019-07-07 20:00:54] Ep. 3 : Up. 53000 : Sen. 2,773,430 : Cost 2.16929054 : Time 188.57s : 34826.88 words/s
[2019-07-07 20:04:01] Ep. 3 : Up. 54000 : Sen. 2,996,934 : Cost 2.17058372 : Time 186.70s : 35433.19 words/s
[2019-07-07 20:07:07] Ep. 3 : Up. 55000 : Sen. 3,218,553 : Cost 2.16528034 : Time 186.05s : 35146.48 words/s
[2019-07-07 20:10:15] Ep. 3 : Up. 56000 : Sen. 3,441,331 : Cost 2.16384292 : Time 188.18s : 35274.91 words/s
[2019-07-07 20:13:21] Ep. 3 : Up. 57000 : Sen. 3,665,124 : Cost 2.16490865 : Time 186.13s : 35286.30 words/s
[2019-07-07 20:16:29] Ep. 3 : Up. 58000 : Sen. 3,889,318 : Cost 2.14585185 : Time 187.88s : 35317.66 words/s
[2019-07-07 20:19:38] Ep. 3 : Up. 59000 : Sen. 4,114,583 : Cost 2.14419222 : Time 188.57s : 34960.89 words/s
[2019-07-07 20:22:45] Ep. 3 : Up. 60000 : Sen. 4,338,030 : Cost 2.14460063 : Time 187.05s : 35322.20 words/s
[2019-07-07 20:22:46] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 20:22:54] [valid] Ep. 3 : Up. 60000 : ce-mean-words : 1.74401 : new best
[2019-07-07 20:22:55] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 20:23:03] [valid] Ep. 3 : Up. 60000 : perplexity : 5.72026 : new best
[2019-07-07 20:23:50] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 20:24:02] [valid] Ep. 3 : Up. 60000 : translation : 31.53 : new best
[2019-07-07 20:24:03] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 20:24:21] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 20:27:17] Seen 4532896 samples
[2019-07-07 20:27:17] Starting epoch 4
[2019-07-07 20:27:17] [data] Shuffling data
[2019-07-07 20:27:20] [data] Done reading 4561263 sentences
[2019-07-07 20:27:47] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 20:28:12] Ep. 4 : Up. 61000 : Sen. 26,478 : Cost 2.12598443 : Time 327.45s : 20003.23 words/s
[2019-07-07 20:31:19] Ep. 4 : Up. 62000 : Sen. 248,758 : Cost 2.06952500 : Time 186.68s : 35191.09 words/s
[2019-07-07 20:34:26] Ep. 4 : Up. 63000 : Sen. 470,867 : Cost 2.07099319 : Time 186.54s : 35221.11 words/s
[2019-07-07 20:37:34] Ep. 4 : Up. 64000 : Sen. 695,709 : Cost 2.07496953 : Time 188.42s : 35060.20 words/s
[2019-07-07 20:40:41] Ep. 4 : Up. 65000 : Sen. 918,233 : Cost 2.06685901 : Time 186.76s : 35275.53 words/s
[2019-07-07 20:43:48] Ep. 4 : Up. 66000 : Sen. 1,142,160 : Cost 2.07080269 : Time 186.93s : 35356.66 words/s
[2019-07-07 20:46:55] Ep. 4 : Up. 67000 : Sen. 1,366,349 : Cost 2.06761599 : Time 186.94s : 35230.14 words/s
[2019-07-07 20:50:01] Ep. 4 : Up. 68000 : Sen. 1,590,003 : Cost 2.07501745 : Time 186.14s : 35438.63 words/s
[2019-07-07 20:53:07] Ep. 4 : Up. 69000 : Sen. 1,813,356 : Cost 2.06866717 : Time 186.03s : 35389.87 words/s
[2019-07-07 20:56:16] Ep. 4 : Up. 70000 : Sen. 2,036,183 : Cost 2.06194925 : Time 188.77s : 34897.67 words/s
[2019-07-07 20:56:17] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 20:56:27] [valid] Ep. 4 : Up. 70000 : ce-mean-words : 1.70405 : new best
[2019-07-07 20:56:28] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 20:56:37] [valid] Ep. 4 : Up. 70000 : perplexity : 5.49617 : new best
[2019-07-07 20:57:23] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 20:57:35] [valid] Ep. 4 : Up. 70000 : translation : 31.98 : new best
[2019-07-07 20:57:36] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 20:57:49] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 21:01:10] Ep. 4 : Up. 71000 : Sen. 2,257,266 : Cost 2.07551742 : Time 294.84s : 22292.37 words/s
[2019-07-07 21:04:17] Ep. 4 : Up. 72000 : Sen. 2,482,068 : Cost 2.06595922 : Time 186.99s : 35316.86 words/s
[2019-07-07 21:07:23] Ep. 4 : Up. 73000 : Sen. 2,705,247 : Cost 2.06263208 : Time 186.02s : 35253.90 words/s
[2019-07-07 21:10:31] Ep. 4 : Up. 74000 : Sen. 2,928,422 : Cost 2.07321310 : Time 187.97s : 35079.28 words/s
[2019-07-07 21:13:41] Ep. 4 : Up. 75000 : Sen. 3,151,524 : Cost 2.06902957 : Time 189.41s : 35029.10 words/s
[2019-07-07 21:16:47] Ep. 4 : Up. 76000 : Sen. 3,374,288 : Cost 2.06408978 : Time 186.15s : 35248.31 words/s
[2019-07-07 21:19:54] Ep. 4 : Up. 77000 : Sen. 3,599,097 : Cost 2.04868317 : Time 186.52s : 35476.90 words/s
[2019-07-07 21:23:00] Ep. 4 : Up. 78000 : Sen. 3,820,654 : Cost 2.07245445 : Time 186.58s : 35149.98 words/s
[2019-07-07 21:26:06] Ep. 4 : Up. 79000 : Sen. 4,045,628 : Cost 2.04498005 : Time 185.69s : 35487.37 words/s
[2019-07-07 21:29:15] Ep. 4 : Up. 80000 : Sen. 4,270,228 : Cost 2.05864763 : Time 189.46s : 35049.78 words/s
[2019-07-07 21:29:16] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 21:29:29] [valid] Ep. 4 : Up. 80000 : ce-mean-words : 1.67021 : new best
[2019-07-07 21:29:30] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 21:29:43] [valid] Ep. 4 : Up. 80000 : perplexity : 5.31327 : new best
[2019-07-07 21:30:29] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 21:30:37] [valid] Ep. 4 : Up. 80000 : translation : 32.31 : new best
[2019-07-07 21:30:37] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 21:30:49] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 21:34:04] Ep. 4 : Up. 81000 : Sen. 4,492,292 : Cost 2.05286384 : Time 288.41s : 22684.27 words/s
[2019-07-07 21:34:38] Seen 4532575 samples
[2019-07-07 21:34:38] Starting epoch 5
[2019-07-07 21:34:38] [data] Shuffling data
[2019-07-07 21:34:41] [data] Done reading 4561263 sentences
[2019-07-07 21:35:03] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 21:37:39] Ep. 5 : Up. 82000 : Sen. 183,686 : Cost 1.99114621 : Time 214.88s : 30600.57 words/s
[2019-07-07 21:40:46] Ep. 5 : Up. 83000 : Sen. 407,382 : Cost 1.98475969 : Time 187.77s : 35323.71 words/s
[2019-07-07 21:43:53] Ep. 5 : Up. 84000 : Sen. 629,216 : Cost 1.99041891 : Time 186.46s : 35208.00 words/s
[2019-07-07 21:47:02] Ep. 5 : Up. 85000 : Sen. 854,400 : Cost 1.97936893 : Time 188.89s : 34967.03 words/s
[2019-07-07 21:50:08] Ep. 5 : Up. 86000 : Sen. 1,079,375 : Cost 1.98517287 : Time 186.85s : 35309.49 words/s
[2019-07-07 21:53:17] Ep. 5 : Up. 87000 : Sen. 1,299,530 : Cost 2.00009966 : Time 188.45s : 35122.16 words/s
[2019-07-07 21:56:23] Ep. 5 : Up. 88000 : Sen. 1,525,119 : Cost 1.99060869 : Time 186.11s : 35311.60 words/s
[2019-07-07 21:59:31] Ep. 5 : Up. 89000 : Sen. 1,746,972 : Cost 1.99700654 : Time 188.19s : 35180.62 words/s
[2019-07-07 22:02:40] Ep. 5 : Up. 90000 : Sen. 1,970,129 : Cost 1.99493039 : Time 188.87s : 34814.69 words/s
[2019-07-07 22:02:41] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 22:02:55] [valid] Ep. 5 : Up. 90000 : ce-mean-words : 1.65119 : new best
[2019-07-07 22:02:56] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 22:03:06] [valid] Ep. 5 : Up. 90000 : perplexity : 5.21321 : new best
[2019-07-07 22:03:52] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 22:03:59] [valid] Ep. 5 : Up. 90000 : translation : 32.53 : new best
[2019-07-07 22:03:59] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 22:04:16] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 22:07:34] Ep. 5 : Up. 91000 : Sen. 2,195,719 : Cost 1.98042142 : Time 293.96s : 22528.48 words/s
[2019-07-07 22:10:42] Ep. 5 : Up. 92000 : Sen. 2,417,772 : Cost 2.00472498 : Time 188.11s : 35064.58 words/s
[2019-07-07 22:13:49] Ep. 5 : Up. 93000 : Sen. 2,642,237 : Cost 1.98721027 : Time 186.42s : 35253.36 words/s
[2019-07-07 22:16:55] Ep. 5 : Up. 94000 : Sen. 2,865,081 : Cost 1.99345517 : Time 186.46s : 35378.37 words/s
[2019-07-07 22:20:03] Ep. 5 : Up. 95000 : Sen. 3,088,813 : Cost 1.99662542 : Time 188.09s : 34767.72 words/s
[2019-07-07 22:23:10] Ep. 5 : Up. 96000 : Sen. 3,309,810 : Cost 2.00082350 : Time 187.19s : 35169.68 words/s
[2019-07-07 22:26:17] Ep. 5 : Up. 97000 : Sen. 3,534,414 : Cost 1.98597085 : Time 186.96s : 35287.90 words/s
[2019-07-07 22:29:24] Ep. 5 : Up. 98000 : Sen. 3,756,128 : Cost 1.99304914 : Time 186.49s : 35173.03 words/s
[2019-07-07 22:32:30] Ep. 5 : Up. 99000 : Sen. 3,981,412 : Cost 1.99737453 : Time 186.61s : 35433.66 words/s
[2019-07-07 22:35:37] Ep. 5 : Up. 100000 : Sen. 4,205,956 : Cost 1.98705649 : Time 186.93s : 35292.09 words/s
[2019-07-07 22:35:39] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 22:35:52] [valid] Ep. 5 : Up. 100000 : ce-mean-words : 1.63189 : new best
[2019-07-07 22:35:54] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 22:36:05] [valid] Ep. 5 : Up. 100000 : perplexity : 5.11355 : new best
[2019-07-07 22:37:00] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 22:37:12] [valid] Ep. 5 : Up. 100000 : translation : 32.83 : new best
[2019-07-07 22:37:13] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 22:37:23] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 22:40:40] Ep. 5 : Up. 101000 : Sen. 4,430,183 : Cost 2.00534892 : Time 302.88s : 21852.46 words/s
[2019-07-07 22:42:08] Seen 4533505 samples
[2019-07-07 22:42:08] Starting epoch 6
[2019-07-07 22:42:08] [data] Shuffling data
[2019-07-07 22:42:11] [data] Done reading 4561263 sentences
[2019-07-07 22:42:34] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 22:44:16] Ep. 6 : Up. 102000 : Sen. 116,303 : Cost 1.96392810 : Time 215.46s : 30367.70 words/s
[2019-07-07 22:47:23] Ep. 6 : Up. 103000 : Sen. 339,041 : Cost 1.92399585 : Time 187.79s : 35053.79 words/s
[2019-07-07 22:50:31] Ep. 6 : Up. 104000 : Sen. 566,089 : Cost 1.90943444 : Time 187.89s : 35347.03 words/s
[2019-07-07 22:53:40] Ep. 6 : Up. 105000 : Sen. 788,302 : Cost 1.92786610 : Time 188.37s : 34835.90 words/s
[2019-07-07 22:56:48] Ep. 6 : Up. 106000 : Sen. 1,013,221 : Cost 1.93296051 : Time 188.26s : 35243.71 words/s
[2019-07-07 22:59:55] Ep. 6 : Up. 107000 : Sen. 1,235,576 : Cost 1.93274462 : Time 186.96s : 35217.97 words/s
[2019-07-07 23:03:02] Ep. 6 : Up. 108000 : Sen. 1,461,359 : Cost 1.93394136 : Time 187.40s : 35328.86 words/s
[2019-07-07 23:06:10] Ep. 6 : Up. 109000 : Sen. 1,682,547 : Cost 1.94635677 : Time 187.67s : 35092.59 words/s
[2019-07-07 23:09:17] Ep. 6 : Up. 110000 : Sen. 1,908,069 : Cost 1.92691159 : Time 187.10s : 35332.43 words/s
[2019-07-07 23:09:18] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 23:09:25] [valid] Ep. 6 : Up. 110000 : ce-mean-words : 1.62204 : new best
[2019-07-07 23:09:26] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 23:09:44] [valid] Ep. 6 : Up. 110000 : perplexity : 5.06339 : new best
[2019-07-07 23:10:36] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 23:10:44] [valid] Ep. 6 : Up. 110000 : translation : 32.93 : new best
[2019-07-07 23:10:44] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 23:10:57] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 23:14:13] Ep. 6 : Up. 111000 : Sen. 2,129,488 : Cost 1.95162058 : Time 296.13s : 22086.99 words/s
[2019-07-07 23:17:21] Ep. 6 : Up. 112000 : Sen. 2,353,526 : Cost 1.93618608 : Time 187.99s : 35369.07 words/s
[2019-07-07 23:20:28] Ep. 6 : Up. 113000 : Sen. 2,577,156 : Cost 1.93917084 : Time 186.73s : 35038.13 words/s
[2019-07-07 23:23:35] Ep. 6 : Up. 114000 : Sen. 2,800,328 : Cost 1.94167471 : Time 187.34s : 35293.13 words/s
[2019-07-07 23:26:42] Ep. 6 : Up. 115000 : Sen. 3,021,053 : Cost 1.95468032 : Time 186.80s : 35103.12 words/s
[2019-07-07 23:29:50] Ep. 6 : Up. 116000 : Sen. 3,245,672 : Cost 1.93757629 : Time 188.26s : 34945.79 words/s
[2019-07-07 23:32:57] Ep. 6 : Up. 117000 : Sen. 3,469,906 : Cost 1.94242287 : Time 186.51s : 35310.26 words/s
[2019-07-07 23:36:04] Ep. 6 : Up. 118000 : Sen. 3,690,246 : Cost 1.94723415 : Time 187.27s : 35028.28 words/s
[2019-07-07 23:39:11] Ep. 6 : Up. 119000 : Sen. 3,914,688 : Cost 1.95266962 : Time 186.77s : 35207.24 words/s
[2019-07-07 23:42:19] Ep. 6 : Up. 120000 : Sen. 4,137,788 : Cost 1.93935716 : Time 187.62s : 35308.39 words/s
[2019-07-07 23:42:20] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-07 23:42:35] [valid] Ep. 6 : Up. 120000 : ce-mean-words : 1.60972 : new best
[2019-07-07 23:42:36] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-07 23:42:55] [valid] Ep. 6 : Up. 120000 : perplexity : 5.00139 : new best
[2019-07-07 23:43:42] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-07 23:43:49] [valid] Ep. 6 : Up. 120000 : translation : 32.96 : new best
[2019-07-07 23:43:51] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-07 23:44:06] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-07 23:47:25] Ep. 6 : Up. 121000 : Sen. 4,362,664 : Cost 1.93694508 : Time 306.58s : 21471.75 words/s
[2019-07-07 23:49:50] Seen 4532353 samples
[2019-07-07 23:49:50] Starting epoch 7
[2019-07-07 23:49:50] [data] Shuffling data
[2019-07-07 23:49:53] [data] Done reading 4561263 sentences
[2019-07-07 23:50:20] [data] Done shuffling 4561263 sentences to temp files
[2019-07-07 23:51:05] Ep. 7 : Up. 122000 : Sen. 50,918 : Cost 1.93554580 : Time 219.69s : 29678.98 words/s
[2019-07-07 23:54:13] Ep. 7 : Up. 123000 : Sen. 273,387 : Cost 1.87634158 : Time 187.84s : 34979.80 words/s
[2019-07-07 23:57:25] Ep. 7 : Up. 124000 : Sen. 496,669 : Cost 1.86668885 : Time 192.60s : 34317.91 words/s
[2019-07-08 00:00:34] Ep. 7 : Up. 125000 : Sen. 718,904 : Cost 1.88114297 : Time 188.27s : 34845.25 words/s
[2019-07-08 00:03:40] Ep. 7 : Up. 126000 : Sen. 942,058 : Cost 1.88450360 : Time 186.20s : 35418.14 words/s
[2019-07-08 00:06:47] Ep. 7 : Up. 127000 : Sen. 1,166,379 : Cost 1.88050425 : Time 187.16s : 35138.75 words/s
[2019-07-08 00:09:54] Ep. 7 : Up. 128000 : Sen. 1,387,330 : Cost 1.89966881 : Time 187.35s : 35244.11 words/s
[2019-07-08 00:13:01] Ep. 7 : Up. 129000 : Sen. 1,611,134 : Cost 1.89171028 : Time 186.93s : 35123.21 words/s
[2019-07-08 00:16:08] Ep. 7 : Up. 130000 : Sen. 1,834,091 : Cost 1.89471066 : Time 186.99s : 35201.41 words/s
[2019-07-08 00:16:09] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-08 00:16:21] [valid] Ep. 7 : Up. 130000 : ce-mean-words : 1.60644 : new best
[2019-07-08 00:16:22] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-08 00:16:48] [valid] Ep. 7 : Up. 130000 : perplexity : 4.98506 : new best
[2019-07-08 00:17:38] [valid] Ep. 7 : Up. 130000 : translation : 32.96 : stalled 1 times (last best: 32.96)
[2019-07-08 00:17:38] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-08 00:17:55] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-08 00:21:21] Ep. 7 : Up. 131000 : Sen. 2,058,059 : Cost 1.89315069 : Time 312.53s : 21136.34 words/s
[2019-07-08 00:24:28] Ep. 7 : Up. 132000 : Sen. 2,280,372 : Cost 1.90115392 : Time 186.93s : 35370.00 words/s
[2019-07-08 00:27:34] Ep. 7 : Up. 133000 : Sen. 2,504,956 : Cost 1.90274680 : Time 186.37s : 35234.95 words/s
[2019-07-08 00:30:41] Ep. 7 : Up. 134000 : Sen. 2,729,414 : Cost 1.88984704 : Time 186.70s : 35348.55 words/s
[2019-07-08 00:33:48] Ep. 7 : Up. 135000 : Sen. 2,952,074 : Cost 1.90640485 : Time 187.08s : 35284.15 words/s
[2019-07-08 00:36:54] Ep. 7 : Up. 136000 : Sen. 3,175,987 : Cost 1.90077388 : Time 186.20s : 35189.67 words/s
[2019-07-08 00:40:02] Ep. 7 : Up. 137000 : Sen. 3,399,998 : Cost 1.90401328 : Time 187.53s : 35268.16 words/s
[2019-07-08 00:43:08] Ep. 7 : Up. 138000 : Sen. 3,622,679 : Cost 1.90705168 : Time 186.88s : 35139.51 words/s
[2019-07-08 00:46:14] Ep. 7 : Up. 139000 : Sen. 3,845,950 : Cost 1.90611112 : Time 185.47s : 35505.01 words/s
[2019-07-08 00:49:21] Ep. 7 : Up. 140000 : Sen. 4,068,795 : Cost 1.91598523 : Time 186.80s : 35396.77 words/s
[2019-07-08 00:49:22] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-08 00:49:41] [valid] Ep. 7 : Up. 140000 : ce-mean-words : 1.60019 : new best
[2019-07-08 00:49:42] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-08 00:49:51] [valid] Ep. 7 : Up. 140000 : perplexity : 4.95397 : new best
[2019-07-08 00:50:43] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-translation.npz
[2019-07-08 00:50:53] [valid] Ep. 7 : Up. 140000 : translation : 33.16 : new best
[2019-07-08 00:50:53] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-08 00:51:15] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-08 00:54:45] Ep. 7 : Up. 141000 : Sen. 4,290,897 : Cost 1.90552759 : Time 323.87s : 20239.80 words/s
[2019-07-08 00:57:50] Ep. 7 : Up. 142000 : Sen. 4,514,686 : Cost 1.90921474 : Time 184.97s : 35299.04 words/s
[2019-07-08 00:58:06] Seen 4533027 samples
[2019-07-08 00:58:06] Starting epoch 8
[2019-07-08 00:58:06] [data] Shuffling data
[2019-07-08 00:58:10] [data] Done reading 4561263 sentences
[2019-07-08 00:58:30] [data] Done shuffling 4561263 sentences to temp files
[2019-07-08 01:01:22] Ep. 8 : Up. 143000 : Sen. 204,041 : Cost 1.83720791 : Time 212.76s : 31084.68 words/s
[2019-07-08 01:04:28] Ep. 8 : Up. 144000 : Sen. 427,214 : Cost 1.82530594 : Time 185.59s : 35380.77 words/s
[2019-07-08 01:07:35] Ep. 8 : Up. 145000 : Sen. 652,196 : Cost 1.83776081 : Time 186.92s : 35347.31 words/s
[2019-07-08 01:10:42] Ep. 8 : Up. 146000 : Sen. 874,628 : Cost 1.85310340 : Time 187.49s : 35248.69 words/s
[2019-07-08 01:13:48] Ep. 8 : Up. 147000 : Sen. 1,097,663 : Cost 1.85492706 : Time 185.94s : 35312.48 words/s
[2019-07-08 01:16:57] Ep. 8 : Up. 148000 : Sen. 1,322,205 : Cost 1.85166276 : Time 188.95s : 35261.79 words/s
[2019-07-08 01:20:04] Ep. 8 : Up. 149000 : Sen. 1,546,683 : Cost 1.85810578 : Time 186.90s : 35301.65 words/s
[2019-07-08 01:23:10] Ep. 8 : Up. 150000 : Sen. 1,771,362 : Cost 1.84565663 : Time 186.27s : 35486.76 words/s
[2019-07-08 01:23:11] [valid] Ep. 8 : Up. 150000 : ce-mean-words : 1.60208 : stalled 1 times (last best: 1.60019)
[2019-07-08 01:23:17] [valid] Ep. 8 : Up. 150000 : perplexity : 4.96335 : stalled 1 times (last best: 4.95397)
[2019-07-08 01:24:12] [valid] Ep. 8 : Up. 150000 : translation : 32.99 : stalled 1 times (last best: 33.16)
[2019-07-08 01:24:12] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-08 01:24:44] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-08 01:28:04] Ep. 8 : Up. 151000 : Sen. 1,992,516 : Cost 1.86864066 : Time 293.38s : 22374.11 words/s
[2019-07-08 01:31:09] Ep. 8 : Up. 152000 : Sen. 2,216,792 : Cost 1.86275136 : Time 185.12s : 35350.38 words/s
[2019-07-08 01:34:16] Ep. 8 : Up. 153000 : Sen. 2,437,816 : Cost 1.86856186 : Time 186.89s : 35293.77 words/s
[2019-07-08 01:37:22] Ep. 8 : Up. 154000 : Sen. 2,661,605 : Cost 1.86056781 : Time 186.53s : 35311.23 words/s
[2019-07-08 01:40:29] Ep. 8 : Up. 155000 : Sen. 2,885,335 : Cost 1.86747313 : Time 186.88s : 35288.81 words/s
[2019-07-08 01:43:37] Ep. 8 : Up. 156000 : Sen. 3,108,685 : Cost 1.87113345 : Time 187.38s : 35268.72 words/s
[2019-07-08 01:46:43] Ep. 8 : Up. 157000 : Sen. 3,331,080 : Cost 1.87187934 : Time 186.16s : 35179.64 words/s
[2019-07-08 01:49:49] Ep. 8 : Up. 158000 : Sen. 3,556,098 : Cost 1.87378359 : Time 186.76s : 35487.79 words/s
[2019-07-08 01:52:57] Ep. 8 : Up. 159000 : Sen. 3,778,207 : Cost 1.86836171 : Time 187.35s : 35136.46 words/s
[2019-07-08 01:56:04] Ep. 8 : Up. 160000 : Sen. 4,001,887 : Cost 1.87041521 : Time 187.36s : 35340.05 words/s
[2019-07-08 01:56:05] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-ce-mean-words.npz
[2019-07-08 01:56:17] [valid] Ep. 8 : Up. 160000 : ce-mean-words : 1.59908 : new best
[2019-07-08 01:56:18] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.best-perplexity.npz
[2019-07-08 01:56:38] [valid] Ep. 8 : Up. 160000 : perplexity : 4.9485 : new best
[2019-07-08 01:57:27] [valid] Ep. 8 : Up. 160000 : translation : 33.03 : stalled 2 times (last best: 33.16)
[2019-07-08 01:57:27] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-08 01:57:36] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-08 02:00:57] Ep. 8 : Up. 161000 : Sen. 4,225,600 : Cost 1.88082504 : Time 293.03s : 22449.38 words/s
[2019-07-08 02:04:03] Ep. 8 : Up. 162000 : Sen. 4,448,567 : Cost 1.88351846 : Time 186.19s : 35115.84 words/s
[2019-07-08 02:05:14] Seen 4532735 samples
[2019-07-08 02:05:14] Starting epoch 9
[2019-07-08 02:05:14] [data] Shuffling data
[2019-07-08 02:05:18] [data] Done reading 4561263 sentences
[2019-07-08 02:05:41] [data] Done shuffling 4561263 sentences to temp files
[2019-07-08 02:07:41] Ep. 9 : Up. 163000 : Sen. 139,320 : Cost 1.82265592 : Time 217.14s : 30459.68 words/s
[2019-07-08 02:10:48] Ep. 9 : Up. 164000 : Sen. 362,330 : Cost 1.79142523 : Time 187.28s : 35101.60 words/s
[2019-07-08 02:13:53] Ep. 9 : Up. 165000 : Sen. 584,156 : Cost 1.81034279 : Time 185.34s : 35215.75 words/s
[2019-07-08 02:16:59] Ep. 9 : Up. 166000 : Sen. 809,271 : Cost 1.79318416 : Time 186.00s : 35546.10 words/s
[2019-07-08 02:20:06] Ep. 9 : Up. 167000 : Sen. 1,031,152 : Cost 1.82529020 : Time 186.83s : 35298.27 words/s
[2019-07-08 02:23:13] Ep. 9 : Up. 168000 : Sen. 1,253,594 : Cost 1.82335079 : Time 186.53s : 35165.91 words/s
[2019-07-08 02:26:20] Ep. 9 : Up. 169000 : Sen. 1,478,323 : Cost 1.81969655 : Time 187.91s : 35259.43 words/s
[2019-07-08 02:29:27] Ep. 9 : Up. 170000 : Sen. 1,699,614 : Cost 1.82300174 : Time 186.12s : 35326.63 words/s
[2019-07-08 02:29:28] [valid] Ep. 9 : Up. 170000 : ce-mean-words : 1.60416 : stalled 1 times (last best: 1.59908)
[2019-07-08 02:29:32] [valid] Ep. 9 : Up. 170000 : perplexity : 4.97366 : stalled 1 times (last best: 4.9485)
[2019-07-08 02:30:30] [valid] Ep. 9 : Up. 170000 : translation : 32.9 : stalled 3 times (last best: 33.16)
[2019-07-08 02:30:30] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-08 02:30:54] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-08 02:34:15] Ep. 9 : Up. 171000 : Sen. 1,923,378 : Cost 1.82957828 : Time 288.75s : 22667.77 words/s
[2019-07-08 02:37:23] Ep. 9 : Up. 172000 : Sen. 2,145,918 : Cost 1.82980919 : Time 187.86s : 35236.60 words/s
[2019-07-08 02:40:29] Ep. 9 : Up. 173000 : Sen. 2,370,540 : Cost 1.83595252 : Time 185.65s : 35426.84 words/s
[2019-07-08 02:43:35] Ep. 9 : Up. 174000 : Sen. 2,592,972 : Cost 1.84269345 : Time 186.54s : 35175.47 words/s
[2019-07-08 02:46:42] Ep. 9 : Up. 175000 : Sen. 2,815,922 : Cost 1.83740926 : Time 186.75s : 35248.32 words/s
[2019-07-08 02:49:48] Ep. 9 : Up. 176000 : Sen. 3,036,941 : Cost 1.84267747 : Time 185.99s : 35294.07 words/s
[2019-07-08 02:52:54] Ep. 9 : Up. 177000 : Sen. 3,261,325 : Cost 1.84352076 : Time 186.38s : 35324.81 words/s
[2019-07-08 02:56:02] Ep. 9 : Up. 178000 : Sen. 3,486,100 : Cost 1.84886944 : Time 187.42s : 35264.06 words/s
[2019-07-08 02:59:09] Ep. 9 : Up. 179000 : Sen. 3,710,091 : Cost 1.83687735 : Time 187.56s : 35246.88 words/s
[2019-07-08 03:02:16] Ep. 9 : Up. 180000 : Sen. 3,931,734 : Cost 1.85214567 : Time 186.24s : 35039.52 words/s
[2019-07-08 03:02:17] [valid] Ep. 9 : Up. 180000 : ce-mean-words : 1.60277 : stalled 2 times (last best: 1.59908)
[2019-07-08 03:02:20] [valid] Ep. 9 : Up. 180000 : perplexity : 4.96678 : stalled 2 times (last best: 4.9485)
[2019-07-08 03:03:11] [valid] Ep. 9 : Up. 180000 : translation : 32.93 : stalled 4 times (last best: 33.16)
[2019-07-08 03:03:11] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-08 03:03:33] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-08 03:07:00] Ep. 9 : Up. 181000 : Sen. 4,154,148 : Cost 1.84847915 : Time 284.74s : 23182.88 words/s
[2019-07-08 03:10:06] Ep. 9 : Up. 182000 : Sen. 4,378,506 : Cost 1.84791803 : Time 185.82s : 35281.97 words/s
[2019-07-08 03:12:18] Seen 4532612 samples
[2019-07-08 03:12:18] Starting epoch 10
[2019-07-08 03:12:18] [data] Shuffling data
[2019-07-08 03:12:21] [data] Done reading 4561263 sentences
[2019-07-08 03:12:43] [data] Done shuffling 4561263 sentences to temp files
[2019-07-08 03:13:41] Ep. 10 : Up. 183000 : Sen. 67,217 : Cost 1.83013248 : Time 214.54s : 30752.69 words/s
[2019-07-08 03:16:48] Ep. 10 : Up. 184000 : Sen. 292,597 : Cost 1.75918388 : Time 186.88s : 35362.14 words/s
[2019-07-08 03:19:54] Ep. 10 : Up. 185000 : Sen. 514,247 : Cost 1.78342700 : Time 186.80s : 35219.29 words/s
[2019-07-08 03:23:02] Ep. 10 : Up. 186000 : Sen. 738,709 : Cost 1.77746630 : Time 187.72s : 35276.61 words/s
[2019-07-08 03:26:08] Ep. 10 : Up. 187000 : Sen. 960,894 : Cost 1.79195249 : Time 185.87s : 35224.95 words/s
[2019-07-08 03:29:14] Ep. 10 : Up. 188000 : Sen. 1,185,086 : Cost 1.79490900 : Time 186.29s : 35304.20 words/s
[2019-07-08 03:32:21] Ep. 10 : Up. 189000 : Sen. 1,407,532 : Cost 1.79172838 : Time 187.12s : 35266.27 words/s
[2019-07-08 03:35:27] Ep. 10 : Up. 190000 : Sen. 1,630,627 : Cost 1.80227447 : Time 185.18s : 35194.60 words/s
[2019-07-08 03:35:28] [valid] Ep. 10 : Up. 190000 : ce-mean-words : 1.6088 : stalled 3 times (last best: 1.59908)
[2019-07-08 03:35:33] [valid] Ep. 10 : Up. 190000 : perplexity : 4.99681 : stalled 3 times (last best: 4.9485)
[2019-07-08 03:36:29] [valid] Ep. 10 : Up. 190000 : translation : 32.88 : stalled 5 times (last best: 33.16)
[2019-07-08 03:36:29] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-08 03:37:02] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-08 03:40:27] Ep. 10 : Up. 191000 : Sen. 1,852,702 : Cost 1.80021238 : Time 299.94s : 22055.52 words/s
[2019-07-08 03:43:34] Ep. 10 : Up. 192000 : Sen. 2,076,291 : Cost 1.80897975 : Time 187.17s : 35332.96 words/s
[2019-07-08 03:46:39] Ep. 10 : Up. 193000 : Sen. 2,298,409 : Cost 1.80936575 : Time 185.41s : 35127.02 words/s
[2019-07-08 03:49:48] Ep. 10 : Up. 194000 : Sen. 2,523,836 : Cost 1.80984163 : Time 188.36s : 35402.90 words/s
[2019-07-08 03:52:54] Ep. 10 : Up. 195000 : Sen. 2,746,588 : Cost 1.81591296 : Time 186.41s : 35179.50 words/s
[2019-07-08 03:56:00] Ep. 10 : Up. 196000 : Sen. 2,970,764 : Cost 1.80888224 : Time 186.21s : 35373.11 words/s
[2019-07-08 03:59:12] Ep. 10 : Up. 197000 : Sen. 3,192,511 : Cost 1.82004321 : Time 187.78s : 35139.56 words/s
[2019-07-08 04:02:18] Ep. 10 : Up. 198000 : Sen. 3,416,360 : Cost 1.82295954 : Time 186.56s : 35266.22 words/s
[2019-07-08 04:05:26] Ep. 10 : Up. 199000 : Sen. 3,638,674 : Cost 1.82004571 : Time 187.43s : 35145.88 words/s
[2019-07-08 04:08:33] Ep. 10 : Up. 200000 : Sen. 3,861,732 : Cost 1.82441580 : Time 187.69s : 35215.39 words/s
[2019-07-08 04:08:35] [valid] Ep. 10 : Up. 200000 : ce-mean-words : 1.60864 : stalled 4 times (last best: 1.59908)
[2019-07-08 04:08:40] [valid] Ep. 10 : Up. 200000 : perplexity : 4.99599 : stalled 4 times (last best: 4.9485)
[2019-07-08 04:09:44] [valid] Ep. 10 : Up. 200000 : translation : 32.87 : stalled 6 times (last best: 33.16)
[2019-07-08 04:09:44] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-08 04:10:20] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
[2019-07-08 04:13:51] Ep. 10 : Up. 201000 : Sen. 4,084,853 : Cost 1.81814301 : Time 317.38s : 20801.95 words/s
[2019-07-08 04:16:57] Ep. 10 : Up. 202000 : Sen. 4,307,209 : Cost 1.81743121 : Time 186.65s : 34951.47 words/s
[2019-07-08 04:20:04] Ep. 10 : Up. 203000 : Sen. 4,529,376 : Cost 1.83332467 : Time 186.72s : 35215.10 words/s
[2019-07-08 04:20:06] Seen 4533138 samples
[2019-07-08 04:20:06] Starting epoch 11
[2019-07-08 04:20:06] Training finished
[2019-07-08 04:20:07] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.orig.npz
[2019-07-08 04:20:20] [valid] Ep. 11 : Up. 203016 : ce-mean-words : 1.60654 : stalled 5 times (last best: 1.59908)
[2019-07-08 04:20:20] [valid] Ep. 11 : Up. 203016 : perplexity : 4.98551 : stalled 5 times (last best: 4.9485)
[2019-07-08 04:21:19] [valid] Ep. 11 : Up. 203016 : translation : 32.91 : stalled 7 times (last best: 33.16)
[2019-07-08 04:21:19] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz
[2019-07-08 04:21:46] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model.back/model.npz.optimizer.npz
