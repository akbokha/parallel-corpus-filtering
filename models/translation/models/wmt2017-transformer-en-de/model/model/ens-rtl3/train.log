[2019-07-17 14:55:26] [marian] Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-17 14:55:26] [marian] Running on fulla as process 175337 with command line:
[2019-07-17 14:55:26] [marian] /fs/bil0/abdel/marian-dev/build/marian --model models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz --type transformer --train-sets models/wmt2017-transformer-en-de/data/all.bpe.en models/wmt2017-transformer-en-de/data/all.bpe.de --max-length 100 --valid-max-length 50 --vocabs models/wmt2017-transformer-en-de/model/vocab.ende.yml models/wmt2017-transformer-en-de/model/vocab.ende.yml --mini-batch-fit -w 8000 --mini-batch 1000 --maxi-batch 1000 --valid-freq 5000 --save-freq 5000 --disp-freq 500 --valid-metrics ce-mean-words perplexity translation --valid-sets models/wmt2017-transformer-en-de/data/valid.bpe.en models/wmt2017-transformer-en-de/data/valid.bpe.de --valid-script-path 'bash ./models/wmt2017-transformer-en-de/validate.sh' --valid-translation-output models/wmt2017-transformer-en-de/data/valid.bpe.en.output --quiet-translation --beam-size 12 --normalize=1 --valid-mini-batch 64 --overwrite --keep-best --early-stopping 5 --after-epochs 8 --cost-type=ce-mean-words --log models/wmt2017-transformer-en-de/model/model/ens-rtl3/train.log --valid-log models/wmt2017-transformer-en-de/model/model/ens-rtl3/valid.log --enc-depth 6 --dec-depth 6 --tied-embeddings-all --transformer-dropout 0.1 --label-smoothing 0.1 --learn-rate 0.0003 --lr-warmup 16000 --lr-decay-inv-sqrt 16000 --lr-report --optimizer-params 0.9 0.98 1e-09 --clip-norm 5 --devices 1 3 --sync-sgd --seed 33333 --exponential-smoothing --right-left
[2019-07-17 14:55:26] [config] after-batches: 0
[2019-07-17 14:55:26] [config] after-epochs: 8
[2019-07-17 14:55:26] [config] allow-unk: false
[2019-07-17 14:55:26] [config] beam-size: 12
[2019-07-17 14:55:26] [config] bert-class-symbol: "[CLS]"
[2019-07-17 14:55:26] [config] bert-mask-symbol: "[MASK]"
[2019-07-17 14:55:26] [config] bert-masking-fraction: 0.15
[2019-07-17 14:55:26] [config] bert-sep-symbol: "[SEP]"
[2019-07-17 14:55:26] [config] bert-train-type-embeddings: true
[2019-07-17 14:55:26] [config] bert-type-vocab-size: 2
[2019-07-17 14:55:26] [config] best-deep: false
[2019-07-17 14:55:26] [config] clip-gemm: 0
[2019-07-17 14:55:26] [config] clip-norm: 5
[2019-07-17 14:55:26] [config] cost-type: ce-mean-words
[2019-07-17 14:55:26] [config] cpu-threads: 0
[2019-07-17 14:55:26] [config] data-weighting: ""
[2019-07-17 14:55:26] [config] data-weighting-type: sentence
[2019-07-17 14:55:26] [config] dec-cell: gru
[2019-07-17 14:55:26] [config] dec-cell-base-depth: 2
[2019-07-17 14:55:26] [config] dec-cell-high-depth: 1
[2019-07-17 14:55:26] [config] dec-depth: 6
[2019-07-17 14:55:26] [config] devices:
[2019-07-17 14:55:26] [config]   - 1
[2019-07-17 14:55:26] [config]   - 3
[2019-07-17 14:55:26] [config] dim-emb: 512
[2019-07-17 14:55:26] [config] dim-rnn: 1024
[2019-07-17 14:55:26] [config] dim-vocabs:
[2019-07-17 14:55:26] [config]   - 0
[2019-07-17 14:55:26] [config]   - 0
[2019-07-17 14:55:26] [config] disp-first: 0
[2019-07-17 14:55:26] [config] disp-freq: 500
[2019-07-17 14:55:26] [config] disp-label-counts: false
[2019-07-17 14:55:26] [config] dropout-rnn: 0
[2019-07-17 14:55:26] [config] dropout-src: 0
[2019-07-17 14:55:26] [config] dropout-trg: 0
[2019-07-17 14:55:26] [config] dump-config: ""
[2019-07-17 14:55:26] [config] early-stopping: 5
[2019-07-17 14:55:26] [config] embedding-fix-src: false
[2019-07-17 14:55:26] [config] embedding-fix-trg: false
[2019-07-17 14:55:26] [config] embedding-normalization: false
[2019-07-17 14:55:26] [config] embedding-vectors:
[2019-07-17 14:55:26] [config]   []
[2019-07-17 14:55:26] [config] enc-cell: gru
[2019-07-17 14:55:26] [config] enc-cell-depth: 1
[2019-07-17 14:55:26] [config] enc-depth: 6
[2019-07-17 14:55:26] [config] enc-type: bidirectional
[2019-07-17 14:55:26] [config] exponential-smoothing: 0.0001
[2019-07-17 14:55:26] [config] grad-dropping-momentum: 0
[2019-07-17 14:55:26] [config] grad-dropping-rate: 0
[2019-07-17 14:55:26] [config] grad-dropping-warmup: 100
[2019-07-17 14:55:26] [config] guided-alignment: none
[2019-07-17 14:55:26] [config] guided-alignment-cost: mse
[2019-07-17 14:55:26] [config] guided-alignment-weight: 0.1
[2019-07-17 14:55:26] [config] ignore-model-config: false
[2019-07-17 14:55:26] [config] input-types:
[2019-07-17 14:55:26] [config]   []
[2019-07-17 14:55:26] [config] interpolate-env-vars: false
[2019-07-17 14:55:26] [config] keep-best: true
[2019-07-17 14:55:26] [config] label-smoothing: 0.1
[2019-07-17 14:55:26] [config] layer-normalization: false
[2019-07-17 14:55:26] [config] learn-rate: 0.0003
[2019-07-17 14:55:26] [config] log: models/wmt2017-transformer-en-de/model/model/ens-rtl3/train.log
[2019-07-17 14:55:26] [config] log-level: info
[2019-07-17 14:55:26] [config] log-time-zone: ""
[2019-07-17 14:55:26] [config] lr-decay: 0
[2019-07-17 14:55:26] [config] lr-decay-freq: 50000
[2019-07-17 14:55:26] [config] lr-decay-inv-sqrt:
[2019-07-17 14:55:26] [config]   - 16000
[2019-07-17 14:55:26] [config] lr-decay-repeat-warmup: false
[2019-07-17 14:55:26] [config] lr-decay-reset-optimizer: false
[2019-07-17 14:55:26] [config] lr-decay-start:
[2019-07-17 14:55:26] [config]   - 10
[2019-07-17 14:55:26] [config]   - 1
[2019-07-17 14:55:26] [config] lr-decay-strategy: epoch+stalled
[2019-07-17 14:55:26] [config] lr-report: true
[2019-07-17 14:55:26] [config] lr-warmup: 16000
[2019-07-17 14:55:26] [config] lr-warmup-at-reload: false
[2019-07-17 14:55:26] [config] lr-warmup-cycle: false
[2019-07-17 14:55:26] [config] lr-warmup-start-rate: 0
[2019-07-17 14:55:26] [config] max-length: 100
[2019-07-17 14:55:26] [config] max-length-crop: false
[2019-07-17 14:55:26] [config] max-length-factor: 3
[2019-07-17 14:55:26] [config] maxi-batch: 1000
[2019-07-17 14:55:26] [config] maxi-batch-sort: trg
[2019-07-17 14:55:26] [config] mini-batch: 1000
[2019-07-17 14:55:26] [config] mini-batch-fit: true
[2019-07-17 14:55:26] [config] mini-batch-fit-step: 10
[2019-07-17 14:55:26] [config] mini-batch-overstuff: 1
[2019-07-17 14:55:26] [config] mini-batch-track-lr: false
[2019-07-17 14:55:26] [config] mini-batch-understuff: 1
[2019-07-17 14:55:26] [config] mini-batch-warmup: 0
[2019-07-17 14:55:26] [config] mini-batch-words: 0
[2019-07-17 14:55:26] [config] mini-batch-words-ref: 0
[2019-07-17 14:55:26] [config] model: models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz
[2019-07-17 14:55:26] [config] multi-loss-type: sum
[2019-07-17 14:55:26] [config] multi-node: false
[2019-07-17 14:55:26] [config] multi-node-overlap: true
[2019-07-17 14:55:26] [config] n-best: false
[2019-07-17 14:55:26] [config] no-nccl: false
[2019-07-17 14:55:26] [config] no-reload: false
[2019-07-17 14:55:26] [config] no-restore-corpus: false
[2019-07-17 14:55:26] [config] no-shuffle: false
[2019-07-17 14:55:26] [config] normalize: 1
[2019-07-17 14:55:26] [config] num-devices: 0
[2019-07-17 14:55:26] [config] optimizer: adam
[2019-07-17 14:55:26] [config] optimizer-delay: 1
[2019-07-17 14:55:26] [config] optimizer-params:
[2019-07-17 14:55:26] [config]   - 0.9
[2019-07-17 14:55:26] [config]   - 0.98
[2019-07-17 14:55:26] [config]   - 1e-09
[2019-07-17 14:55:26] [config] overwrite: true
[2019-07-17 14:55:26] [config] pretrained-model: ""
[2019-07-17 14:55:26] [config] quiet: false
[2019-07-17 14:55:26] [config] quiet-translation: true
[2019-07-17 14:55:26] [config] relative-paths: false
[2019-07-17 14:55:26] [config] right-left: true
[2019-07-17 14:55:26] [config] save-freq: 5000
[2019-07-17 14:55:26] [config] seed: 33333
[2019-07-17 14:55:26] [config] shuffle-in-ram: false
[2019-07-17 14:55:26] [config] skip: false
[2019-07-17 14:55:26] [config] sqlite: ""
[2019-07-17 14:55:26] [config] sqlite-drop: false
[2019-07-17 14:55:26] [config] sync-sgd: true
[2019-07-17 14:55:26] [config] tempdir: /tmp
[2019-07-17 14:55:26] [config] tied-embeddings: false
[2019-07-17 14:55:26] [config] tied-embeddings-all: true
[2019-07-17 14:55:26] [config] tied-embeddings-src: false
[2019-07-17 14:55:26] [config] train-sets:
[2019-07-17 14:55:26] [config]   - models/wmt2017-transformer-en-de/data/all.bpe.en
[2019-07-17 14:55:26] [config]   - models/wmt2017-transformer-en-de/data/all.bpe.de
[2019-07-17 14:55:26] [config] transformer-aan-activation: swish
[2019-07-17 14:55:26] [config] transformer-aan-depth: 2
[2019-07-17 14:55:26] [config] transformer-aan-nogate: false
[2019-07-17 14:55:26] [config] transformer-decoder-autoreg: self-attention
[2019-07-17 14:55:26] [config] transformer-dim-aan: 2048
[2019-07-17 14:55:26] [config] transformer-dim-ffn: 2048
[2019-07-17 14:55:26] [config] transformer-dropout: 0.1
[2019-07-17 14:55:26] [config] transformer-dropout-attention: 0
[2019-07-17 14:55:26] [config] transformer-dropout-ffn: 0
[2019-07-17 14:55:26] [config] transformer-ffn-activation: swish
[2019-07-17 14:55:26] [config] transformer-ffn-depth: 2
[2019-07-17 14:55:26] [config] transformer-guided-alignment-layer: last
[2019-07-17 14:55:26] [config] transformer-heads: 8
[2019-07-17 14:55:26] [config] transformer-no-projection: false
[2019-07-17 14:55:26] [config] transformer-postprocess: dan
[2019-07-17 14:55:26] [config] transformer-postprocess-emb: d
[2019-07-17 14:55:26] [config] transformer-preprocess: ""
[2019-07-17 14:55:26] [config] transformer-tied-layers:
[2019-07-17 14:55:26] [config]   []
[2019-07-17 14:55:26] [config] transformer-train-position-embeddings: false
[2019-07-17 14:55:26] [config] type: transformer
[2019-07-17 14:55:26] [config] ulr: false
[2019-07-17 14:55:26] [config] ulr-dim-emb: 0
[2019-07-17 14:55:26] [config] ulr-dropout: 0
[2019-07-17 14:55:26] [config] ulr-keys-vectors: ""
[2019-07-17 14:55:26] [config] ulr-query-vectors: ""
[2019-07-17 14:55:26] [config] ulr-softmax-temperature: 1
[2019-07-17 14:55:26] [config] ulr-trainable-transformation: false
[2019-07-17 14:55:26] [config] valid-freq: 5000
[2019-07-17 14:55:26] [config] valid-log: models/wmt2017-transformer-en-de/model/model/ens-rtl3/valid.log
[2019-07-17 14:55:26] [config] valid-max-length: 50
[2019-07-17 14:55:26] [config] valid-metrics:
[2019-07-17 14:55:26] [config]   - ce-mean-words
[2019-07-17 14:55:26] [config]   - perplexity
[2019-07-17 14:55:26] [config]   - translation
[2019-07-17 14:55:26] [config] valid-mini-batch: 64
[2019-07-17 14:55:26] [config] valid-script-path: bash ./models/wmt2017-transformer-en-de/validate.sh
[2019-07-17 14:55:26] [config] valid-sets:
[2019-07-17 14:55:26] [config]   - models/wmt2017-transformer-en-de/data/valid.bpe.en
[2019-07-17 14:55:26] [config]   - models/wmt2017-transformer-en-de/data/valid.bpe.de
[2019-07-17 14:55:26] [config] valid-translation-output: models/wmt2017-transformer-en-de/data/valid.bpe.en.output
[2019-07-17 14:55:26] [config] vocabs:
[2019-07-17 14:55:26] [config]   - models/wmt2017-transformer-en-de/model/vocab.ende.yml
[2019-07-17 14:55:26] [config]   - models/wmt2017-transformer-en-de/model/vocab.ende.yml
[2019-07-17 14:55:26] [config] word-penalty: 0
[2019-07-17 14:55:26] [config] workspace: 8000
[2019-07-17 14:55:26] [config] Model is being created with Marian v1.7.8 ec2d66e 2019-05-27 14:52:11 +0100
[2019-07-17 14:55:26] Using synchronous training
[2019-07-17 14:55:26] [data] Loading vocabulary from JSON/Yaml file models/wmt2017-transformer-en-de/model/vocab.ende.yml
[2019-07-17 14:55:27] [data] Setting vocabulary size for input 0 to 36000
[2019-07-17 14:55:27] [data] Loading vocabulary from JSON/Yaml file models/wmt2017-transformer-en-de/model/vocab.ende.yml
[2019-07-17 14:55:27] [data] Setting vocabulary size for input 1 to 36000
[2019-07-17 14:55:27] Compiled without MPI support. Falling back to FakeMPIWrapper
[2019-07-17 14:55:27] [batching] Collecting statistics for batch fitting with step size 10
[2019-07-17 14:55:28] [memory] Extending reserved space to 8064 MB (device gpu1)
[2019-07-17 14:55:30] [memory] Extending reserved space to 8064 MB (device gpu3)
[2019-07-17 14:55:30] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-17 14:55:30] [comm] NCCLCommunicator constructed successfully.
[2019-07-17 14:55:30] [training] Using 2 GPUs
[2019-07-17 14:55:30] [memory] Reserving 238 MB, device gpu1
[2019-07-17 14:55:30] [gpu] 16-bit TensorCores enabled for float32 matrix operations
[2019-07-17 14:55:30] [memory] Reserving 238 MB, device gpu1
[2019-07-17 14:55:37] [batching] Done. Typical MB size is 10656 target words
[2019-07-17 14:55:37] [memory] Extending reserved space to 8064 MB (device gpu1)
[2019-07-17 14:55:38] [memory] Extending reserved space to 8064 MB (device gpu3)
[2019-07-17 14:55:38] [comm] Using NCCL 2.4.2 for GPU communication
[2019-07-17 14:55:38] [comm] NCCLCommunicator constructed successfully.
[2019-07-17 14:55:38] [training] Using 2 GPUs
[2019-07-17 14:55:38] Training started
[2019-07-17 14:55:38] [data] Shuffling data
[2019-07-17 14:55:51] [data] Done reading 19122526 sentences
[2019-07-17 14:57:36] [data] Done shuffling 19122526 sentences to temp files
[2019-07-17 14:58:08] [training] Batches are processed as 1 process(es) x 2 devices/process
[2019-07-17 14:58:08] [memory] Reserving 238 MB, device gpu1
[2019-07-17 14:58:08] [memory] Reserving 238 MB, device gpu3
[2019-07-17 14:58:08] [memory] Reserving 238 MB, device gpu3
[2019-07-17 14:58:08] [memory] Reserving 238 MB, device gpu1
[2019-07-17 14:58:08] [memory] Reserving 119 MB, device gpu1
[2019-07-17 14:58:08] [memory] Reserving 119 MB, device gpu3
[2019-07-17 14:58:09] [memory] Reserving 238 MB, device gpu3
[2019-07-17 14:58:09] [memory] Reserving 238 MB, device gpu1
[2019-07-17 15:00:16] Ep. 1 : Up. 500 : Sen. 158,109 : Cost 9.49869156 : Time 289.38s : 15284.60 words/s : L.r. 9.3750e-06
[2019-07-17 15:02:25] Ep. 1 : Up. 1000 : Sen. 321,149 : Cost 8.17237377 : Time 128.95s : 34423.60 words/s : L.r. 1.8750e-05
[2019-07-17 15:04:35] Ep. 1 : Up. 1500 : Sen. 481,462 : Cost 7.84101629 : Time 130.10s : 34533.90 words/s : L.r. 2.8125e-05
[2019-07-17 15:06:44] Ep. 1 : Up. 2000 : Sen. 646,592 : Cost 7.58433342 : Time 128.51s : 34686.99 words/s : L.r. 3.7500e-05
[2019-07-17 15:08:51] Ep. 1 : Up. 2500 : Sen. 806,120 : Cost 7.33214521 : Time 127.13s : 34505.68 words/s : L.r. 4.6875e-05
[2019-07-17 15:10:59] Ep. 1 : Up. 3000 : Sen. 968,290 : Cost 7.05702305 : Time 128.02s : 34667.87 words/s : L.r. 5.6250e-05
[2019-07-17 15:13:07] Ep. 1 : Up. 3500 : Sen. 1,130,223 : Cost 6.81241369 : Time 127.57s : 34392.12 words/s : L.r. 6.5625e-05
[2019-07-17 15:15:14] Ep. 1 : Up. 4000 : Sen. 1,291,354 : Cost 6.61414862 : Time 127.37s : 34828.38 words/s : L.r. 7.5000e-05
[2019-07-17 15:17:23] Ep. 1 : Up. 4500 : Sen. 1,452,778 : Cost 6.43225670 : Time 128.78s : 34773.79 words/s : L.r. 8.4375e-05
[2019-07-17 15:19:32] Ep. 1 : Up. 5000 : Sen. 1,620,667 : Cost 6.20467043 : Time 128.72s : 34631.11 words/s : L.r. 9.3750e-05
[2019-07-17 15:19:32] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.orig.npz
[2019-07-17 15:19:38] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz
[2019-07-17 15:19:44] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.optimizer.npz
[2019-07-17 15:19:56] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.best-ce-mean-words.npz
[2019-07-17 15:20:01] [valid] Ep. 1 : Up. 5000 : ce-mean-words : 5.31307 : new best
[2019-07-17 15:20:02] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.best-perplexity.npz
[2019-07-17 15:20:06] [valid] Ep. 1 : Up. 5000 : perplexity : 202.972 : new best
[2019-07-17 15:21:28] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.best-translation.npz
[2019-07-17 15:21:33] [valid] Ep. 1 : Up. 5000 : translation : 0 : new best
[2019-07-17 15:23:43] Ep. 1 : Up. 5500 : Sen. 1,777,227 : Cost 6.00378370 : Time 251.00s : 17690.91 words/s : L.r. 1.0313e-04
[2019-07-17 15:25:52] Ep. 1 : Up. 6000 : Sen. 1,934,297 : Cost 5.75895214 : Time 129.32s : 34837.46 words/s : L.r. 1.1250e-04
[2019-07-17 15:28:01] Ep. 1 : Up. 6500 : Sen. 2,105,312 : Cost 5.45157194 : Time 128.70s : 34253.18 words/s : L.r. 1.2188e-04
[2019-07-17 15:30:09] Ep. 1 : Up. 7000 : Sen. 2,265,595 : Cost 5.26489067 : Time 128.40s : 34520.99 words/s : L.r. 1.3125e-04
[2019-07-17 15:32:17] Ep. 1 : Up. 7500 : Sen. 2,423,919 : Cost 4.96888542 : Time 128.50s : 34963.49 words/s : L.r. 1.4063e-04
[2019-07-17 15:34:27] Ep. 1 : Up. 8000 : Sen. 2,587,116 : Cost 4.75378895 : Time 129.13s : 34548.59 words/s : L.r. 1.5000e-04
[2019-07-17 15:36:35] Ep. 1 : Up. 8500 : Sen. 2,749,614 : Cost 4.53962231 : Time 127.97s : 34267.46 words/s : L.r. 1.5938e-04
[2019-07-17 15:38:43] Ep. 1 : Up. 9000 : Sen. 2,908,453 : Cost 4.38016844 : Time 128.30s : 34764.33 words/s : L.r. 1.6875e-04
[2019-07-17 15:40:50] Ep. 1 : Up. 9500 : Sen. 3,068,587 : Cost 4.24394751 : Time 127.60s : 34492.57 words/s : L.r. 1.7813e-04
[2019-07-17 15:42:58] Ep. 1 : Up. 10000 : Sen. 3,230,501 : Cost 4.13639021 : Time 127.79s : 34592.07 words/s : L.r. 1.8750e-04
[2019-07-17 15:42:58] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.orig.npz
[2019-07-17 15:43:05] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz
[2019-07-17 15:43:11] Saving Adam parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.optimizer.npz
[2019-07-17 15:43:24] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.best-ce-mean-words.npz
[2019-07-17 15:43:28] [valid] Ep. 1 : Up. 10000 : ce-mean-words : 2.74101 : new best
[2019-07-17 15:43:29] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.best-perplexity.npz
[2019-07-17 15:43:34] [valid] Ep. 1 : Up. 10000 : perplexity : 15.5026 : new best
[2019-07-17 15:44:10] Saving model weights and runtime parameters to models/wmt2017-transformer-en-de/model/model/ens-rtl3/model.npz.best-translation.npz
[2019-07-17 15:44:14] [valid] Ep. 1 : Up. 10000 : translation : 17.99 : new best
[2019-07-17 15:46:24] Ep. 1 : Up. 10500 : Sen. 3,390,850 : Cost 4.02789974 : Time 206.00s : 21544.01 words/s : L.r. 1.9688e-04
[2019-07-17 15:48:33] Ep. 1 : Up. 11000 : Sen. 3,555,995 : Cost 3.94575715 : Time 129.11s : 34588.33 words/s : L.r. 2.0625e-04
[2019-07-17 15:50:43] Ep. 1 : Up. 11500 : Sen. 3,714,297 : Cost 3.86941361 : Time 129.91s : 34708.43 words/s : L.r. 2.1563e-04
[2019-07-17 15:52:52] Ep. 1 : Up. 12000 : Sen. 3,881,119 : Cost 3.81227875 : Time 128.18s : 34436.80 words/s : L.r. 2.2500e-04
[2019-07-17 15:55:00] Ep. 1 : Up. 12500 : Sen. 4,039,343 : Cost 3.75865459 : Time 128.90s : 34502.52 words/s : L.r. 2.3438e-04
[2019-07-17 15:57:08] Ep. 1 : Up. 13000 : Sen. 4,204,407 : Cost 3.70983958 : Time 127.40s : 34847.12 words/s : L.r. 2.4375e-04
