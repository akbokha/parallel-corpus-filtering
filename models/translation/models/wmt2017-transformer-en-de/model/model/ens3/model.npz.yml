workspace: 8000
log: models/wmt2017-transformer-en-de/model/model/ens3/train.log
log-level: info
log-time-zone: ""
quiet: false
quiet-translation: true
seed: 3333
clip-gemm: 0
interpolate-env-vars: false
relative-paths: false
dump-config: ""
model: models/wmt2017-transformer-en-de/model/model/ens3/model.npz
pretrained-model: ""
ignore-model-config: false
type: transformer
dim-vocabs:
  - 36000
  - 36000
dim-emb: 512
dim-rnn: 1024
enc-type: bidirectional
enc-cell: gru
enc-cell-depth: 1
enc-depth: 6
dec-cell: gru
dec-cell-base-depth: 2
dec-cell-high-depth: 1
dec-depth: 6
skip: false
layer-normalization: false
right-left: false
input-types:
  []
best-deep: false
tied-embeddings: false
tied-embeddings-src: false
tied-embeddings-all: true
transformer-heads: 8
transformer-no-projection: false
transformer-dim-ffn: 2048
transformer-ffn-depth: 2
transformer-ffn-activation: swish
transformer-dim-aan: 2048
transformer-aan-depth: 2
transformer-aan-activation: swish
transformer-aan-nogate: false
transformer-decoder-autoreg: self-attention
transformer-tied-layers:
  []
transformer-guided-alignment-layer: last
transformer-preprocess: ""
transformer-postprocess-emb: d
transformer-postprocess: dan
transformer-train-position-embeddings: false
bert-mask-symbol: "[MASK]"
bert-sep-symbol: "[SEP]"
bert-class-symbol: "[CLS]"
bert-masking-fraction: 0.15
bert-train-type-embeddings: true
bert-type-vocab-size: 2
dropout-rnn: 0
dropout-src: 0
dropout-trg: 0
grad-dropping-rate: 0
grad-dropping-momentum: 0
grad-dropping-warmup: 100
transformer-dropout: 0.1
transformer-dropout-attention: 0
transformer-dropout-ffn: 0
cost-type: ce-mean-words
multi-loss-type: sum
overwrite: true
no-reload: false
train-sets:
  - models/wmt2017-transformer-en-de/data/all.bpe.en
  - models/wmt2017-transformer-en-de/data/all.bpe.de
vocabs:
  - models/wmt2017-transformer-en-de/model/vocab.ende.yml
  - models/wmt2017-transformer-en-de/model/vocab.ende.yml
after-epochs: 8
after-batches: 0
disp-freq: 500
disp-first: 0
disp-label-counts: false
save-freq: 5000
max-length: 50
max-length-crop: false
no-shuffle: false
no-restore-corpus: false
tempdir: /tmp
sqlite: ""
sqlite-drop: false
devices:
  - 1
  - 3
num-devices: 0
no-nccl: false
cpu-threads: 0
mini-batch: 1000
mini-batch-words: 0
mini-batch-fit: true
mini-batch-fit-step: 10
maxi-batch: 1000
maxi-batch-sort: trg
shuffle-in-ram: false
mini-batch-words-ref: 0
mini-batch-warmup: 0
mini-batch-track-lr: false
mini-batch-overstuff: 1
mini-batch-understuff: 1
optimizer: adam
optimizer-params:
  - 0.9
  - 0.98
  - 1e-09
optimizer-delay: 1
sync-sgd: true
learn-rate: 0.0003
lr-report: true
lr-decay: 0
lr-decay-strategy: epoch+stalled
lr-decay-start:
  - 10
  - 1
lr-decay-freq: 50000
lr-decay-reset-optimizer: false
lr-decay-repeat-warmup: false
lr-decay-inv-sqrt:
  - 16000
lr-warmup: 16000
lr-warmup-start-rate: 0
lr-warmup-cycle: false
lr-warmup-at-reload: false
label-smoothing: 0.1
clip-norm: 5
exponential-smoothing: 0.0001
guided-alignment: none
guided-alignment-cost: mse
guided-alignment-weight: 0.1
data-weighting: ""
data-weighting-type: sentence
embedding-vectors:
  []
embedding-normalization: false
embedding-fix-src: false
embedding-fix-trg: false
multi-node: false
multi-node-overlap: true
ulr: false
ulr-query-vectors: ""
ulr-keys-vectors: ""
ulr-trainable-transformation: false
ulr-dim-emb: 0
ulr-dropout: 0
ulr-softmax-temperature: 1
valid-sets:
  - models/wmt2017-transformer-en-de/data/valid.bpe.en
  - models/wmt2017-transformer-en-de/data/valid.bpe.de
valid-freq: 5000
valid-metrics:
  - ce-mean-words
  - perplexity
  - translation
early-stopping: 5
beam-size: 12
normalize: 1
max-length-factor: 3
word-penalty: 0
allow-unk: false
n-best: false
valid-mini-batch: 64
valid-max-length: 50
valid-script-path: bash ./models/wmt2017-transformer-en-de/validate.sh
valid-translation-output: models/wmt2017-transformer-en-de/data/valid.bpe.en.output
keep-best: true
valid-log: models/wmt2017-transformer-en-de/model/model/ens3/valid.log